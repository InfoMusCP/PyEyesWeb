{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyEyesWeb Movement Analysis Toolkit \ud83d\ude80 Extract features from raw human movement data \ud83c\udf93 Apply in science, health, and the arts \ud83e\udd1d Developed with the partial support of the EU ICT STARTS Resilence Project \ud83d\udcd6 Get Started | GitHub Repository Why PyEyesWeb? PyEyesWeb builds on the Expressive Gesture Analysis library of EyesWeb , bringing expressive movement analysis into Python as a core aim of the project, creating a modern, modular, and accessible toolkit for research, health, and artistic applications. PyEyesWeb is designed to facilitate adoption in artificial intelligence and machine learning pipelines , while also enabling seamless integration with creative and interactive platforms (e.g., TouchDesigner, Unity, and Max/MSP among others), supporting innovative, cross-disciplinary projects at the intersection of science and the arts. Learn more about EyesWeb EyesWeb is an open software research platform for the design and development of real-time multimodal systems and interfaces . It supports a wide variety of inputs, including motion capture, cameras, game controllers (Kinect, Wii), multichannel audio, and physiological signals. Outputs include multichannel audio, video, analog devices, and robotic platforms. EyesWeb provides libraries such as Non-Verbal Expressive Gesture Analysis and Non-Verbal Social Signals Analysis , and a visual programming environment that enables users to develop real-time, networked applications . Originally started in 1997, EyesWeb has been adopted worldwide in scientific research, education, and industry, including EU projects and collaborations with organizations such as INTEL and NYU. Use Cases \ud83c\udf93 Research & Science Quantify movement expressivity and analyze biomechanics with validated methods. \ud83d\udd17 Interactive Media Integrate PyEyesWeb in real-time with TouchDesigner . \ud83c\udfe5 Health & Rehabilitation Assess movement disorders, monitor recovery, and support clinical studies. \ud83c\udfad Artistic Performance Explore synchrony, smoothness, and expressive qualities in dance and live performance. Methodological Foundation PyEyesWeb is informed by decades of research at the intersection of movement science, computational modeling, and expressive gesture analysis . A key methodological foundation of PyEyesWeb is the layered conceptual framework for analyzing expressive qualities of movement developed by InfoMus Lab \u2013 Casa Paganini [1]. This framework models an observer of a dance performance through four interconnected layers: from the physical signals captured by sensors, to the higher-level expressive qualities conveyed by movement (such as emotions). References [1] Camurri, A., Volpe, G., Piana, S., Mancini, M., Niewiadomski, R., Ferrari, N., & Canepa, C. (2016, July). The dancer in the eye: towards a multi-layered computational framework of qualities in movement. In Proceedings of the 3rd International Symposium on Movement and Computing (pp. 1-7). Project Context About the Authors PyEyesWeb is developed by InfoMus Lab \u2013 Casa Paganini , University of Genoa, as partners of the Resilence EU Project. PyEyesWeb is developed with the partial support of the EU ICT STARTS Resilence Project , funded by the European Union\u2019s Horizon programme. Community & Collaboration Whether you are a researcher, artist, or developer , PyEyesWeb helps you bridge movement, computation, and expression. It is designed to be modular, accessible, and integrable , supporting a variety of use cases from scientific analysis to interactive artistic performances. \ud83d\udca1 We welcome contributions, collaborations, and use cases from the community. Check out our GitHub repository to report issues, suggest features, or contribute code. MIT Licensed \u00b7 Open for collaboration","title":"Home"},{"location":"#pyeyesweb","text":"Movement Analysis Toolkit \ud83d\ude80 Extract features from raw human movement data \ud83c\udf93 Apply in science, health, and the arts \ud83e\udd1d Developed with the partial support of the EU ICT STARTS Resilence Project \ud83d\udcd6 Get Started | GitHub Repository","title":"PyEyesWeb"},{"location":"#why-pyeyesweb","text":"PyEyesWeb builds on the Expressive Gesture Analysis library of EyesWeb , bringing expressive movement analysis into Python as a core aim of the project, creating a modern, modular, and accessible toolkit for research, health, and artistic applications. PyEyesWeb is designed to facilitate adoption in artificial intelligence and machine learning pipelines , while also enabling seamless integration with creative and interactive platforms (e.g., TouchDesigner, Unity, and Max/MSP among others), supporting innovative, cross-disciplinary projects at the intersection of science and the arts. Learn more about EyesWeb EyesWeb is an open software research platform for the design and development of real-time multimodal systems and interfaces . It supports a wide variety of inputs, including motion capture, cameras, game controllers (Kinect, Wii), multichannel audio, and physiological signals. Outputs include multichannel audio, video, analog devices, and robotic platforms. EyesWeb provides libraries such as Non-Verbal Expressive Gesture Analysis and Non-Verbal Social Signals Analysis , and a visual programming environment that enables users to develop real-time, networked applications . Originally started in 1997, EyesWeb has been adopted worldwide in scientific research, education, and industry, including EU projects and collaborations with organizations such as INTEL and NYU.","title":"Why PyEyesWeb?"},{"location":"#use-cases","text":"\ud83c\udf93 Research & Science Quantify movement expressivity and analyze biomechanics with validated methods. \ud83d\udd17 Interactive Media Integrate PyEyesWeb in real-time with TouchDesigner . \ud83c\udfe5 Health & Rehabilitation Assess movement disorders, monitor recovery, and support clinical studies. \ud83c\udfad Artistic Performance Explore synchrony, smoothness, and expressive qualities in dance and live performance.","title":"Use Cases"},{"location":"#methodological-foundation","text":"PyEyesWeb is informed by decades of research at the intersection of movement science, computational modeling, and expressive gesture analysis . A key methodological foundation of PyEyesWeb is the layered conceptual framework for analyzing expressive qualities of movement developed by InfoMus Lab \u2013 Casa Paganini [1]. This framework models an observer of a dance performance through four interconnected layers: from the physical signals captured by sensors, to the higher-level expressive qualities conveyed by movement (such as emotions). References [1] Camurri, A., Volpe, G., Piana, S., Mancini, M., Niewiadomski, R., Ferrari, N., & Canepa, C. (2016, July). The dancer in the eye: towards a multi-layered computational framework of qualities in movement. In Proceedings of the 3rd International Symposium on Movement and Computing (pp. 1-7).","title":"Methodological Foundation"},{"location":"#project-context","text":"About the Authors PyEyesWeb is developed by InfoMus Lab \u2013 Casa Paganini , University of Genoa, as partners of the Resilence EU Project. PyEyesWeb is developed with the partial support of the EU ICT STARTS Resilence Project , funded by the European Union\u2019s Horizon programme. Community & Collaboration Whether you are a researcher, artist, or developer , PyEyesWeb helps you bridge movement, computation, and expression. It is designed to be modular, accessible, and integrable , supporting a variety of use cases from scientific analysis to interactive artistic performances. \ud83d\udca1 We welcome contributions, collaborations, and use cases from the community. Check out our GitHub repository to report issues, suggest features, or contribute code. MIT Licensed \u00b7 Open for collaboration","title":"Project Context"},{"location":"API/","text":"API Reference This section contains the automatically generated reference documentation for the pyeyesweb package. You can browse the modules using the sidebar navigation.","title":"API Reference"},{"location":"API/#api-reference","text":"This section contains the automatically generated reference documentation for the pyeyesweb package. You can browse the modules using the sidebar navigation.","title":"API Reference"},{"location":"API/SUMMARY/","text":"Analysis Primitives Bilateral Symmetry Clusterability Mse Dominance Rarity Statistical Moment Synchronization Data Models Sliding Window Thread Safe Buffer Low Level Contraction Expansion Equilibrium Kinetic Energy Smoothness Mid Level Lightness Utils Math Utils Signal Generators Signal Processing Tsv Reader Validators","title":"SUMMARY"},{"location":"API/analysis_primitives/bilateral_symmetry/","text":"Bilateral Symmetry Bilateral Symmetry Analysis for Motion Capture Data Below are a few research paper references: 1. \"Movement Symmetry Assessment by Bilateral Motion Data Fusion\" (2018) Link: https://pubmed.ncbi.nlm.nih.gov/29993408/ Provides Canonical Correlation Analysis (CCA) methodology and introduces bilateral motion data fusion concepts \"Symmetry Analysis of Manual Wheelchair Propulsion Using Motion Capture Techniques\" (2022) Link: https://www.mdpi.com/2073-8994/14/6/1164/htm Provides coefficient of variation calculation method and shows marker-based symmetry comparison approach General bilateral coordination research concepts from multiple biomechanics papers Phase synchronization using Hilbert transform and bilateral symmetry index calculations Note: Specific implementation details are adapted for a real-time MoCap analysis. Must be tested by us at CP. BilateralSymmetryAnalyzer Real-time bilateral symmetry analysis for motion capture data. Based on research methods from bilateral motion data fusion and wheelchair propulsion symmetry analysis papers. Read more in the User Guide . Source code in pyeyesweb/analysis_primitives/bilateral_symmetry.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 class BilateralSymmetryAnalyzer : \"\"\" Real-time bilateral symmetry analysis for motion capture data. Based on research methods from bilateral motion data fusion and wheelchair propulsion symmetry analysis papers. Read more in the [User Guide](/PyEyesWeb/user_guide/theoretical_framework/analysis_primitives/bilateral_symmetry/). \"\"\" def __init__ ( self , window_size = 100 , joint_pairs = None , filter_params = None ): \"\"\" Initialize bilateral symmetry analyzer. Args: window_size: Number of frames for sliding window analysis joint_pairs: List of tuples defining bilateral joint pairs filter_params: Optional tuple of (lowcut_hz, highcut_hz, sampling_rate_hz) for band-pass filtering. If None, no filtering is applied. \"\"\" self . window_size = validate_window_size ( window_size ) # Use ThreadSafeHistoryBuffer instead of deque + lock self . history = ThreadSafeHistoryBuffer ( maxlen = self . window_size ) # Default joint pairs for standard MoCap setup if joint_pairs is None : self . joint_pairs = [ ( 4 , 5 ), # left_shoulder, right_shoulder ( 6 , 7 ), # left_elbow, right_elbow ( 8 , 9 ), # left_wrist, right_wrist ( 10 , 11 ), # left_hip, right_hip ( 12 , 13 ), # left_knee, right_knee ( 14 , 15 ), # left_ankle, right_ankle ] else : self . joint_pairs = joint_pairs # validate and normalize filter params self . filter_params = validate_and_normalize_filter_params ( filter_params ) def _compute_bilateral_symmetry_index ( self , left_data , right_data ): \"\"\" Compute Bilateral Symmetry Index based on research methodology. Inspired by wheelchair propulsion research paper method: Compares mirrored bilateral movements and calculates relative differences. Args: left_data: (n_frames, 3) array of left joint positions right_data: (n_frames, 3) array of right joint positions Returns: float: Symmetry index (0-1, where 1 is perfect symmetry) \"\"\" # Mirror right side data across sagittal plane (flip x-coordinate) right_mirrored = right_data . copy () right_mirrored [:, 0 ] *= - 1 # Flip x-axis for bilateral comparison # Calculate relative differences (from wheelchair research) diff = np . abs ( left_data - right_mirrored ) sum_val = np . abs ( left_data ) + np . abs ( right_mirrored ) # Avoid division by zero sum_val = np . where ( sum_val == 0 , 1e-8 , sum_val ) # Relative asymmetry percentage relative_asymmetry = np . mean ( diff / sum_val ) * 100 # Convert to symmetry index (100% asymmetry = 0 symmetry) symmetry_index = max ( 0 , 1 - ( relative_asymmetry / 100 )) return symmetry_index def _compute_cca_correlation ( self , left_data , right_data ): \"\"\" Compute canonical correlation between bilateral data. Based on \"Movement Symmetry Assessment by Bilateral Motion Data Fusion\" paper methodology using Canonical Correlation Analysis. Args: left_data: (n_frames, 3) left joint data right_data: (n_frames, 3) right joint data Returns: float: Canonical correlation (0-1) \"\"\" if left_data . shape [ 0 ] < 5 : # Need minimum samples for CCA return np . nan # Not enough data for CCA try : # Flatten spatial coordinates for CCA analysis left_features = left_data . reshape ( left_data . shape [ 0 ], - 1 ) right_features = right_data . reshape ( right_data . shape [ 0 ], - 1 ) # Apply CCA with single component cca = CCA ( n_components = 1 ) left_c , right_c = cca . fit_transform ( left_features , right_features ) # Compute canonical correlation correlation = np . corrcoef ( left_c . flatten (), right_c . flatten ())[ 0 , 1 ] # Handle NaN cases if np . isnan ( correlation ): return np . nan # Correlation computation failed return abs ( correlation ) # Take absolute value except Exception as e : import warnings warnings . warn ( f \"CCA correlation computation failed: { e } \" , RuntimeWarning ) return np . nan def analyze_frame ( self , mocap_frame ): \"\"\" Analyze single frame of MoCap data for bilateral symmetry. Args: mocap_frame: (n_joints, 3) array of joint positions for one frame Returns: dict: Symmetry metrics for current frame \"\"\" self . history . append ( mocap_frame ) history_length = len ( self . history ) if history_length < 10 : # Need minimum history for analysis return { 'overall_symmetry' : 0.0 , 'phase_sync' : 0.0 , 'cca_correlation' : 0.0 , 'joint_symmetries' : {} } # ThreadSafeHistoryBuffer provides thread-safe get_array method history_array = self . history . get_array () # (n_frames, n_joints, 3) joint_symmetries = {} symmetry_scores = [] phase_scores = [] cca_scores = [] # Analyze each bilateral joint pair for left_idx , right_idx in self . joint_pairs : left_joint_data = history_array [:, left_idx , :] right_joint_data = history_array [:, right_idx , :] # Compute multiple symmetry metrics bsi = self . _compute_bilateral_symmetry_index ( left_joint_data , right_joint_data ) # Use vertical movement for phase analysis (most relevant for gait) # Compute phase synchronization directly try : sig = np . column_stack ([ left_joint_data [:, 2 ], right_joint_data [:, 2 ]]) phase_sync = compute_phase_synchronization ( sig , self . filter_params ) except Exception as e : warnings . warn ( f \"Phase symmetry computation failed: { e } \" , RuntimeWarning ) phase_sync = np . nan cca_corr = self . _compute_cca_correlation ( left_joint_data , right_joint_data ) joint_pair_name = f \"joint_ { left_idx } _ { right_idx } \" joint_symmetries [ joint_pair_name ] = { 'bilateral_symmetry_index' : bsi , 'phase_synchronization' : phase_sync , 'cca_correlation' : cca_corr } symmetry_scores . append ( bsi ) phase_scores . append ( phase_sync ) cca_scores . append ( cca_corr ) # Compute overall metrics overall_symmetry = np . mean ( symmetry_scores ) if symmetry_scores else 0.0 overall_phase_sync = np . mean ( phase_scores ) if phase_scores else 0.0 overall_cca = np . mean ( cca_scores ) if cca_scores else 0.0 return { 'overall_symmetry' : overall_symmetry , 'phase_sync' : overall_phase_sync , 'cca_correlation' : overall_cca , 'joint_symmetries' : joint_symmetries } def __call__ ( self , mocap_frame ): \"\"\" Compute bilateral symmetry metrics for motion capture frame. This method provides a standardized API interface by delegating to analyze_frame. Args: mocap_frame: (n_joints, 3) array of joint positions for one frame Returns: dict: Symmetry metrics containing: - overall_symmetry: Overall bilateral symmetry score (0-1) - phase_sync: Phase synchronization value - cca_correlation: Canonical correlation coefficient - joint_symmetries: Per-joint-pair symmetry metrics \"\"\" return self . analyze_frame ( mocap_frame ) __call__ ( mocap_frame ) Compute bilateral symmetry metrics for motion capture frame. This method provides a standardized API interface by delegating to analyze_frame. Args: mocap_frame: (n_joints, 3) array of joint positions for one frame Returns: dict: Symmetry metrics containing: - overall_symmetry: Overall bilateral symmetry score (0-1) - phase_sync: Phase synchronization value - cca_correlation: Canonical correlation coefficient - joint_symmetries: Per-joint-pair symmetry metrics Source code in pyeyesweb/analysis_primitives/bilateral_symmetry.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def __call__ ( self , mocap_frame ): \"\"\" Compute bilateral symmetry metrics for motion capture frame. This method provides a standardized API interface by delegating to analyze_frame. Args: mocap_frame: (n_joints, 3) array of joint positions for one frame Returns: dict: Symmetry metrics containing: - overall_symmetry: Overall bilateral symmetry score (0-1) - phase_sync: Phase synchronization value - cca_correlation: Canonical correlation coefficient - joint_symmetries: Per-joint-pair symmetry metrics \"\"\" return self . analyze_frame ( mocap_frame ) __init__ ( window_size = 100 , joint_pairs = None , filter_params = None ) Initialize bilateral symmetry analyzer. Args: window_size: Number of frames for sliding window analysis joint_pairs: List of tuples defining bilateral joint pairs filter_params: Optional tuple of (lowcut_hz, highcut_hz, sampling_rate_hz) for band-pass filtering. If None, no filtering is applied. Source code in pyeyesweb/analysis_primitives/bilateral_symmetry.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , window_size = 100 , joint_pairs = None , filter_params = None ): \"\"\" Initialize bilateral symmetry analyzer. Args: window_size: Number of frames for sliding window analysis joint_pairs: List of tuples defining bilateral joint pairs filter_params: Optional tuple of (lowcut_hz, highcut_hz, sampling_rate_hz) for band-pass filtering. If None, no filtering is applied. \"\"\" self . window_size = validate_window_size ( window_size ) # Use ThreadSafeHistoryBuffer instead of deque + lock self . history = ThreadSafeHistoryBuffer ( maxlen = self . window_size ) # Default joint pairs for standard MoCap setup if joint_pairs is None : self . joint_pairs = [ ( 4 , 5 ), # left_shoulder, right_shoulder ( 6 , 7 ), # left_elbow, right_elbow ( 8 , 9 ), # left_wrist, right_wrist ( 10 , 11 ), # left_hip, right_hip ( 12 , 13 ), # left_knee, right_knee ( 14 , 15 ), # left_ankle, right_ankle ] else : self . joint_pairs = joint_pairs # validate and normalize filter params self . filter_params = validate_and_normalize_filter_params ( filter_params ) analyze_frame ( mocap_frame ) Analyze single frame of MoCap data for bilateral symmetry. Args: mocap_frame: (n_joints, 3) array of joint positions for one frame Returns: dict: Symmetry metrics for current frame Source code in pyeyesweb/analysis_primitives/bilateral_symmetry.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def analyze_frame ( self , mocap_frame ): \"\"\" Analyze single frame of MoCap data for bilateral symmetry. Args: mocap_frame: (n_joints, 3) array of joint positions for one frame Returns: dict: Symmetry metrics for current frame \"\"\" self . history . append ( mocap_frame ) history_length = len ( self . history ) if history_length < 10 : # Need minimum history for analysis return { 'overall_symmetry' : 0.0 , 'phase_sync' : 0.0 , 'cca_correlation' : 0.0 , 'joint_symmetries' : {} } # ThreadSafeHistoryBuffer provides thread-safe get_array method history_array = self . history . get_array () # (n_frames, n_joints, 3) joint_symmetries = {} symmetry_scores = [] phase_scores = [] cca_scores = [] # Analyze each bilateral joint pair for left_idx , right_idx in self . joint_pairs : left_joint_data = history_array [:, left_idx , :] right_joint_data = history_array [:, right_idx , :] # Compute multiple symmetry metrics bsi = self . _compute_bilateral_symmetry_index ( left_joint_data , right_joint_data ) # Use vertical movement for phase analysis (most relevant for gait) # Compute phase synchronization directly try : sig = np . column_stack ([ left_joint_data [:, 2 ], right_joint_data [:, 2 ]]) phase_sync = compute_phase_synchronization ( sig , self . filter_params ) except Exception as e : warnings . warn ( f \"Phase symmetry computation failed: { e } \" , RuntimeWarning ) phase_sync = np . nan cca_corr = self . _compute_cca_correlation ( left_joint_data , right_joint_data ) joint_pair_name = f \"joint_ { left_idx } _ { right_idx } \" joint_symmetries [ joint_pair_name ] = { 'bilateral_symmetry_index' : bsi , 'phase_synchronization' : phase_sync , 'cca_correlation' : cca_corr } symmetry_scores . append ( bsi ) phase_scores . append ( phase_sync ) cca_scores . append ( cca_corr ) # Compute overall metrics overall_symmetry = np . mean ( symmetry_scores ) if symmetry_scores else 0.0 overall_phase_sync = np . mean ( phase_scores ) if phase_scores else 0.0 overall_cca = np . mean ( cca_scores ) if cca_scores else 0.0 return { 'overall_symmetry' : overall_symmetry , 'phase_sync' : overall_phase_sync , 'cca_correlation' : overall_cca , 'joint_symmetries' : joint_symmetries }","title":"Bilateral Symmetry"},{"location":"API/analysis_primitives/bilateral_symmetry/#bilateral-symmetry","text":"Bilateral Symmetry Analysis for Motion Capture Data Below are a few research paper references: 1. \"Movement Symmetry Assessment by Bilateral Motion Data Fusion\" (2018) Link: https://pubmed.ncbi.nlm.nih.gov/29993408/ Provides Canonical Correlation Analysis (CCA) methodology and introduces bilateral motion data fusion concepts \"Symmetry Analysis of Manual Wheelchair Propulsion Using Motion Capture Techniques\" (2022) Link: https://www.mdpi.com/2073-8994/14/6/1164/htm Provides coefficient of variation calculation method and shows marker-based symmetry comparison approach General bilateral coordination research concepts from multiple biomechanics papers Phase synchronization using Hilbert transform and bilateral symmetry index calculations Note: Specific implementation details are adapted for a real-time MoCap analysis. Must be tested by us at CP.","title":"Bilateral Symmetry"},{"location":"API/analysis_primitives/bilateral_symmetry/#pyeyesweb.analysis_primitives.bilateral_symmetry.BilateralSymmetryAnalyzer","text":"Real-time bilateral symmetry analysis for motion capture data. Based on research methods from bilateral motion data fusion and wheelchair propulsion symmetry analysis papers. Read more in the User Guide . Source code in pyeyesweb/analysis_primitives/bilateral_symmetry.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 class BilateralSymmetryAnalyzer : \"\"\" Real-time bilateral symmetry analysis for motion capture data. Based on research methods from bilateral motion data fusion and wheelchair propulsion symmetry analysis papers. Read more in the [User Guide](/PyEyesWeb/user_guide/theoretical_framework/analysis_primitives/bilateral_symmetry/). \"\"\" def __init__ ( self , window_size = 100 , joint_pairs = None , filter_params = None ): \"\"\" Initialize bilateral symmetry analyzer. Args: window_size: Number of frames for sliding window analysis joint_pairs: List of tuples defining bilateral joint pairs filter_params: Optional tuple of (lowcut_hz, highcut_hz, sampling_rate_hz) for band-pass filtering. If None, no filtering is applied. \"\"\" self . window_size = validate_window_size ( window_size ) # Use ThreadSafeHistoryBuffer instead of deque + lock self . history = ThreadSafeHistoryBuffer ( maxlen = self . window_size ) # Default joint pairs for standard MoCap setup if joint_pairs is None : self . joint_pairs = [ ( 4 , 5 ), # left_shoulder, right_shoulder ( 6 , 7 ), # left_elbow, right_elbow ( 8 , 9 ), # left_wrist, right_wrist ( 10 , 11 ), # left_hip, right_hip ( 12 , 13 ), # left_knee, right_knee ( 14 , 15 ), # left_ankle, right_ankle ] else : self . joint_pairs = joint_pairs # validate and normalize filter params self . filter_params = validate_and_normalize_filter_params ( filter_params ) def _compute_bilateral_symmetry_index ( self , left_data , right_data ): \"\"\" Compute Bilateral Symmetry Index based on research methodology. Inspired by wheelchair propulsion research paper method: Compares mirrored bilateral movements and calculates relative differences. Args: left_data: (n_frames, 3) array of left joint positions right_data: (n_frames, 3) array of right joint positions Returns: float: Symmetry index (0-1, where 1 is perfect symmetry) \"\"\" # Mirror right side data across sagittal plane (flip x-coordinate) right_mirrored = right_data . copy () right_mirrored [:, 0 ] *= - 1 # Flip x-axis for bilateral comparison # Calculate relative differences (from wheelchair research) diff = np . abs ( left_data - right_mirrored ) sum_val = np . abs ( left_data ) + np . abs ( right_mirrored ) # Avoid division by zero sum_val = np . where ( sum_val == 0 , 1e-8 , sum_val ) # Relative asymmetry percentage relative_asymmetry = np . mean ( diff / sum_val ) * 100 # Convert to symmetry index (100% asymmetry = 0 symmetry) symmetry_index = max ( 0 , 1 - ( relative_asymmetry / 100 )) return symmetry_index def _compute_cca_correlation ( self , left_data , right_data ): \"\"\" Compute canonical correlation between bilateral data. Based on \"Movement Symmetry Assessment by Bilateral Motion Data Fusion\" paper methodology using Canonical Correlation Analysis. Args: left_data: (n_frames, 3) left joint data right_data: (n_frames, 3) right joint data Returns: float: Canonical correlation (0-1) \"\"\" if left_data . shape [ 0 ] < 5 : # Need minimum samples for CCA return np . nan # Not enough data for CCA try : # Flatten spatial coordinates for CCA analysis left_features = left_data . reshape ( left_data . shape [ 0 ], - 1 ) right_features = right_data . reshape ( right_data . shape [ 0 ], - 1 ) # Apply CCA with single component cca = CCA ( n_components = 1 ) left_c , right_c = cca . fit_transform ( left_features , right_features ) # Compute canonical correlation correlation = np . corrcoef ( left_c . flatten (), right_c . flatten ())[ 0 , 1 ] # Handle NaN cases if np . isnan ( correlation ): return np . nan # Correlation computation failed return abs ( correlation ) # Take absolute value except Exception as e : import warnings warnings . warn ( f \"CCA correlation computation failed: { e } \" , RuntimeWarning ) return np . nan def analyze_frame ( self , mocap_frame ): \"\"\" Analyze single frame of MoCap data for bilateral symmetry. Args: mocap_frame: (n_joints, 3) array of joint positions for one frame Returns: dict: Symmetry metrics for current frame \"\"\" self . history . append ( mocap_frame ) history_length = len ( self . history ) if history_length < 10 : # Need minimum history for analysis return { 'overall_symmetry' : 0.0 , 'phase_sync' : 0.0 , 'cca_correlation' : 0.0 , 'joint_symmetries' : {} } # ThreadSafeHistoryBuffer provides thread-safe get_array method history_array = self . history . get_array () # (n_frames, n_joints, 3) joint_symmetries = {} symmetry_scores = [] phase_scores = [] cca_scores = [] # Analyze each bilateral joint pair for left_idx , right_idx in self . joint_pairs : left_joint_data = history_array [:, left_idx , :] right_joint_data = history_array [:, right_idx , :] # Compute multiple symmetry metrics bsi = self . _compute_bilateral_symmetry_index ( left_joint_data , right_joint_data ) # Use vertical movement for phase analysis (most relevant for gait) # Compute phase synchronization directly try : sig = np . column_stack ([ left_joint_data [:, 2 ], right_joint_data [:, 2 ]]) phase_sync = compute_phase_synchronization ( sig , self . filter_params ) except Exception as e : warnings . warn ( f \"Phase symmetry computation failed: { e } \" , RuntimeWarning ) phase_sync = np . nan cca_corr = self . _compute_cca_correlation ( left_joint_data , right_joint_data ) joint_pair_name = f \"joint_ { left_idx } _ { right_idx } \" joint_symmetries [ joint_pair_name ] = { 'bilateral_symmetry_index' : bsi , 'phase_synchronization' : phase_sync , 'cca_correlation' : cca_corr } symmetry_scores . append ( bsi ) phase_scores . append ( phase_sync ) cca_scores . append ( cca_corr ) # Compute overall metrics overall_symmetry = np . mean ( symmetry_scores ) if symmetry_scores else 0.0 overall_phase_sync = np . mean ( phase_scores ) if phase_scores else 0.0 overall_cca = np . mean ( cca_scores ) if cca_scores else 0.0 return { 'overall_symmetry' : overall_symmetry , 'phase_sync' : overall_phase_sync , 'cca_correlation' : overall_cca , 'joint_symmetries' : joint_symmetries } def __call__ ( self , mocap_frame ): \"\"\" Compute bilateral symmetry metrics for motion capture frame. This method provides a standardized API interface by delegating to analyze_frame. Args: mocap_frame: (n_joints, 3) array of joint positions for one frame Returns: dict: Symmetry metrics containing: - overall_symmetry: Overall bilateral symmetry score (0-1) - phase_sync: Phase synchronization value - cca_correlation: Canonical correlation coefficient - joint_symmetries: Per-joint-pair symmetry metrics \"\"\" return self . analyze_frame ( mocap_frame )","title":"BilateralSymmetryAnalyzer"},{"location":"API/analysis_primitives/bilateral_symmetry/#pyeyesweb.analysis_primitives.bilateral_symmetry.BilateralSymmetryAnalyzer.__call__","text":"Compute bilateral symmetry metrics for motion capture frame. This method provides a standardized API interface by delegating to analyze_frame. Args: mocap_frame: (n_joints, 3) array of joint positions for one frame Returns: dict: Symmetry metrics containing: - overall_symmetry: Overall bilateral symmetry score (0-1) - phase_sync: Phase synchronization value - cca_correlation: Canonical correlation coefficient - joint_symmetries: Per-joint-pair symmetry metrics Source code in pyeyesweb/analysis_primitives/bilateral_symmetry.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def __call__ ( self , mocap_frame ): \"\"\" Compute bilateral symmetry metrics for motion capture frame. This method provides a standardized API interface by delegating to analyze_frame. Args: mocap_frame: (n_joints, 3) array of joint positions for one frame Returns: dict: Symmetry metrics containing: - overall_symmetry: Overall bilateral symmetry score (0-1) - phase_sync: Phase synchronization value - cca_correlation: Canonical correlation coefficient - joint_symmetries: Per-joint-pair symmetry metrics \"\"\" return self . analyze_frame ( mocap_frame )","title":"__call__"},{"location":"API/analysis_primitives/bilateral_symmetry/#pyeyesweb.analysis_primitives.bilateral_symmetry.BilateralSymmetryAnalyzer.__init__","text":"Initialize bilateral symmetry analyzer. Args: window_size: Number of frames for sliding window analysis joint_pairs: List of tuples defining bilateral joint pairs filter_params: Optional tuple of (lowcut_hz, highcut_hz, sampling_rate_hz) for band-pass filtering. If None, no filtering is applied. Source code in pyeyesweb/analysis_primitives/bilateral_symmetry.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , window_size = 100 , joint_pairs = None , filter_params = None ): \"\"\" Initialize bilateral symmetry analyzer. Args: window_size: Number of frames for sliding window analysis joint_pairs: List of tuples defining bilateral joint pairs filter_params: Optional tuple of (lowcut_hz, highcut_hz, sampling_rate_hz) for band-pass filtering. If None, no filtering is applied. \"\"\" self . window_size = validate_window_size ( window_size ) # Use ThreadSafeHistoryBuffer instead of deque + lock self . history = ThreadSafeHistoryBuffer ( maxlen = self . window_size ) # Default joint pairs for standard MoCap setup if joint_pairs is None : self . joint_pairs = [ ( 4 , 5 ), # left_shoulder, right_shoulder ( 6 , 7 ), # left_elbow, right_elbow ( 8 , 9 ), # left_wrist, right_wrist ( 10 , 11 ), # left_hip, right_hip ( 12 , 13 ), # left_knee, right_knee ( 14 , 15 ), # left_ankle, right_ankle ] else : self . joint_pairs = joint_pairs # validate and normalize filter params self . filter_params = validate_and_normalize_filter_params ( filter_params )","title":"__init__"},{"location":"API/analysis_primitives/bilateral_symmetry/#pyeyesweb.analysis_primitives.bilateral_symmetry.BilateralSymmetryAnalyzer.analyze_frame","text":"Analyze single frame of MoCap data for bilateral symmetry. Args: mocap_frame: (n_joints, 3) array of joint positions for one frame Returns: dict: Symmetry metrics for current frame Source code in pyeyesweb/analysis_primitives/bilateral_symmetry.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def analyze_frame ( self , mocap_frame ): \"\"\" Analyze single frame of MoCap data for bilateral symmetry. Args: mocap_frame: (n_joints, 3) array of joint positions for one frame Returns: dict: Symmetry metrics for current frame \"\"\" self . history . append ( mocap_frame ) history_length = len ( self . history ) if history_length < 10 : # Need minimum history for analysis return { 'overall_symmetry' : 0.0 , 'phase_sync' : 0.0 , 'cca_correlation' : 0.0 , 'joint_symmetries' : {} } # ThreadSafeHistoryBuffer provides thread-safe get_array method history_array = self . history . get_array () # (n_frames, n_joints, 3) joint_symmetries = {} symmetry_scores = [] phase_scores = [] cca_scores = [] # Analyze each bilateral joint pair for left_idx , right_idx in self . joint_pairs : left_joint_data = history_array [:, left_idx , :] right_joint_data = history_array [:, right_idx , :] # Compute multiple symmetry metrics bsi = self . _compute_bilateral_symmetry_index ( left_joint_data , right_joint_data ) # Use vertical movement for phase analysis (most relevant for gait) # Compute phase synchronization directly try : sig = np . column_stack ([ left_joint_data [:, 2 ], right_joint_data [:, 2 ]]) phase_sync = compute_phase_synchronization ( sig , self . filter_params ) except Exception as e : warnings . warn ( f \"Phase symmetry computation failed: { e } \" , RuntimeWarning ) phase_sync = np . nan cca_corr = self . _compute_cca_correlation ( left_joint_data , right_joint_data ) joint_pair_name = f \"joint_ { left_idx } _ { right_idx } \" joint_symmetries [ joint_pair_name ] = { 'bilateral_symmetry_index' : bsi , 'phase_synchronization' : phase_sync , 'cca_correlation' : cca_corr } symmetry_scores . append ( bsi ) phase_scores . append ( phase_sync ) cca_scores . append ( cca_corr ) # Compute overall metrics overall_symmetry = np . mean ( symmetry_scores ) if symmetry_scores else 0.0 overall_phase_sync = np . mean ( phase_scores ) if phase_scores else 0.0 overall_cca = np . mean ( cca_scores ) if cca_scores else 0.0 return { 'overall_symmetry' : overall_symmetry , 'phase_sync' : overall_phase_sync , 'cca_correlation' : overall_cca , 'joint_symmetries' : joint_symmetries }","title":"analyze_frame"},{"location":"API/analysis_primitives/clusterability/","text":"Clusterability Clusterability Compute clusterability metric. Clusterability measures how strongly a dataset tends to form clusters rather than being randomly distributed. Parameters: n_neighbors ( int ) \u2013 Number of nearest neighbors used in the Hopkins statistic computation. Notes The Hopkins statistic is a commonly used measure of clusterability. It compares the distances of points in the dataset to their nearest neighbors with distances from uniformly distributed random points to their nearest neighbors in the dataset. If points are aggregated, Clusterability approached 1, whereas a value close to 0.5 suggests randomness. Read more in the User Guide References Lawson, R. G., & Jurs, P. C. (1990). New index for clustering tendency and its application to chemical problems. Journal of chemical information and computer sciences, 30(1), 36-41. Source code in pyeyesweb/analysis_primitives/clusterability.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class Clusterability : \"\"\" Compute clusterability metric. Clusterability measures how strongly a dataset tends to form clusters rather than being randomly distributed. Parameters ---------- n_neighbors : int Number of nearest neighbors used in the Hopkins statistic computation. Notes ----- The Hopkins statistic is a commonly used measure of clusterability. It compares the distances of points in the dataset to their nearest neighbors with distances from uniformly distributed random points to their nearest neighbors in the dataset. If points are aggregated, Clusterability approached 1, whereas a value close to 0.5 suggests randomness. Read more in the [User Guide](/PyEyesWeb/user_guide/theoretical_framework/analysis_primitives/clusterability/) References ---------- Lawson, R. G., & Jurs, P. C. (1990). New index for clustering tendency and its application to chemical problems. Journal of chemical information and computer sciences, 30(1), 36-41. \"\"\" def __init__ ( self , n_neighbors : int ) -> None : \"\"\" Initialize the Clusterability object. Parameters ---------- n_neighbors : int Number of nearest neighbors to use in the Hopkins statistic computation. random_state : int, optional Random seed for reproducibility. Default is None. \"\"\" self . n_neighbors = n_neighbors def compute_hopkins_statistic ( self , data : np . ndarray ) -> float : \"\"\" Compute the Hopkins statistic for a given dataset. Parameters ---------- data : np.ndarray Input data of shape (n_samples, n_features). Returns ------- float Hopkins statistic value. Returns NaN if data is insufficient or invalid. \"\"\" if data . shape [ 0 ] < 5 : return np . nan # Generate uniform random sample within data bounds mins = np . min ( data , axis = 0 ) maxs = np . max ( data , axis = 0 ) # Uniform random sample uniform_sample = np . random . uniform ( mins , maxs , size = data . shape ) # Compute nearest neighbor distances n_neighbors = min ( data . shape [ 0 ], self . n_neighbors ) neighbors = NearestNeighbors ( n_neighbors = n_neighbors ) . fit ( data ) # Distances from data points to their nearest neighbors data_distances , _ = neighbors . kneighbors ( data ) u = np . sum ( data_distances [:, 1 ]) # exclude self-distance (0) # Distances from uniform sample points to their nearest neighbors uniform_distances , _ = neighbors . kneighbors ( uniform_sample ) w = np . sum ( uniform_distances [:, 0 ]) hopkins_stat = w / ( u + w + 1e-10 ) return float ( hopkins_stat ) def compute_clusterability ( self , signals : SlidingWindow ) -> Dict [ str , float ]: \"\"\" Compute the clusterability of a sliding window of signals using the Hopkins statistic. Parameters ---------- signals : SlidingWindow A sliding window object containing signal data. Returns ------- dict Dictionary containing: - 'hopkins_statistic' (float): Computed Hopkins statistic. Returns NaN if the window is not full or computation fails. \"\"\" if not signals . is_full (): return { \"clusterability\" : np . nan } try : data , _ = signals . to_array () hopkins_value = self . compute_hopkins_statistic ( data ) except Exception : # TODO: add logging for better traceability hopkins_value = np . nan return { \"clusterability\" : hopkins_value } def __call__ ( self , sliding_window : SlidingWindow ) -> Dict [ str , float ]: \"\"\" Callable interface to compute clusterability directly on a SlidingWindow instance. Parameters ---------- sliding_window : SlidingWindow The sliding window object containing the data. Returns ------- dict Output of `compute_clusterability`. \"\"\" return self . compute_clusterability ( sliding_window ) __call__ ( sliding_window ) Callable interface to compute clusterability directly on a SlidingWindow instance. Parameters: sliding_window ( SlidingWindow ) \u2013 The sliding window object containing the data. Returns: dict \u2013 Output of compute_clusterability . Source code in pyeyesweb/analysis_primitives/clusterability.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def __call__ ( self , sliding_window : SlidingWindow ) -> Dict [ str , float ]: \"\"\" Callable interface to compute clusterability directly on a SlidingWindow instance. Parameters ---------- sliding_window : SlidingWindow The sliding window object containing the data. Returns ------- dict Output of `compute_clusterability`. \"\"\" return self . compute_clusterability ( sliding_window ) __init__ ( n_neighbors ) Initialize the Clusterability object. Parameters: n_neighbors ( int ) \u2013 Number of nearest neighbors to use in the Hopkins statistic computation. random_state ( int ) \u2013 Random seed for reproducibility. Default is None. Source code in pyeyesweb/analysis_primitives/clusterability.py 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , n_neighbors : int ) -> None : \"\"\" Initialize the Clusterability object. Parameters ---------- n_neighbors : int Number of nearest neighbors to use in the Hopkins statistic computation. random_state : int, optional Random seed for reproducibility. Default is None. \"\"\" self . n_neighbors = n_neighbors compute_clusterability ( signals ) Compute the clusterability of a sliding window of signals using the Hopkins statistic. Parameters: signals ( SlidingWindow ) \u2013 A sliding window object containing signal data. Returns: dict \u2013 Dictionary containing: - 'hopkins_statistic' (float): Computed Hopkins statistic. Returns NaN if the window is not full or computation fails. Source code in pyeyesweb/analysis_primitives/clusterability.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def compute_clusterability ( self , signals : SlidingWindow ) -> Dict [ str , float ]: \"\"\" Compute the clusterability of a sliding window of signals using the Hopkins statistic. Parameters ---------- signals : SlidingWindow A sliding window object containing signal data. Returns ------- dict Dictionary containing: - 'hopkins_statistic' (float): Computed Hopkins statistic. Returns NaN if the window is not full or computation fails. \"\"\" if not signals . is_full (): return { \"clusterability\" : np . nan } try : data , _ = signals . to_array () hopkins_value = self . compute_hopkins_statistic ( data ) except Exception : # TODO: add logging for better traceability hopkins_value = np . nan return { \"clusterability\" : hopkins_value } compute_hopkins_statistic ( data ) Compute the Hopkins statistic for a given dataset. Parameters: data ( ndarray ) \u2013 Input data of shape (n_samples, n_features). Returns: float \u2013 Hopkins statistic value. Returns NaN if data is insufficient or invalid. Source code in pyeyesweb/analysis_primitives/clusterability.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def compute_hopkins_statistic ( self , data : np . ndarray ) -> float : \"\"\" Compute the Hopkins statistic for a given dataset. Parameters ---------- data : np.ndarray Input data of shape (n_samples, n_features). Returns ------- float Hopkins statistic value. Returns NaN if data is insufficient or invalid. \"\"\" if data . shape [ 0 ] < 5 : return np . nan # Generate uniform random sample within data bounds mins = np . min ( data , axis = 0 ) maxs = np . max ( data , axis = 0 ) # Uniform random sample uniform_sample = np . random . uniform ( mins , maxs , size = data . shape ) # Compute nearest neighbor distances n_neighbors = min ( data . shape [ 0 ], self . n_neighbors ) neighbors = NearestNeighbors ( n_neighbors = n_neighbors ) . fit ( data ) # Distances from data points to their nearest neighbors data_distances , _ = neighbors . kneighbors ( data ) u = np . sum ( data_distances [:, 1 ]) # exclude self-distance (0) # Distances from uniform sample points to their nearest neighbors uniform_distances , _ = neighbors . kneighbors ( uniform_sample ) w = np . sum ( uniform_distances [:, 0 ]) hopkins_stat = w / ( u + w + 1e-10 ) return float ( hopkins_stat )","title":"Clusterability"},{"location":"API/analysis_primitives/clusterability/#clusterability","text":"","title":"Clusterability"},{"location":"API/analysis_primitives/clusterability/#pyeyesweb.analysis_primitives.clusterability.Clusterability","text":"Compute clusterability metric. Clusterability measures how strongly a dataset tends to form clusters rather than being randomly distributed. Parameters: n_neighbors ( int ) \u2013 Number of nearest neighbors used in the Hopkins statistic computation. Notes The Hopkins statistic is a commonly used measure of clusterability. It compares the distances of points in the dataset to their nearest neighbors with distances from uniformly distributed random points to their nearest neighbors in the dataset. If points are aggregated, Clusterability approached 1, whereas a value close to 0.5 suggests randomness. Read more in the User Guide References Lawson, R. G., & Jurs, P. C. (1990). New index for clustering tendency and its application to chemical problems. Journal of chemical information and computer sciences, 30(1), 36-41. Source code in pyeyesweb/analysis_primitives/clusterability.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class Clusterability : \"\"\" Compute clusterability metric. Clusterability measures how strongly a dataset tends to form clusters rather than being randomly distributed. Parameters ---------- n_neighbors : int Number of nearest neighbors used in the Hopkins statistic computation. Notes ----- The Hopkins statistic is a commonly used measure of clusterability. It compares the distances of points in the dataset to their nearest neighbors with distances from uniformly distributed random points to their nearest neighbors in the dataset. If points are aggregated, Clusterability approached 1, whereas a value close to 0.5 suggests randomness. Read more in the [User Guide](/PyEyesWeb/user_guide/theoretical_framework/analysis_primitives/clusterability/) References ---------- Lawson, R. G., & Jurs, P. C. (1990). New index for clustering tendency and its application to chemical problems. Journal of chemical information and computer sciences, 30(1), 36-41. \"\"\" def __init__ ( self , n_neighbors : int ) -> None : \"\"\" Initialize the Clusterability object. Parameters ---------- n_neighbors : int Number of nearest neighbors to use in the Hopkins statistic computation. random_state : int, optional Random seed for reproducibility. Default is None. \"\"\" self . n_neighbors = n_neighbors def compute_hopkins_statistic ( self , data : np . ndarray ) -> float : \"\"\" Compute the Hopkins statistic for a given dataset. Parameters ---------- data : np.ndarray Input data of shape (n_samples, n_features). Returns ------- float Hopkins statistic value. Returns NaN if data is insufficient or invalid. \"\"\" if data . shape [ 0 ] < 5 : return np . nan # Generate uniform random sample within data bounds mins = np . min ( data , axis = 0 ) maxs = np . max ( data , axis = 0 ) # Uniform random sample uniform_sample = np . random . uniform ( mins , maxs , size = data . shape ) # Compute nearest neighbor distances n_neighbors = min ( data . shape [ 0 ], self . n_neighbors ) neighbors = NearestNeighbors ( n_neighbors = n_neighbors ) . fit ( data ) # Distances from data points to their nearest neighbors data_distances , _ = neighbors . kneighbors ( data ) u = np . sum ( data_distances [:, 1 ]) # exclude self-distance (0) # Distances from uniform sample points to their nearest neighbors uniform_distances , _ = neighbors . kneighbors ( uniform_sample ) w = np . sum ( uniform_distances [:, 0 ]) hopkins_stat = w / ( u + w + 1e-10 ) return float ( hopkins_stat ) def compute_clusterability ( self , signals : SlidingWindow ) -> Dict [ str , float ]: \"\"\" Compute the clusterability of a sliding window of signals using the Hopkins statistic. Parameters ---------- signals : SlidingWindow A sliding window object containing signal data. Returns ------- dict Dictionary containing: - 'hopkins_statistic' (float): Computed Hopkins statistic. Returns NaN if the window is not full or computation fails. \"\"\" if not signals . is_full (): return { \"clusterability\" : np . nan } try : data , _ = signals . to_array () hopkins_value = self . compute_hopkins_statistic ( data ) except Exception : # TODO: add logging for better traceability hopkins_value = np . nan return { \"clusterability\" : hopkins_value } def __call__ ( self , sliding_window : SlidingWindow ) -> Dict [ str , float ]: \"\"\" Callable interface to compute clusterability directly on a SlidingWindow instance. Parameters ---------- sliding_window : SlidingWindow The sliding window object containing the data. Returns ------- dict Output of `compute_clusterability`. \"\"\" return self . compute_clusterability ( sliding_window )","title":"Clusterability"},{"location":"API/analysis_primitives/clusterability/#pyeyesweb.analysis_primitives.clusterability.Clusterability.__call__","text":"Callable interface to compute clusterability directly on a SlidingWindow instance. Parameters: sliding_window ( SlidingWindow ) \u2013 The sliding window object containing the data. Returns: dict \u2013 Output of compute_clusterability . Source code in pyeyesweb/analysis_primitives/clusterability.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def __call__ ( self , sliding_window : SlidingWindow ) -> Dict [ str , float ]: \"\"\" Callable interface to compute clusterability directly on a SlidingWindow instance. Parameters ---------- sliding_window : SlidingWindow The sliding window object containing the data. Returns ------- dict Output of `compute_clusterability`. \"\"\" return self . compute_clusterability ( sliding_window )","title":"__call__"},{"location":"API/analysis_primitives/clusterability/#pyeyesweb.analysis_primitives.clusterability.Clusterability.__init__","text":"Initialize the Clusterability object. Parameters: n_neighbors ( int ) \u2013 Number of nearest neighbors to use in the Hopkins statistic computation. random_state ( int ) \u2013 Random seed for reproducibility. Default is None. Source code in pyeyesweb/analysis_primitives/clusterability.py 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , n_neighbors : int ) -> None : \"\"\" Initialize the Clusterability object. Parameters ---------- n_neighbors : int Number of nearest neighbors to use in the Hopkins statistic computation. random_state : int, optional Random seed for reproducibility. Default is None. \"\"\" self . n_neighbors = n_neighbors","title":"__init__"},{"location":"API/analysis_primitives/clusterability/#pyeyesweb.analysis_primitives.clusterability.Clusterability.compute_clusterability","text":"Compute the clusterability of a sliding window of signals using the Hopkins statistic. Parameters: signals ( SlidingWindow ) \u2013 A sliding window object containing signal data. Returns: dict \u2013 Dictionary containing: - 'hopkins_statistic' (float): Computed Hopkins statistic. Returns NaN if the window is not full or computation fails. Source code in pyeyesweb/analysis_primitives/clusterability.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def compute_clusterability ( self , signals : SlidingWindow ) -> Dict [ str , float ]: \"\"\" Compute the clusterability of a sliding window of signals using the Hopkins statistic. Parameters ---------- signals : SlidingWindow A sliding window object containing signal data. Returns ------- dict Dictionary containing: - 'hopkins_statistic' (float): Computed Hopkins statistic. Returns NaN if the window is not full or computation fails. \"\"\" if not signals . is_full (): return { \"clusterability\" : np . nan } try : data , _ = signals . to_array () hopkins_value = self . compute_hopkins_statistic ( data ) except Exception : # TODO: add logging for better traceability hopkins_value = np . nan return { \"clusterability\" : hopkins_value }","title":"compute_clusterability"},{"location":"API/analysis_primitives/clusterability/#pyeyesweb.analysis_primitives.clusterability.Clusterability.compute_hopkins_statistic","text":"Compute the Hopkins statistic for a given dataset. Parameters: data ( ndarray ) \u2013 Input data of shape (n_samples, n_features). Returns: float \u2013 Hopkins statistic value. Returns NaN if data is insufficient or invalid. Source code in pyeyesweb/analysis_primitives/clusterability.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def compute_hopkins_statistic ( self , data : np . ndarray ) -> float : \"\"\" Compute the Hopkins statistic for a given dataset. Parameters ---------- data : np.ndarray Input data of shape (n_samples, n_features). Returns ------- float Hopkins statistic value. Returns NaN if data is insufficient or invalid. \"\"\" if data . shape [ 0 ] < 5 : return np . nan # Generate uniform random sample within data bounds mins = np . min ( data , axis = 0 ) maxs = np . max ( data , axis = 0 ) # Uniform random sample uniform_sample = np . random . uniform ( mins , maxs , size = data . shape ) # Compute nearest neighbor distances n_neighbors = min ( data . shape [ 0 ], self . n_neighbors ) neighbors = NearestNeighbors ( n_neighbors = n_neighbors ) . fit ( data ) # Distances from data points to their nearest neighbors data_distances , _ = neighbors . kneighbors ( data ) u = np . sum ( data_distances [:, 1 ]) # exclude self-distance (0) # Distances from uniform sample points to their nearest neighbors uniform_distances , _ = neighbors . kneighbors ( uniform_sample ) w = np . sum ( uniform_distances [:, 0 ]) hopkins_stat = w / ( u + w + 1e-10 ) return float ( hopkins_stat )","title":"compute_hopkins_statistic"},{"location":"API/analysis_primitives/mse_dominance/","text":"Mse Dominance Multi-scale entropy analysis module for dominance detection in ensemble performances. This module implements the multi-scale entropy (MSE) algorithm for analyzing dominance and leadership in social creative activities. The method quantifies the complexity of movement dynamics across multiple time scales to identify leadership patterns in musical ensembles. The multi-scale entropy algorithm includes: 1. Coarse-graining procedure for multi-scale signal representation 2. Sample entropy calculation for irregularity quantification 3. Complexity index computation across scales 4. Dominance analysis based on complexity differences Typical use cases include: 1. Leadership detection in string quartet performances 2. Dominance analysis in social creative interactions 3. Group coordination pattern analysis 4. Movement complexity characterization 5. Real-time ensemble performance monitoring References Glowinski, D., Coletta, P., Volpe, G., Camurri, A., Chiorri, C., & Schenone, A. (2010). Multi-scale entropy analysis of dominance in social creative activities. In Proceedings of the 18th ACM international conference on Multimedia (pp. 1035-1038). Costa, M., Goldberger, A. L., & Peng, C.-K. (2005). Multiscale entropy analysis of biological signals. Physical Review E, 71(2), 021906. MultiScaleEntropyDominance Real-time multi-scale entropy analyzer for dominance detection. This class implements the multi-scale entropy algorithm to analyze dominance in ensemble performances by computing complexity indices of movement dynamics across multiple time scales. The algorithm follows the methodology described in: Glowinski et al. (2010). Multi-scale entropy analysis of dominance in social creative activities. ACM Multimedia, 1035-1038. Source code in pyeyesweb/analysis_primitives/mse_dominance.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 class MultiScaleEntropyDominance : \"\"\"Real-time multi-scale entropy analyzer for dominance detection. This class implements the multi-scale entropy algorithm to analyze dominance in ensemble performances by computing complexity indices of movement dynamics across multiple time scales. The algorithm follows the methodology described in: Glowinski et al. (2010). Multi-scale entropy analysis of dominance in social creative activities. ACM Multimedia, 1035-1038. \"\"\" def __init__ ( self ): \"\"\"Initialize the multi-scale entropy analyzer with default parameters.\"\"\" # Algorithm parameters as per reference papers self . m = 2 # Embedding dimension for sample entropy self . r = 0.15 # Tolerance parameter (15% of standard deviation) self . max_scale = 6 # Maximum scale factor for coarse-graining self . min_points = 500 # Minimum data points required per scale def _coarse_grain ( self , data : np . ndarray , scale : int ) -> np . ndarray : \"\"\"Apply coarse-graining procedure to time series data. Implements the exact equation from Costa et al. (2005): y_j^(\u03c4) = (1/\u03c4) * \u03a3_{i=(j-1)\u03c4+1}^{j\u03c4} x_i, for j=1..floor(N/\u03c4) Parameters ---------- data : np.ndarray Input time series data (1D array) scale : int Scale factor for coarse-graining (\u03c4 in the equation) Returns ------- np.ndarray Coarse-grained time series \"\"\" if data is None or data . size == 0 : return np . array ([], dtype = float ) x = np . asarray ( data , dtype = float ) . ravel () if scale is None or scale < 1 : return np . array ([], dtype = float ) if scale == 1 : return x N = x . shape [ 0 ] if N < scale : return np . array ([], dtype = float ) # Calculate number of complete blocks num_points = N // scale # Trim data to complete blocks trimmed = x [: num_points * scale ] # Reshape and average: each block becomes one point coarse = trimmed . reshape ( num_points , scale ) . mean ( axis = 1 ) return coarse def _sample_entropy ( self , data : np . ndarray ) -> float : \"\"\"Calculate sample entropy (SampEn) for a time series. Parameters ---------- data : np.ndarray Input time series data (1D array) Returns ------- float Sample entropy value, or 0.0 if insufficient data \"\"\" x = np . asarray ( data , dtype = float ) . reshape ( - 1 ) N = x . shape [ 0 ] m = int ( self . m ) r = float ( self . r ) if N <= m + 10 : return 0.0 mu = float ( np . mean ( x )) sd = float ( np . std ( x )) if sd < 1e-10 : return 0.0 u = ( x - mu ) / sd templates_m = np . array ([ u [ i : i + m ] for i in range ( N - m )], dtype = float ) templates_m1 = np . array ([ u [ i : i + m + 1 ] for i in range ( N - m - 1 )], dtype = float ) n_m = templates_m . shape [ 0 ] n_m1 = templates_m1 . shape [ 0 ] if n_m <= 1 or n_m1 <= 1 : return 0.0 B_matches = 0 A_matches = 0 for i in range ( n_m ): dist = np . max ( np . abs ( templates_m [ i ] - templates_m ), axis = 1 ) B_matches += int ( np . sum ( dist < r ) - 1 ) for i in range ( n_m1 ): dist = np . max ( np . abs ( templates_m1 [ i ] - templates_m1 ), axis = 1 ) A_matches += int ( np . sum ( dist < r ) - 1 ) if B_matches <= 0 or A_matches <= 0 : return 0.0 B = B_matches / ( n_m * ( n_m - 1 )) A = A_matches / ( n_m1 * ( n_m1 - 1 )) if A <= 0 or B <= 0 : return 0.0 return float ( - np . log ( A / B )) def _calculate_complexity_index ( self , data : np . ndarray ) -> float : \"\"\"Calculate complexity index by integrating sample entropy across scales. Parameters ---------- data : np.ndarray Input time series data Returns ------- float Complexity index value \"\"\" sampen_values = [] for scale in range ( 1 , int ( self . max_scale ) + 1 ): coarse = self . _coarse_grain ( data , scale ) if coarse . shape [ 0 ] < int ( self . min_points ): break sampen = self . _sample_entropy ( coarse ) sampen_values . append ( sampen ) if len ( sampen_values ) > 1 : scales = np . arange ( 1 , len ( sampen_values ) + 1 , dtype = float ) return float ( np . trapz ( np . asarray ( sampen_values , dtype = float ), x = scales )) if len ( sampen_values ) == 1 : return float ( sampen_values [ 0 ]) return 0.0 def compute_analysis ( self , signals : SlidingWindow , methods : list ) -> dict : \"\"\"Compute dominance analysis for ensemble performance data. Parameters ---------- signals : SlidingWindow Sliding window buffer containing movement velocity data. methods : list of str List of analysis methods to compute. Available options: 'complexity_index', 'dominance_score', 'leader_identification' Returns ------- dict Dictionary containing dominance analysis results. \"\"\" if not signals . is_full (): return {} data , _ = signals . to_array () n_samples , n_features = data . shape if n_samples < int ( self . min_points ): return {} complexity_indices = [] for i in range ( n_features ): ci = self . _calculate_complexity_index ( data [:, i ]) complexity_indices . append ( ci ) result = {} for method in methods : if method == 'complexity_index' : values = np . array ( complexity_indices , dtype = float ) result [ 'complexity_index' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'dominance_score' : cis = np . array ( complexity_indices , dtype = float ) if cis . size > 0 : max_ci = float ( np . max ( cis )) if max_ci > 0 : scores = ( 1.0 - ( cis / max_ci )) else : scores = np . zeros_like ( cis ) result [ 'dominance_score' ] = float ( scores [ 0 ]) if len ( scores ) == 1 else scores . tolist () elif method == 'leader_identification' : if complexity_indices : leader_idx = np . argmin ( complexity_indices ) result [ 'leader' ] = int ( leader_idx ) result [ 'leader_complexity' ] = float ( complexity_indices [ leader_idx ]) else : continue return result def __call__ ( self , sliding_window : SlidingWindow , methods : list ) -> dict : \"\"\"Compute dominance analysis metrics. Parameters ---------- sliding_window : SlidingWindow Buffer containing multivariate data to analyze. methods : list of str List of analysis methods to compute. Returns ------- dict Dictionary containing dominance analysis metrics. \"\"\" return self . compute_analysis ( sliding_window , methods ) __call__ ( sliding_window , methods ) Compute dominance analysis metrics. Parameters: sliding_window ( SlidingWindow ) \u2013 Buffer containing multivariate data to analyze. methods ( list of str ) \u2013 List of analysis methods to compute. Returns: dict \u2013 Dictionary containing dominance analysis metrics. Source code in pyeyesweb/analysis_primitives/mse_dominance.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def __call__ ( self , sliding_window : SlidingWindow , methods : list ) -> dict : \"\"\"Compute dominance analysis metrics. Parameters ---------- sliding_window : SlidingWindow Buffer containing multivariate data to analyze. methods : list of str List of analysis methods to compute. Returns ------- dict Dictionary containing dominance analysis metrics. \"\"\" return self . compute_analysis ( sliding_window , methods ) __init__ () Initialize the multi-scale entropy analyzer with default parameters. Source code in pyeyesweb/analysis_primitives/mse_dominance.py 48 49 50 51 52 53 54 def __init__ ( self ): \"\"\"Initialize the multi-scale entropy analyzer with default parameters.\"\"\" # Algorithm parameters as per reference papers self . m = 2 # Embedding dimension for sample entropy self . r = 0.15 # Tolerance parameter (15% of standard deviation) self . max_scale = 6 # Maximum scale factor for coarse-graining self . min_points = 500 # Minimum data points required per scale compute_analysis ( signals , methods ) Compute dominance analysis for ensemble performance data. Parameters: signals ( SlidingWindow ) \u2013 Sliding window buffer containing movement velocity data. methods ( list of str ) \u2013 List of analysis methods to compute. Available options: 'complexity_index', 'dominance_score', 'leader_identification' Returns: dict \u2013 Dictionary containing dominance analysis results. Source code in pyeyesweb/analysis_primitives/mse_dominance.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def compute_analysis ( self , signals : SlidingWindow , methods : list ) -> dict : \"\"\"Compute dominance analysis for ensemble performance data. Parameters ---------- signals : SlidingWindow Sliding window buffer containing movement velocity data. methods : list of str List of analysis methods to compute. Available options: 'complexity_index', 'dominance_score', 'leader_identification' Returns ------- dict Dictionary containing dominance analysis results. \"\"\" if not signals . is_full (): return {} data , _ = signals . to_array () n_samples , n_features = data . shape if n_samples < int ( self . min_points ): return {} complexity_indices = [] for i in range ( n_features ): ci = self . _calculate_complexity_index ( data [:, i ]) complexity_indices . append ( ci ) result = {} for method in methods : if method == 'complexity_index' : values = np . array ( complexity_indices , dtype = float ) result [ 'complexity_index' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'dominance_score' : cis = np . array ( complexity_indices , dtype = float ) if cis . size > 0 : max_ci = float ( np . max ( cis )) if max_ci > 0 : scores = ( 1.0 - ( cis / max_ci )) else : scores = np . zeros_like ( cis ) result [ 'dominance_score' ] = float ( scores [ 0 ]) if len ( scores ) == 1 else scores . tolist () elif method == 'leader_identification' : if complexity_indices : leader_idx = np . argmin ( complexity_indices ) result [ 'leader' ] = int ( leader_idx ) result [ 'leader_complexity' ] = float ( complexity_indices [ leader_idx ]) else : continue return result","title":"Mse Dominance"},{"location":"API/analysis_primitives/mse_dominance/#mse-dominance","text":"Multi-scale entropy analysis module for dominance detection in ensemble performances. This module implements the multi-scale entropy (MSE) algorithm for analyzing dominance and leadership in social creative activities. The method quantifies the complexity of movement dynamics across multiple time scales to identify leadership patterns in musical ensembles. The multi-scale entropy algorithm includes: 1. Coarse-graining procedure for multi-scale signal representation 2. Sample entropy calculation for irregularity quantification 3. Complexity index computation across scales 4. Dominance analysis based on complexity differences Typical use cases include: 1. Leadership detection in string quartet performances 2. Dominance analysis in social creative interactions 3. Group coordination pattern analysis 4. Movement complexity characterization 5. Real-time ensemble performance monitoring References Glowinski, D., Coletta, P., Volpe, G., Camurri, A., Chiorri, C., & Schenone, A. (2010). Multi-scale entropy analysis of dominance in social creative activities. In Proceedings of the 18th ACM international conference on Multimedia (pp. 1035-1038). Costa, M., Goldberger, A. L., & Peng, C.-K. (2005). Multiscale entropy analysis of biological signals. Physical Review E, 71(2), 021906.","title":"Mse Dominance"},{"location":"API/analysis_primitives/mse_dominance/#pyeyesweb.analysis_primitives.mse_dominance.MultiScaleEntropyDominance","text":"Real-time multi-scale entropy analyzer for dominance detection. This class implements the multi-scale entropy algorithm to analyze dominance in ensemble performances by computing complexity indices of movement dynamics across multiple time scales. The algorithm follows the methodology described in: Glowinski et al. (2010). Multi-scale entropy analysis of dominance in social creative activities. ACM Multimedia, 1035-1038. Source code in pyeyesweb/analysis_primitives/mse_dominance.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 class MultiScaleEntropyDominance : \"\"\"Real-time multi-scale entropy analyzer for dominance detection. This class implements the multi-scale entropy algorithm to analyze dominance in ensemble performances by computing complexity indices of movement dynamics across multiple time scales. The algorithm follows the methodology described in: Glowinski et al. (2010). Multi-scale entropy analysis of dominance in social creative activities. ACM Multimedia, 1035-1038. \"\"\" def __init__ ( self ): \"\"\"Initialize the multi-scale entropy analyzer with default parameters.\"\"\" # Algorithm parameters as per reference papers self . m = 2 # Embedding dimension for sample entropy self . r = 0.15 # Tolerance parameter (15% of standard deviation) self . max_scale = 6 # Maximum scale factor for coarse-graining self . min_points = 500 # Minimum data points required per scale def _coarse_grain ( self , data : np . ndarray , scale : int ) -> np . ndarray : \"\"\"Apply coarse-graining procedure to time series data. Implements the exact equation from Costa et al. (2005): y_j^(\u03c4) = (1/\u03c4) * \u03a3_{i=(j-1)\u03c4+1}^{j\u03c4} x_i, for j=1..floor(N/\u03c4) Parameters ---------- data : np.ndarray Input time series data (1D array) scale : int Scale factor for coarse-graining (\u03c4 in the equation) Returns ------- np.ndarray Coarse-grained time series \"\"\" if data is None or data . size == 0 : return np . array ([], dtype = float ) x = np . asarray ( data , dtype = float ) . ravel () if scale is None or scale < 1 : return np . array ([], dtype = float ) if scale == 1 : return x N = x . shape [ 0 ] if N < scale : return np . array ([], dtype = float ) # Calculate number of complete blocks num_points = N // scale # Trim data to complete blocks trimmed = x [: num_points * scale ] # Reshape and average: each block becomes one point coarse = trimmed . reshape ( num_points , scale ) . mean ( axis = 1 ) return coarse def _sample_entropy ( self , data : np . ndarray ) -> float : \"\"\"Calculate sample entropy (SampEn) for a time series. Parameters ---------- data : np.ndarray Input time series data (1D array) Returns ------- float Sample entropy value, or 0.0 if insufficient data \"\"\" x = np . asarray ( data , dtype = float ) . reshape ( - 1 ) N = x . shape [ 0 ] m = int ( self . m ) r = float ( self . r ) if N <= m + 10 : return 0.0 mu = float ( np . mean ( x )) sd = float ( np . std ( x )) if sd < 1e-10 : return 0.0 u = ( x - mu ) / sd templates_m = np . array ([ u [ i : i + m ] for i in range ( N - m )], dtype = float ) templates_m1 = np . array ([ u [ i : i + m + 1 ] for i in range ( N - m - 1 )], dtype = float ) n_m = templates_m . shape [ 0 ] n_m1 = templates_m1 . shape [ 0 ] if n_m <= 1 or n_m1 <= 1 : return 0.0 B_matches = 0 A_matches = 0 for i in range ( n_m ): dist = np . max ( np . abs ( templates_m [ i ] - templates_m ), axis = 1 ) B_matches += int ( np . sum ( dist < r ) - 1 ) for i in range ( n_m1 ): dist = np . max ( np . abs ( templates_m1 [ i ] - templates_m1 ), axis = 1 ) A_matches += int ( np . sum ( dist < r ) - 1 ) if B_matches <= 0 or A_matches <= 0 : return 0.0 B = B_matches / ( n_m * ( n_m - 1 )) A = A_matches / ( n_m1 * ( n_m1 - 1 )) if A <= 0 or B <= 0 : return 0.0 return float ( - np . log ( A / B )) def _calculate_complexity_index ( self , data : np . ndarray ) -> float : \"\"\"Calculate complexity index by integrating sample entropy across scales. Parameters ---------- data : np.ndarray Input time series data Returns ------- float Complexity index value \"\"\" sampen_values = [] for scale in range ( 1 , int ( self . max_scale ) + 1 ): coarse = self . _coarse_grain ( data , scale ) if coarse . shape [ 0 ] < int ( self . min_points ): break sampen = self . _sample_entropy ( coarse ) sampen_values . append ( sampen ) if len ( sampen_values ) > 1 : scales = np . arange ( 1 , len ( sampen_values ) + 1 , dtype = float ) return float ( np . trapz ( np . asarray ( sampen_values , dtype = float ), x = scales )) if len ( sampen_values ) == 1 : return float ( sampen_values [ 0 ]) return 0.0 def compute_analysis ( self , signals : SlidingWindow , methods : list ) -> dict : \"\"\"Compute dominance analysis for ensemble performance data. Parameters ---------- signals : SlidingWindow Sliding window buffer containing movement velocity data. methods : list of str List of analysis methods to compute. Available options: 'complexity_index', 'dominance_score', 'leader_identification' Returns ------- dict Dictionary containing dominance analysis results. \"\"\" if not signals . is_full (): return {} data , _ = signals . to_array () n_samples , n_features = data . shape if n_samples < int ( self . min_points ): return {} complexity_indices = [] for i in range ( n_features ): ci = self . _calculate_complexity_index ( data [:, i ]) complexity_indices . append ( ci ) result = {} for method in methods : if method == 'complexity_index' : values = np . array ( complexity_indices , dtype = float ) result [ 'complexity_index' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'dominance_score' : cis = np . array ( complexity_indices , dtype = float ) if cis . size > 0 : max_ci = float ( np . max ( cis )) if max_ci > 0 : scores = ( 1.0 - ( cis / max_ci )) else : scores = np . zeros_like ( cis ) result [ 'dominance_score' ] = float ( scores [ 0 ]) if len ( scores ) == 1 else scores . tolist () elif method == 'leader_identification' : if complexity_indices : leader_idx = np . argmin ( complexity_indices ) result [ 'leader' ] = int ( leader_idx ) result [ 'leader_complexity' ] = float ( complexity_indices [ leader_idx ]) else : continue return result def __call__ ( self , sliding_window : SlidingWindow , methods : list ) -> dict : \"\"\"Compute dominance analysis metrics. Parameters ---------- sliding_window : SlidingWindow Buffer containing multivariate data to analyze. methods : list of str List of analysis methods to compute. Returns ------- dict Dictionary containing dominance analysis metrics. \"\"\" return self . compute_analysis ( sliding_window , methods )","title":"MultiScaleEntropyDominance"},{"location":"API/analysis_primitives/mse_dominance/#pyeyesweb.analysis_primitives.mse_dominance.MultiScaleEntropyDominance.__call__","text":"Compute dominance analysis metrics. Parameters: sliding_window ( SlidingWindow ) \u2013 Buffer containing multivariate data to analyze. methods ( list of str ) \u2013 List of analysis methods to compute. Returns: dict \u2013 Dictionary containing dominance analysis metrics. Source code in pyeyesweb/analysis_primitives/mse_dominance.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def __call__ ( self , sliding_window : SlidingWindow , methods : list ) -> dict : \"\"\"Compute dominance analysis metrics. Parameters ---------- sliding_window : SlidingWindow Buffer containing multivariate data to analyze. methods : list of str List of analysis methods to compute. Returns ------- dict Dictionary containing dominance analysis metrics. \"\"\" return self . compute_analysis ( sliding_window , methods )","title":"__call__"},{"location":"API/analysis_primitives/mse_dominance/#pyeyesweb.analysis_primitives.mse_dominance.MultiScaleEntropyDominance.__init__","text":"Initialize the multi-scale entropy analyzer with default parameters. Source code in pyeyesweb/analysis_primitives/mse_dominance.py 48 49 50 51 52 53 54 def __init__ ( self ): \"\"\"Initialize the multi-scale entropy analyzer with default parameters.\"\"\" # Algorithm parameters as per reference papers self . m = 2 # Embedding dimension for sample entropy self . r = 0.15 # Tolerance parameter (15% of standard deviation) self . max_scale = 6 # Maximum scale factor for coarse-graining self . min_points = 500 # Minimum data points required per scale","title":"__init__"},{"location":"API/analysis_primitives/mse_dominance/#pyeyesweb.analysis_primitives.mse_dominance.MultiScaleEntropyDominance.compute_analysis","text":"Compute dominance analysis for ensemble performance data. Parameters: signals ( SlidingWindow ) \u2013 Sliding window buffer containing movement velocity data. methods ( list of str ) \u2013 List of analysis methods to compute. Available options: 'complexity_index', 'dominance_score', 'leader_identification' Returns: dict \u2013 Dictionary containing dominance analysis results. Source code in pyeyesweb/analysis_primitives/mse_dominance.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def compute_analysis ( self , signals : SlidingWindow , methods : list ) -> dict : \"\"\"Compute dominance analysis for ensemble performance data. Parameters ---------- signals : SlidingWindow Sliding window buffer containing movement velocity data. methods : list of str List of analysis methods to compute. Available options: 'complexity_index', 'dominance_score', 'leader_identification' Returns ------- dict Dictionary containing dominance analysis results. \"\"\" if not signals . is_full (): return {} data , _ = signals . to_array () n_samples , n_features = data . shape if n_samples < int ( self . min_points ): return {} complexity_indices = [] for i in range ( n_features ): ci = self . _calculate_complexity_index ( data [:, i ]) complexity_indices . append ( ci ) result = {} for method in methods : if method == 'complexity_index' : values = np . array ( complexity_indices , dtype = float ) result [ 'complexity_index' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'dominance_score' : cis = np . array ( complexity_indices , dtype = float ) if cis . size > 0 : max_ci = float ( np . max ( cis )) if max_ci > 0 : scores = ( 1.0 - ( cis / max_ci )) else : scores = np . zeros_like ( cis ) result [ 'dominance_score' ] = float ( scores [ 0 ]) if len ( scores ) == 1 else scores . tolist () elif method == 'leader_identification' : if complexity_indices : leader_idx = np . argmin ( complexity_indices ) result [ 'leader' ] = int ( leader_idx ) result [ 'leader_complexity' ] = float ( complexity_indices [ leader_idx ]) else : continue return result","title":"compute_analysis"},{"location":"API/analysis_primitives/rarity/","text":"Rarity","title":"Rarity"},{"location":"API/analysis_primitives/rarity/#rarity","text":"","title":"Rarity"},{"location":"API/analysis_primitives/statistical_moment/","text":"Statistical Moment Statistical moments analysis module for signal processing. This module provides tools for computing various statistical moments from multivariate signal data. Statistical moments characterize the shape and properties of probability distributions and are fundamental in signal analysis. The available statistical moments include: 1. Mean - Central tendency of the data 2. Standard Deviation - Dispersion around the mean 3. Skewness - Asymmetry of the distribution 4. Kurtosis - Tailedness of the distribution Typical use cases include: 1. Signal characterization and feature extraction 2. Quality assessment of sensor data 3. Motion pattern analysis in movement data 4. Anomaly detection in time series data 5. Distribution analysis in multivariate signals References Pearson, K. (1895). Contributions to the Mathematical Theory of Evolution. Fisher, R. A. (1925). Statistical Methods for Research Workers. StatisticalMoment Real time statistical moments analyzer for signal data. This class computes various statistical moments (mean, standard deviation, skewness, kurtosis) from sliding window data to characterize signal distributions and properties. Source code in pyeyesweb/analysis_primitives/statistical_moment.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class StatisticalMoment : \"\"\"Real time statistical moments analyzer for signal data. This class computes various statistical moments (mean, standard deviation, skewness, kurtosis) from sliding window data to characterize signal distributions and properties. \"\"\" def __init__ ( self ): # No parameters in constructor as per comments pass def compute_statistics ( self , signals : SlidingWindow , methods : list ) -> dict : \"\"\"Compute statistical analysis for multivariate signals. Parameters ---------- signals : SlidingWindow Sliding window buffer containing multivariate signal data. methods : list of str List of statistical methods to compute. Available options: 'mean', 'std_dev', 'skewness', 'kurtosis' Returns ------- dict Dictionary containing statistical metrics. \"\"\" if not signals . is_full (): return np . nan data , _ = signals . to_array () n_samples , n_features = data . shape if n_samples < 2 : return np . nan result = {} # Compute only the requested statistical moments for method in methods : if method == 'mean' : values = np . mean ( data , axis = 0 ) result [ 'mean' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'std_dev' : values = np . std ( data , axis = 0 , ddof = 1 ) result [ 'std' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'skewness' : values = stats . skew ( data , axis = 0 ) result [ 'skewness' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'kurtosis' : values = stats . kurtosis ( data , axis = 0 ) result [ 'kurtosis' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () else : # Skip invalid methods silently continue return result def __call__ ( self , sliding_window : SlidingWindow , methods : list ) -> dict : \"\"\"Compute statistical metrics. Parameters ---------- sliding_window : SlidingWindow Buffer containing multivariate data to analyze. methods : list of str List of statistical methods to compute. Returns ------- dict Dictionary containing statistical metrics. \"\"\" return self . compute_statistics ( sliding_window , methods ) __call__ ( sliding_window , methods ) Compute statistical metrics. Parameters: sliding_window ( SlidingWindow ) \u2013 Buffer containing multivariate data to analyze. methods ( list of str ) \u2013 List of statistical methods to compute. Returns: dict \u2013 Dictionary containing statistical metrics. Source code in pyeyesweb/analysis_primitives/statistical_moment.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __call__ ( self , sliding_window : SlidingWindow , methods : list ) -> dict : \"\"\"Compute statistical metrics. Parameters ---------- sliding_window : SlidingWindow Buffer containing multivariate data to analyze. methods : list of str List of statistical methods to compute. Returns ------- dict Dictionary containing statistical metrics. \"\"\" return self . compute_statistics ( sliding_window , methods ) compute_statistics ( signals , methods ) Compute statistical analysis for multivariate signals. Parameters: signals ( SlidingWindow ) \u2013 Sliding window buffer containing multivariate signal data. methods ( list of str ) \u2013 List of statistical methods to compute. Available options: 'mean', 'std_dev', 'skewness', 'kurtosis' Returns: dict \u2013 Dictionary containing statistical metrics. Source code in pyeyesweb/analysis_primitives/statistical_moment.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def compute_statistics ( self , signals : SlidingWindow , methods : list ) -> dict : \"\"\"Compute statistical analysis for multivariate signals. Parameters ---------- signals : SlidingWindow Sliding window buffer containing multivariate signal data. methods : list of str List of statistical methods to compute. Available options: 'mean', 'std_dev', 'skewness', 'kurtosis' Returns ------- dict Dictionary containing statistical metrics. \"\"\" if not signals . is_full (): return np . nan data , _ = signals . to_array () n_samples , n_features = data . shape if n_samples < 2 : return np . nan result = {} # Compute only the requested statistical moments for method in methods : if method == 'mean' : values = np . mean ( data , axis = 0 ) result [ 'mean' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'std_dev' : values = np . std ( data , axis = 0 , ddof = 1 ) result [ 'std' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'skewness' : values = stats . skew ( data , axis = 0 ) result [ 'skewness' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'kurtosis' : values = stats . kurtosis ( data , axis = 0 ) result [ 'kurtosis' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () else : # Skip invalid methods silently continue return result","title":"Statistical Moment"},{"location":"API/analysis_primitives/statistical_moment/#statistical-moment","text":"Statistical moments analysis module for signal processing. This module provides tools for computing various statistical moments from multivariate signal data. Statistical moments characterize the shape and properties of probability distributions and are fundamental in signal analysis. The available statistical moments include: 1. Mean - Central tendency of the data 2. Standard Deviation - Dispersion around the mean 3. Skewness - Asymmetry of the distribution 4. Kurtosis - Tailedness of the distribution Typical use cases include: 1. Signal characterization and feature extraction 2. Quality assessment of sensor data 3. Motion pattern analysis in movement data 4. Anomaly detection in time series data 5. Distribution analysis in multivariate signals References Pearson, K. (1895). Contributions to the Mathematical Theory of Evolution. Fisher, R. A. (1925). Statistical Methods for Research Workers.","title":"Statistical Moment"},{"location":"API/analysis_primitives/statistical_moment/#pyeyesweb.analysis_primitives.statistical_moment.StatisticalMoment","text":"Real time statistical moments analyzer for signal data. This class computes various statistical moments (mean, standard deviation, skewness, kurtosis) from sliding window data to characterize signal distributions and properties. Source code in pyeyesweb/analysis_primitives/statistical_moment.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class StatisticalMoment : \"\"\"Real time statistical moments analyzer for signal data. This class computes various statistical moments (mean, standard deviation, skewness, kurtosis) from sliding window data to characterize signal distributions and properties. \"\"\" def __init__ ( self ): # No parameters in constructor as per comments pass def compute_statistics ( self , signals : SlidingWindow , methods : list ) -> dict : \"\"\"Compute statistical analysis for multivariate signals. Parameters ---------- signals : SlidingWindow Sliding window buffer containing multivariate signal data. methods : list of str List of statistical methods to compute. Available options: 'mean', 'std_dev', 'skewness', 'kurtosis' Returns ------- dict Dictionary containing statistical metrics. \"\"\" if not signals . is_full (): return np . nan data , _ = signals . to_array () n_samples , n_features = data . shape if n_samples < 2 : return np . nan result = {} # Compute only the requested statistical moments for method in methods : if method == 'mean' : values = np . mean ( data , axis = 0 ) result [ 'mean' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'std_dev' : values = np . std ( data , axis = 0 , ddof = 1 ) result [ 'std' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'skewness' : values = stats . skew ( data , axis = 0 ) result [ 'skewness' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'kurtosis' : values = stats . kurtosis ( data , axis = 0 ) result [ 'kurtosis' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () else : # Skip invalid methods silently continue return result def __call__ ( self , sliding_window : SlidingWindow , methods : list ) -> dict : \"\"\"Compute statistical metrics. Parameters ---------- sliding_window : SlidingWindow Buffer containing multivariate data to analyze. methods : list of str List of statistical methods to compute. Returns ------- dict Dictionary containing statistical metrics. \"\"\" return self . compute_statistics ( sliding_window , methods )","title":"StatisticalMoment"},{"location":"API/analysis_primitives/statistical_moment/#pyeyesweb.analysis_primitives.statistical_moment.StatisticalMoment.__call__","text":"Compute statistical metrics. Parameters: sliding_window ( SlidingWindow ) \u2013 Buffer containing multivariate data to analyze. methods ( list of str ) \u2013 List of statistical methods to compute. Returns: dict \u2013 Dictionary containing statistical metrics. Source code in pyeyesweb/analysis_primitives/statistical_moment.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __call__ ( self , sliding_window : SlidingWindow , methods : list ) -> dict : \"\"\"Compute statistical metrics. Parameters ---------- sliding_window : SlidingWindow Buffer containing multivariate data to analyze. methods : list of str List of statistical methods to compute. Returns ------- dict Dictionary containing statistical metrics. \"\"\" return self . compute_statistics ( sliding_window , methods )","title":"__call__"},{"location":"API/analysis_primitives/statistical_moment/#pyeyesweb.analysis_primitives.statistical_moment.StatisticalMoment.compute_statistics","text":"Compute statistical analysis for multivariate signals. Parameters: signals ( SlidingWindow ) \u2013 Sliding window buffer containing multivariate signal data. methods ( list of str ) \u2013 List of statistical methods to compute. Available options: 'mean', 'std_dev', 'skewness', 'kurtosis' Returns: dict \u2013 Dictionary containing statistical metrics. Source code in pyeyesweb/analysis_primitives/statistical_moment.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def compute_statistics ( self , signals : SlidingWindow , methods : list ) -> dict : \"\"\"Compute statistical analysis for multivariate signals. Parameters ---------- signals : SlidingWindow Sliding window buffer containing multivariate signal data. methods : list of str List of statistical methods to compute. Available options: 'mean', 'std_dev', 'skewness', 'kurtosis' Returns ------- dict Dictionary containing statistical metrics. \"\"\" if not signals . is_full (): return np . nan data , _ = signals . to_array () n_samples , n_features = data . shape if n_samples < 2 : return np . nan result = {} # Compute only the requested statistical moments for method in methods : if method == 'mean' : values = np . mean ( data , axis = 0 ) result [ 'mean' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'std_dev' : values = np . std ( data , axis = 0 , ddof = 1 ) result [ 'std' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'skewness' : values = stats . skew ( data , axis = 0 ) result [ 'skewness' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () elif method == 'kurtosis' : values = stats . kurtosis ( data , axis = 0 ) result [ 'kurtosis' ] = float ( values [ 0 ]) if len ( values ) == 1 else values . tolist () else : # Skip invalid methods silently continue return result","title":"compute_statistics"},{"location":"API/analysis_primitives/synchronization/","text":"Synchronization Synchronization analysis module for real time signal phase locking. This module provides tools for computing phase synchronization between paired signals using the Hilbert Transform and Phase Locking Value (PLV) analysis. It is designed for real time analysis of motion capture or sensor data streams. The synchronization analysis follows these steps: 1. Optional band pass filtering to isolate frequencies of interest 2. Signal centering (mean removal) to eliminate DC bias 3. Hilbert Transform to extract instantaneous phase information 4. Phase Locking Value computation to quantify synchronization strength Typical use cases include: 1. Movement coordination analysis between limbs 2. Human-human or human-robot interaction studies 3. Neural oscillation synchronization 4. Periodic signal coupling analysis References Lachaux et al. (1999). Measuring phase synchrony in brain signals. Human Brain Mapping, 8(4), 194-208. Rosenblum et al. (1996). Phase synchronization of chaotic oscillators. Physical Review Letters, 76(11), 1804. Synchronization Real time phase synchronization analyzer for paired signals. This class computes the Phase Locking Value (PLV) between two signals using the Hilbert Transform to extract instantaneous phase information. It maintains a history buffer for tracking synchronization over time and can optionally apply band-pass filtering to focus on specific frequency bands. The PLV ranges from 0 (no synchronization) to 1 (perfect synchronization) and is computed as the absolute value of the mean complex phase difference between the two signals. Read more in the User Guide Parameters: sensitivity ( int , default: 100 ) \u2013 Size of the PLV history buffer. Larger values provide more temporal context but increase memory usage. Must be positive integer between 1 and 10,000 (default: 100). output_phase ( bool , default: False ) \u2013 If True, outputs phase synchronization status as \"IN PHASE\" or \"OUT OF PHASE\" based on the phase_threshold. Must be boolean (default: False). filter_params ( tuple of (float, float, float) or None , default: None ) \u2013 Band-pass filter parameters as (lowcut_hz, highcut_hz, sampling_rate_hz). All frequencies must be positive with lowcut < highcut < sampling_rate/2. Example: (0.5, 30, 100) for 0.5-30 Hz band with 100 Hz sampling. If None, no filtering is applied (default: None). phase_threshold ( float , default: 0.7 ) \u2013 PLV threshold for phase status determination. Values above this are considered \"IN PHASE\". Must be between 0 and 1 inclusive (default: 0.7). Raises: TypeError \u2013 If sensitivity is not int, output_phase is not bool, phase_threshold is not numeric, or filter_params is not tuple/list. ValueError \u2013 If sensitivity <= 0 or > 10,000, phase_threshold outside [0, 1], or filter_params contains invalid frequencies. Attributes: plv_history ( ThreadSafeHistoryBuffer ) \u2013 Thread-safe rolling buffer storing recent PLV values for temporal analysis. output_phase ( bool ) \u2013 Flag controlling phase status output. filter_params ( tuple or None ) \u2013 Band-pass filter configuration. phase_threshold ( float ) \u2013 Threshold for phase synchronization classification. Examples: >>> from pyeyesweb.analysis_primitives.synchronization import Synchronization >>> from pyeyesweb.data_models.sliding_window import SlidingWindow >>> >>> # Create synchronization analyzer with filtering >>> sync = Synchronization ( ... sensitivity = 50 , ... output_phase = True , ... filter_params = ( 1.0 , 10.0 , 100.0 ), # 1-10 Hz band at 100 Hz ... phase_threshold = 0.8 ... ) >>> >>> # Create sliding window for two signals >>> window = SlidingWindow ( max_length = 200 , n_columns = 2 ) >>> >>> # Add signal data (e.g., from two sensors) >>> for i in range ( 200 ): ... window . append ([ signal1 [ i ], signal2 [ i ]]) >>> >>> # Compute synchronization >>> result = sync ( window ) >>> print ( f \"PLV: { result [ 'plv' ] : .3f } , Status: { result [ 'phase_status' ] } \" ) Notes Requires at least a full window of data to compute meaningful results The Hilbert Transform assumes narrowband or filtered signals for best results Phase differences are most meaningful for signals with similar frequencies For broadband signals, consider using filter_params to isolate frequency bands Source code in pyeyesweb/analysis_primitives/synchronization.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 class Synchronization : \"\"\"Real time phase synchronization analyzer for paired signals. This class computes the Phase Locking Value (PLV) between two signals using the Hilbert Transform to extract instantaneous phase information. It maintains a history buffer for tracking synchronization over time and can optionally apply band-pass filtering to focus on specific frequency bands. The PLV ranges from 0 (no synchronization) to 1 (perfect synchronization) and is computed as the absolute value of the mean complex phase difference between the two signals. Read more in the [User Guide](/PyEyesWeb/user_guide/theoretical_framework/analysis_primitives/synchronization/) Parameters ---------- sensitivity : int, optional Size of the PLV history buffer. Larger values provide more temporal context but increase memory usage. Must be positive integer between 1 and 10,000 (default: 100). output_phase : bool, optional If True, outputs phase synchronization status as \"IN PHASE\" or \"OUT OF PHASE\" based on the phase_threshold. Must be boolean (default: False). filter_params : tuple of (float, float, float) or None, optional Band-pass filter parameters as (lowcut_hz, highcut_hz, sampling_rate_hz). All frequencies must be positive with lowcut < highcut < sampling_rate/2. Example: (0.5, 30, 100) for 0.5-30 Hz band with 100 Hz sampling. If None, no filtering is applied (default: None). phase_threshold : float, optional PLV threshold for phase status determination. Values above this are considered \"IN PHASE\". Must be between 0 and 1 inclusive (default: 0.7). Raises ------ TypeError If sensitivity is not int, output_phase is not bool, phase_threshold is not numeric, or filter_params is not tuple/list. ValueError If sensitivity <= 0 or > 10,000, phase_threshold outside [0, 1], or filter_params contains invalid frequencies. Attributes ---------- plv_history : ThreadSafeHistoryBuffer Thread-safe rolling buffer storing recent PLV values for temporal analysis. output_phase : bool Flag controlling phase status output. filter_params : tuple or None Band-pass filter configuration. phase_threshold : float Threshold for phase synchronization classification. Examples -------- >>> from pyeyesweb.analysis_primitives.synchronization import Synchronization >>> from pyeyesweb.data_models.sliding_window import SlidingWindow >>> >>> # Create synchronization analyzer with filtering >>> sync = Synchronization( ... sensitivity=50, ... output_phase=True, ... filter_params=(1.0, 10.0, 100.0), # 1-10 Hz band at 100 Hz ... phase_threshold=0.8 ... ) >>> >>> # Create sliding window for two signals >>> window = SlidingWindow(max_length=200, n_columns=2) >>> >>> # Add signal data (e.g., from two sensors) >>> for i in range(200): ... window.append([signal1[i], signal2[i]]) >>> >>> # Compute synchronization >>> result = sync(window) >>> print(f\"PLV: {result['plv']:.3f}, Status: {result['phase_status']}\") Notes ----- - Requires at least a full window of data to compute meaningful results - The Hilbert Transform assumes narrowband or filtered signals for best results - Phase differences are most meaningful for signals with similar frequencies - For broadband signals, consider using filter_params to isolate frequency bands \"\"\" def __init__ ( self , sensitivity = 100 , output_phase = False , filter_params = None , phase_threshold = 0.7 ): sensitivity = validate_integer ( sensitivity , 'sensitivity' , min_val = 1 , max_val = 10000 ) self . output_phase = validate_boolean ( output_phase , 'output_phase' ) self . phase_threshold = validate_numeric ( phase_threshold , 'phase_threshold' , min_val = 0 , max_val = 1 ) # validate and normalize filter params self . filter_params = validate_and_normalize_filter_params ( filter_params ) # Use ThreadSafeHistoryBuffer instead of deque + lock self . plv_history = ThreadSafeHistoryBuffer ( maxlen = sensitivity ) def compute_synchronization ( self , signals : SlidingWindow ): \"\"\"Compute phase synchronization between two signals. Processes the signal pair through filtering (optional), centering, Hilbert Transform, and PLV computation to quantify synchronization. Parameters ---------- signals : SlidingWindow Sliding window buffer containing exactly 2 columns of signal data. Must be full (contain max_length samples) for computation. Returns ------- dict Dictionary containing synchronization metrics: - 'plv': Phase Locking Value between 0 (no sync) and 1 (perfect sync). Returns NaN if the window is not full. - 'phase_status': If output_phase is True, returns \"IN PHASE\" when PLV > phase_threshold, \"OUT OF PHASE\" otherwise. Returns None if output_phase is False or if the window is not full. Notes ----- The computation pipeline: 1. Check if window has sufficient data (is_full) 2. Apply band-pass filter if filter_params is set 3. Center signals by removing mean (eliminates DC component) 4. Apply Hilbert Transform to get analytic signal and phase 5. Compute PLV from phase difference 6. Update PLV history buffer 7. Determine phase status if requested \"\"\" # Validate input has exactly 2 columns if signals . _n_columns != 2 : raise ValueError ( f \"Synchronization requires exactly 2 signal channels, got { signals . _n_columns } \" ) if not signals . is_full (): return { \"plv\" : np . nan , \"phase_status\" : None } sig , _ = signals . to_array () # combines all phase sync steps plv = compute_phase_synchronization ( sig , self . filter_params ) self . plv_history . append ( plv ) phase_status = None if self . output_phase : # Determine phase synchronization status based on threshold phase_status = \"IN PHASE\" if plv > self . phase_threshold else \"OUT OF PHASE\" return { \"plv\" : plv , \"phase_status\" : phase_status } def __call__ ( self , sliding_window : SlidingWindow ): \"\"\"Compute and optionally display synchronization metrics. This method allows the class to be used as a callable, providing a convenient interface for real time processing pipelines. Parameters ---------- sliding_window : SlidingWindow Buffer containing two signal columns to analyze. Returns ------- dict Dictionary containing synchronization metrics: - 'plv': Phase Locking Value (0-1) or NaN if insufficient data. - 'phase_status': Phase status (\"IN PHASE\"/\"OUT OF PHASE\") or None. Output ------------ Prints synchronization metrics to stdout if PLV is computed successfully. Format depends on output_phase setting. \"\"\" return self . compute_synchronization ( sliding_window ) __call__ ( sliding_window ) Compute and optionally display synchronization metrics. This method allows the class to be used as a callable, providing a convenient interface for real time processing pipelines. Parameters: sliding_window ( SlidingWindow ) \u2013 Buffer containing two signal columns to analyze. Returns: dict \u2013 Dictionary containing synchronization metrics: - 'plv': Phase Locking Value (0-1) or NaN if insufficient data. - 'phase_status': Phase status (\"IN PHASE\"/\"OUT OF PHASE\") or None. Output Prints synchronization metrics to stdout if PLV is computed successfully. Format depends on output_phase setting. Source code in pyeyesweb/analysis_primitives/synchronization.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def __call__ ( self , sliding_window : SlidingWindow ): \"\"\"Compute and optionally display synchronization metrics. This method allows the class to be used as a callable, providing a convenient interface for real time processing pipelines. Parameters ---------- sliding_window : SlidingWindow Buffer containing two signal columns to analyze. Returns ------- dict Dictionary containing synchronization metrics: - 'plv': Phase Locking Value (0-1) or NaN if insufficient data. - 'phase_status': Phase status (\"IN PHASE\"/\"OUT OF PHASE\") or None. Output ------------ Prints synchronization metrics to stdout if PLV is computed successfully. Format depends on output_phase setting. \"\"\" return self . compute_synchronization ( sliding_window ) compute_synchronization ( signals ) Compute phase synchronization between two signals. Processes the signal pair through filtering (optional), centering, Hilbert Transform, and PLV computation to quantify synchronization. Parameters: signals ( SlidingWindow ) \u2013 Sliding window buffer containing exactly 2 columns of signal data. Must be full (contain max_length samples) for computation. Returns: dict \u2013 Dictionary containing synchronization metrics: - 'plv': Phase Locking Value between 0 (no sync) and 1 (perfect sync). Returns NaN if the window is not full. - 'phase_status': If output_phase is True, returns \"IN PHASE\" when PLV > phase_threshold, \"OUT OF PHASE\" otherwise. Returns None if output_phase is False or if the window is not full. Notes The computation pipeline: 1. Check if window has sufficient data (is_full) 2. Apply band-pass filter if filter_params is set 3. Center signals by removing mean (eliminates DC component) 4. Apply Hilbert Transform to get analytic signal and phase 5. Compute PLV from phase difference 6. Update PLV history buffer 7. Determine phase status if requested Source code in pyeyesweb/analysis_primitives/synchronization.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def compute_synchronization ( self , signals : SlidingWindow ): \"\"\"Compute phase synchronization between two signals. Processes the signal pair through filtering (optional), centering, Hilbert Transform, and PLV computation to quantify synchronization. Parameters ---------- signals : SlidingWindow Sliding window buffer containing exactly 2 columns of signal data. Must be full (contain max_length samples) for computation. Returns ------- dict Dictionary containing synchronization metrics: - 'plv': Phase Locking Value between 0 (no sync) and 1 (perfect sync). Returns NaN if the window is not full. - 'phase_status': If output_phase is True, returns \"IN PHASE\" when PLV > phase_threshold, \"OUT OF PHASE\" otherwise. Returns None if output_phase is False or if the window is not full. Notes ----- The computation pipeline: 1. Check if window has sufficient data (is_full) 2. Apply band-pass filter if filter_params is set 3. Center signals by removing mean (eliminates DC component) 4. Apply Hilbert Transform to get analytic signal and phase 5. Compute PLV from phase difference 6. Update PLV history buffer 7. Determine phase status if requested \"\"\" # Validate input has exactly 2 columns if signals . _n_columns != 2 : raise ValueError ( f \"Synchronization requires exactly 2 signal channels, got { signals . _n_columns } \" ) if not signals . is_full (): return { \"plv\" : np . nan , \"phase_status\" : None } sig , _ = signals . to_array () # combines all phase sync steps plv = compute_phase_synchronization ( sig , self . filter_params ) self . plv_history . append ( plv ) phase_status = None if self . output_phase : # Determine phase synchronization status based on threshold phase_status = \"IN PHASE\" if plv > self . phase_threshold else \"OUT OF PHASE\" return { \"plv\" : plv , \"phase_status\" : phase_status }","title":"Synchronization"},{"location":"API/analysis_primitives/synchronization/#synchronization","text":"Synchronization analysis module for real time signal phase locking. This module provides tools for computing phase synchronization between paired signals using the Hilbert Transform and Phase Locking Value (PLV) analysis. It is designed for real time analysis of motion capture or sensor data streams. The synchronization analysis follows these steps: 1. Optional band pass filtering to isolate frequencies of interest 2. Signal centering (mean removal) to eliminate DC bias 3. Hilbert Transform to extract instantaneous phase information 4. Phase Locking Value computation to quantify synchronization strength Typical use cases include: 1. Movement coordination analysis between limbs 2. Human-human or human-robot interaction studies 3. Neural oscillation synchronization 4. Periodic signal coupling analysis References Lachaux et al. (1999). Measuring phase synchrony in brain signals. Human Brain Mapping, 8(4), 194-208. Rosenblum et al. (1996). Phase synchronization of chaotic oscillators. Physical Review Letters, 76(11), 1804.","title":"Synchronization"},{"location":"API/analysis_primitives/synchronization/#pyeyesweb.analysis_primitives.synchronization.Synchronization","text":"Real time phase synchronization analyzer for paired signals. This class computes the Phase Locking Value (PLV) between two signals using the Hilbert Transform to extract instantaneous phase information. It maintains a history buffer for tracking synchronization over time and can optionally apply band-pass filtering to focus on specific frequency bands. The PLV ranges from 0 (no synchronization) to 1 (perfect synchronization) and is computed as the absolute value of the mean complex phase difference between the two signals. Read more in the User Guide Parameters: sensitivity ( int , default: 100 ) \u2013 Size of the PLV history buffer. Larger values provide more temporal context but increase memory usage. Must be positive integer between 1 and 10,000 (default: 100). output_phase ( bool , default: False ) \u2013 If True, outputs phase synchronization status as \"IN PHASE\" or \"OUT OF PHASE\" based on the phase_threshold. Must be boolean (default: False). filter_params ( tuple of (float, float, float) or None , default: None ) \u2013 Band-pass filter parameters as (lowcut_hz, highcut_hz, sampling_rate_hz). All frequencies must be positive with lowcut < highcut < sampling_rate/2. Example: (0.5, 30, 100) for 0.5-30 Hz band with 100 Hz sampling. If None, no filtering is applied (default: None). phase_threshold ( float , default: 0.7 ) \u2013 PLV threshold for phase status determination. Values above this are considered \"IN PHASE\". Must be between 0 and 1 inclusive (default: 0.7). Raises: TypeError \u2013 If sensitivity is not int, output_phase is not bool, phase_threshold is not numeric, or filter_params is not tuple/list. ValueError \u2013 If sensitivity <= 0 or > 10,000, phase_threshold outside [0, 1], or filter_params contains invalid frequencies. Attributes: plv_history ( ThreadSafeHistoryBuffer ) \u2013 Thread-safe rolling buffer storing recent PLV values for temporal analysis. output_phase ( bool ) \u2013 Flag controlling phase status output. filter_params ( tuple or None ) \u2013 Band-pass filter configuration. phase_threshold ( float ) \u2013 Threshold for phase synchronization classification. Examples: >>> from pyeyesweb.analysis_primitives.synchronization import Synchronization >>> from pyeyesweb.data_models.sliding_window import SlidingWindow >>> >>> # Create synchronization analyzer with filtering >>> sync = Synchronization ( ... sensitivity = 50 , ... output_phase = True , ... filter_params = ( 1.0 , 10.0 , 100.0 ), # 1-10 Hz band at 100 Hz ... phase_threshold = 0.8 ... ) >>> >>> # Create sliding window for two signals >>> window = SlidingWindow ( max_length = 200 , n_columns = 2 ) >>> >>> # Add signal data (e.g., from two sensors) >>> for i in range ( 200 ): ... window . append ([ signal1 [ i ], signal2 [ i ]]) >>> >>> # Compute synchronization >>> result = sync ( window ) >>> print ( f \"PLV: { result [ 'plv' ] : .3f } , Status: { result [ 'phase_status' ] } \" ) Notes Requires at least a full window of data to compute meaningful results The Hilbert Transform assumes narrowband or filtered signals for best results Phase differences are most meaningful for signals with similar frequencies For broadband signals, consider using filter_params to isolate frequency bands Source code in pyeyesweb/analysis_primitives/synchronization.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 class Synchronization : \"\"\"Real time phase synchronization analyzer for paired signals. This class computes the Phase Locking Value (PLV) between two signals using the Hilbert Transform to extract instantaneous phase information. It maintains a history buffer for tracking synchronization over time and can optionally apply band-pass filtering to focus on specific frequency bands. The PLV ranges from 0 (no synchronization) to 1 (perfect synchronization) and is computed as the absolute value of the mean complex phase difference between the two signals. Read more in the [User Guide](/PyEyesWeb/user_guide/theoretical_framework/analysis_primitives/synchronization/) Parameters ---------- sensitivity : int, optional Size of the PLV history buffer. Larger values provide more temporal context but increase memory usage. Must be positive integer between 1 and 10,000 (default: 100). output_phase : bool, optional If True, outputs phase synchronization status as \"IN PHASE\" or \"OUT OF PHASE\" based on the phase_threshold. Must be boolean (default: False). filter_params : tuple of (float, float, float) or None, optional Band-pass filter parameters as (lowcut_hz, highcut_hz, sampling_rate_hz). All frequencies must be positive with lowcut < highcut < sampling_rate/2. Example: (0.5, 30, 100) for 0.5-30 Hz band with 100 Hz sampling. If None, no filtering is applied (default: None). phase_threshold : float, optional PLV threshold for phase status determination. Values above this are considered \"IN PHASE\". Must be between 0 and 1 inclusive (default: 0.7). Raises ------ TypeError If sensitivity is not int, output_phase is not bool, phase_threshold is not numeric, or filter_params is not tuple/list. ValueError If sensitivity <= 0 or > 10,000, phase_threshold outside [0, 1], or filter_params contains invalid frequencies. Attributes ---------- plv_history : ThreadSafeHistoryBuffer Thread-safe rolling buffer storing recent PLV values for temporal analysis. output_phase : bool Flag controlling phase status output. filter_params : tuple or None Band-pass filter configuration. phase_threshold : float Threshold for phase synchronization classification. Examples -------- >>> from pyeyesweb.analysis_primitives.synchronization import Synchronization >>> from pyeyesweb.data_models.sliding_window import SlidingWindow >>> >>> # Create synchronization analyzer with filtering >>> sync = Synchronization( ... sensitivity=50, ... output_phase=True, ... filter_params=(1.0, 10.0, 100.0), # 1-10 Hz band at 100 Hz ... phase_threshold=0.8 ... ) >>> >>> # Create sliding window for two signals >>> window = SlidingWindow(max_length=200, n_columns=2) >>> >>> # Add signal data (e.g., from two sensors) >>> for i in range(200): ... window.append([signal1[i], signal2[i]]) >>> >>> # Compute synchronization >>> result = sync(window) >>> print(f\"PLV: {result['plv']:.3f}, Status: {result['phase_status']}\") Notes ----- - Requires at least a full window of data to compute meaningful results - The Hilbert Transform assumes narrowband or filtered signals for best results - Phase differences are most meaningful for signals with similar frequencies - For broadband signals, consider using filter_params to isolate frequency bands \"\"\" def __init__ ( self , sensitivity = 100 , output_phase = False , filter_params = None , phase_threshold = 0.7 ): sensitivity = validate_integer ( sensitivity , 'sensitivity' , min_val = 1 , max_val = 10000 ) self . output_phase = validate_boolean ( output_phase , 'output_phase' ) self . phase_threshold = validate_numeric ( phase_threshold , 'phase_threshold' , min_val = 0 , max_val = 1 ) # validate and normalize filter params self . filter_params = validate_and_normalize_filter_params ( filter_params ) # Use ThreadSafeHistoryBuffer instead of deque + lock self . plv_history = ThreadSafeHistoryBuffer ( maxlen = sensitivity ) def compute_synchronization ( self , signals : SlidingWindow ): \"\"\"Compute phase synchronization between two signals. Processes the signal pair through filtering (optional), centering, Hilbert Transform, and PLV computation to quantify synchronization. Parameters ---------- signals : SlidingWindow Sliding window buffer containing exactly 2 columns of signal data. Must be full (contain max_length samples) for computation. Returns ------- dict Dictionary containing synchronization metrics: - 'plv': Phase Locking Value between 0 (no sync) and 1 (perfect sync). Returns NaN if the window is not full. - 'phase_status': If output_phase is True, returns \"IN PHASE\" when PLV > phase_threshold, \"OUT OF PHASE\" otherwise. Returns None if output_phase is False or if the window is not full. Notes ----- The computation pipeline: 1. Check if window has sufficient data (is_full) 2. Apply band-pass filter if filter_params is set 3. Center signals by removing mean (eliminates DC component) 4. Apply Hilbert Transform to get analytic signal and phase 5. Compute PLV from phase difference 6. Update PLV history buffer 7. Determine phase status if requested \"\"\" # Validate input has exactly 2 columns if signals . _n_columns != 2 : raise ValueError ( f \"Synchronization requires exactly 2 signal channels, got { signals . _n_columns } \" ) if not signals . is_full (): return { \"plv\" : np . nan , \"phase_status\" : None } sig , _ = signals . to_array () # combines all phase sync steps plv = compute_phase_synchronization ( sig , self . filter_params ) self . plv_history . append ( plv ) phase_status = None if self . output_phase : # Determine phase synchronization status based on threshold phase_status = \"IN PHASE\" if plv > self . phase_threshold else \"OUT OF PHASE\" return { \"plv\" : plv , \"phase_status\" : phase_status } def __call__ ( self , sliding_window : SlidingWindow ): \"\"\"Compute and optionally display synchronization metrics. This method allows the class to be used as a callable, providing a convenient interface for real time processing pipelines. Parameters ---------- sliding_window : SlidingWindow Buffer containing two signal columns to analyze. Returns ------- dict Dictionary containing synchronization metrics: - 'plv': Phase Locking Value (0-1) or NaN if insufficient data. - 'phase_status': Phase status (\"IN PHASE\"/\"OUT OF PHASE\") or None. Output ------------ Prints synchronization metrics to stdout if PLV is computed successfully. Format depends on output_phase setting. \"\"\" return self . compute_synchronization ( sliding_window )","title":"Synchronization"},{"location":"API/analysis_primitives/synchronization/#pyeyesweb.analysis_primitives.synchronization.Synchronization.__call__","text":"Compute and optionally display synchronization metrics. This method allows the class to be used as a callable, providing a convenient interface for real time processing pipelines. Parameters: sliding_window ( SlidingWindow ) \u2013 Buffer containing two signal columns to analyze. Returns: dict \u2013 Dictionary containing synchronization metrics: - 'plv': Phase Locking Value (0-1) or NaN if insufficient data. - 'phase_status': Phase status (\"IN PHASE\"/\"OUT OF PHASE\") or None. Output Prints synchronization metrics to stdout if PLV is computed successfully. Format depends on output_phase setting. Source code in pyeyesweb/analysis_primitives/synchronization.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def __call__ ( self , sliding_window : SlidingWindow ): \"\"\"Compute and optionally display synchronization metrics. This method allows the class to be used as a callable, providing a convenient interface for real time processing pipelines. Parameters ---------- sliding_window : SlidingWindow Buffer containing two signal columns to analyze. Returns ------- dict Dictionary containing synchronization metrics: - 'plv': Phase Locking Value (0-1) or NaN if insufficient data. - 'phase_status': Phase status (\"IN PHASE\"/\"OUT OF PHASE\") or None. Output ------------ Prints synchronization metrics to stdout if PLV is computed successfully. Format depends on output_phase setting. \"\"\" return self . compute_synchronization ( sliding_window )","title":"__call__"},{"location":"API/analysis_primitives/synchronization/#pyeyesweb.analysis_primitives.synchronization.Synchronization.compute_synchronization","text":"Compute phase synchronization between two signals. Processes the signal pair through filtering (optional), centering, Hilbert Transform, and PLV computation to quantify synchronization. Parameters: signals ( SlidingWindow ) \u2013 Sliding window buffer containing exactly 2 columns of signal data. Must be full (contain max_length samples) for computation. Returns: dict \u2013 Dictionary containing synchronization metrics: - 'plv': Phase Locking Value between 0 (no sync) and 1 (perfect sync). Returns NaN if the window is not full. - 'phase_status': If output_phase is True, returns \"IN PHASE\" when PLV > phase_threshold, \"OUT OF PHASE\" otherwise. Returns None if output_phase is False or if the window is not full. Notes The computation pipeline: 1. Check if window has sufficient data (is_full) 2. Apply band-pass filter if filter_params is set 3. Center signals by removing mean (eliminates DC component) 4. Apply Hilbert Transform to get analytic signal and phase 5. Compute PLV from phase difference 6. Update PLV history buffer 7. Determine phase status if requested Source code in pyeyesweb/analysis_primitives/synchronization.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def compute_synchronization ( self , signals : SlidingWindow ): \"\"\"Compute phase synchronization between two signals. Processes the signal pair through filtering (optional), centering, Hilbert Transform, and PLV computation to quantify synchronization. Parameters ---------- signals : SlidingWindow Sliding window buffer containing exactly 2 columns of signal data. Must be full (contain max_length samples) for computation. Returns ------- dict Dictionary containing synchronization metrics: - 'plv': Phase Locking Value between 0 (no sync) and 1 (perfect sync). Returns NaN if the window is not full. - 'phase_status': If output_phase is True, returns \"IN PHASE\" when PLV > phase_threshold, \"OUT OF PHASE\" otherwise. Returns None if output_phase is False or if the window is not full. Notes ----- The computation pipeline: 1. Check if window has sufficient data (is_full) 2. Apply band-pass filter if filter_params is set 3. Center signals by removing mean (eliminates DC component) 4. Apply Hilbert Transform to get analytic signal and phase 5. Compute PLV from phase difference 6. Update PLV history buffer 7. Determine phase status if requested \"\"\" # Validate input has exactly 2 columns if signals . _n_columns != 2 : raise ValueError ( f \"Synchronization requires exactly 2 signal channels, got { signals . _n_columns } \" ) if not signals . is_full (): return { \"plv\" : np . nan , \"phase_status\" : None } sig , _ = signals . to_array () # combines all phase sync steps plv = compute_phase_synchronization ( sig , self . filter_params ) self . plv_history . append ( plv ) phase_status = None if self . output_phase : # Determine phase synchronization status based on threshold phase_status = \"IN PHASE\" if plv > self . phase_threshold else \"OUT OF PHASE\" return { \"plv\" : plv , \"phase_status\" : phase_status }","title":"compute_synchronization"},{"location":"API/data_models/sliding_window/","text":"Sliding Window SlidingWindow A thread-safe sliding window buffer for storing samples with timestamps. This class implements a circular buffer that maintains a fixed-size window of the most recent samples. When the buffer is full, new samples overwrite the oldest ones. Each sample is associated with a timestamp. Parameters: max_length ( int ) \u2013 Maximum number of samples the window can hold. n_columns ( int ) \u2013 Number of columns (features) in each sample. Attributes: _lock ( Lock ) \u2013 Thread lock for thread-safe operations. _max_length ( int ) \u2013 Maximum capacity of the buffer. _n_columns ( int ) \u2013 Number of columns per sample. _buffer ( ndarray ) \u2013 Circular buffer storing the samples, shape (max_length, n_columns). _timestamp ( ndarray ) \u2013 Array storing timestamps for each sample, shape (max_length,). _start ( int ) \u2013 Index of the oldest sample in the buffer. _size ( int ) \u2013 Current number of samples in the buffer. Examples: >>> window = SlidingWindow ( max_length = 100 , n_columns = 3 ) >>> window . append ([ 1.0 , 2.0 , 3.0 ]) >>> window . append ( np . array ([ 4.0 , 5.0 , 6.0 ]), timestamp = 1234567890.0 ) >>> data , timestamps = window . to_array () >>> print ( f \"Buffer contains { len ( window ) } samples\" ) Source code in pyeyesweb/data_models/sliding_window.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 class SlidingWindow : \"\"\" A thread-safe sliding window buffer for storing samples with timestamps. This class implements a circular buffer that maintains a fixed-size window of the most recent samples. When the buffer is full, new samples overwrite the oldest ones. Each sample is associated with a timestamp. Parameters ---------- max_length : int Maximum number of samples the window can hold. n_columns : int Number of columns (features) in each sample. Attributes ---------- _lock : threading.Lock Thread lock for thread-safe operations. _max_length : int Maximum capacity of the buffer. _n_columns : int Number of columns per sample. _buffer : np.ndarray Circular buffer storing the samples, shape (max_length, n_columns). _timestamp : np.ndarray Array storing timestamps for each sample, shape (max_length,). _start : int Index of the oldest sample in the buffer. _size : int Current number of samples in the buffer. Examples -------- >>> window = SlidingWindow(max_length=100, n_columns=3) >>> window.append([1.0, 2.0, 3.0]) >>> window.append(np.array([4.0, 5.0, 6.0]), timestamp=1234567890.0) >>> data, timestamps = window.to_array() >>> print(f\"Buffer contains {len(window)} samples\") \"\"\" @property def max_length ( self ): return self . _max_length @max_length . setter def max_length ( self , value : int ): if value <= 0 : raise ValueError ( \"max_length must be positive.\" ) if value != self . _max_length : old_max_length = self . _max_length # keep old value self . _max_length = value self . _resize ( old_max_length ) def __init__ ( self , max_length : int , n_columns : int ): self . _max_length = validate_integer ( max_length , 'max_length' , min_val = 1 , max_val = 10_000_000 ) self . _n_columns = validate_integer ( n_columns , 'n_columns' , min_val = 1 , max_val = 10_000 ) self . _lock = threading . RLock () self . _buffer = np . empty (( self . _max_length , self . _n_columns ), dtype = np . float32 ) self . _timestamp = np . empty ( self . _max_length , dtype = np . float64 ) self . _start = 0 self . _size = 0 def __del__ ( self ): \"\"\"Clean up allocated numpy arrays when the object is destroyed. This helps ensure memory is released promptly rather than waiting for Python's garbage collector. \"\"\" # Explicitly delete numpy arrays to free memory if hasattr ( self , '_buffer' ): del self . _buffer if hasattr ( self , '_timestamp' ): del self . _timestamp def __len__ ( self ) -> int : \"\"\" Return the current number of samples in the sliding window. Returns ------- int Number of samples currently stored in the buffer. Examples -------- >>> window = SlidingWindow(max_length=10, n_columns=2) >>> print(len(window)) # 0 >>> window.append([1.0, 2.0]) >>> print(len(window)) # 1 \"\"\" with self . _lock : return self . _size def __repr__ ( self ) -> str : \"\"\" Return a concise representation showing max length, shape, and the samples with timestamps. \"\"\" data , timestamps = self . to_array () return f \"\"\"data= \\n { data } , \\n timestamps= \\n { timestamps } \"\"\" def _resize ( self , old_max_length : int ): # build indices with old_max_length, not the new one indices = ( self . _start + np . arange ( self . _size )) % old_max_length old_data = self . _buffer [ indices ] . copy () old_timestamps = self . _timestamp [ indices ] . copy () new_buffer = np . empty (( self . _max_length , self . _n_columns ), dtype = np . float32 ) new_timestamps = np . empty ( self . _max_length , dtype = np . float64 ) keep = min ( len ( old_data ), self . _max_length ) new_buffer [: keep , : self . _n_columns ] = old_data [ - keep :, : self . _n_columns ] new_timestamps [: keep ] = old_timestamps [ - keep :] self . _buffer = new_buffer self . _timestamp = new_timestamps self . _start = 0 self . _size = keep def append ( self , samples : Union [ np . ndarray , list ], timestamp : Optional [ float ] = None ) -> None : \"\"\" Append a new sample to the sliding window. If the buffer is not full, the sample is added to the next available position. If the buffer is full, the oldest sample is overwritten. Parameters ---------- samples : np.ndarray or list Sample data to append. Must have exactly n_columns elements. timestamp : float, optional Timestamp associated with the sample. If None, uses the current monotonic time. Raises ------ TypeError If samples is not a numpy array or list. ValueError If the sample shape doesn't match the expected number of columns. Examples -------- >>> window = SlidingWindow(max_length=10, n_columns=2) >>> window.append([1.0, 2.0]) >>> window.append(np.array([3.0, 4.0]), timestamp=1234567890.0) \"\"\" with self . _lock : if not isinstance ( samples , ( np . ndarray , list )): raise TypeError ( \"Expected sample should be of type np.ndarray or list.\" ) value = np . asarray ( samples , dtype = np . float32 ) . reshape ( - 1 ) if value . shape [ 0 ] != self . _n_columns : raise ValueError ( f \"Expected shape ( { self . _n_columns } ,), got { value . shape } \" ) if timestamp is None : timestamp = time . monotonic () if self . _size < self . _max_length : idx = ( self . _start + self . _size ) % self . _max_length self . _size += 1 else : idx = self . _start self . _start = ( self . _start + 1 ) % self . _max_length if idx >= self . _buffer . shape [ 0 ]: raise RuntimeError ( f \"Internal error: idx= { idx } but buffer length= { self . _buffer . shape [ 0 ] } \" ) self . _buffer [ idx ] = value self . _timestamp [ idx ] = timestamp def to_array ( self ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\" Return the current contents of the sliding window as arrays. The returned arrays contain samples and timestamps in chronological order, with the oldest sample first and the newest sample last. Returns ------- samples : np.ndarray Array of shape (current_size, n_columns) containing all samples in the buffer in chronological order. timestamps : np.ndarray Array of shape (current_size,) containing timestamps corresponding to each sample in chronological order. Examples -------- >>> window = SlidingWindow(max_length=5, n_columns=2) >>> window.append([1.0, 2.0]) >>> window.append([3.0, 4.0]) >>> samples, timestamps = window.to_array() >>> print(samples.shape) # (2, 2) >>> print(timestamps.shape) # (2,) \"\"\" with self . _lock : indices = ( self . _start + np . arange ( self . _size )) % self . _max_length return self . _buffer [ indices ] . copy (), self . _timestamp [ indices ] . copy () def reset ( self ) -> None : \"\"\" Reset the sliding window to empty state. Clears all samples and timestamps from the buffer and resets internal counters. The buffer arrays are filled with NaN values. Examples -------- >>> window = SlidingWindow(max_length=10, n_columns=2) >>> window.append([1.0, 2.0]) >>> print(len(window)) # 1 >>> window.reset() >>> print(len(window)) # 0 \"\"\" with self . _lock : self . _start = 0 self . _size = 0 self . _buffer . fill ( np . nan ) self . _timestamp . fill ( np . nan ) def is_full ( self ) -> bool : \"\"\" Check if the sliding window buffer is at maximum capacity. Returns ------- bool True if the buffer contains max_length samples, False otherwise. Examples -------- >>> window = SlidingWindow(max_length=2, n_columns=1) >>> print(window.is_full()) # False >>> window.append([1.0]) >>> window.append([2.0]) >>> print(window.is_full()) # True \"\"\" with self . _lock : return self . _size == self . _max_length __del__ () Clean up allocated numpy arrays when the object is destroyed. This helps ensure memory is released promptly rather than waiting for Python's garbage collector. Source code in pyeyesweb/data_models/sliding_window.py 74 75 76 77 78 79 80 81 82 83 84 def __del__ ( self ): \"\"\"Clean up allocated numpy arrays when the object is destroyed. This helps ensure memory is released promptly rather than waiting for Python's garbage collector. \"\"\" # Explicitly delete numpy arrays to free memory if hasattr ( self , '_buffer' ): del self . _buffer if hasattr ( self , '_timestamp' ): del self . _timestamp __len__ () Return the current number of samples in the sliding window. Returns: int \u2013 Number of samples currently stored in the buffer. Examples: >>> window = SlidingWindow ( max_length = 10 , n_columns = 2 ) >>> print ( len ( window )) # 0 >>> window . append ([ 1.0 , 2.0 ]) >>> print ( len ( window )) # 1 Source code in pyeyesweb/data_models/sliding_window.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __len__ ( self ) -> int : \"\"\" Return the current number of samples in the sliding window. Returns ------- int Number of samples currently stored in the buffer. Examples -------- >>> window = SlidingWindow(max_length=10, n_columns=2) >>> print(len(window)) # 0 >>> window.append([1.0, 2.0]) >>> print(len(window)) # 1 \"\"\" with self . _lock : return self . _size __repr__ () Return a concise representation showing max length, shape, and the samples with timestamps. Source code in pyeyesweb/data_models/sliding_window.py 105 106 107 108 109 110 def __repr__ ( self ) -> str : \"\"\" Return a concise representation showing max length, shape, and the samples with timestamps. \"\"\" data , timestamps = self . to_array () return f \"\"\"data= \\n { data } , \\n timestamps= \\n { timestamps } \"\"\" append ( samples , timestamp = None ) Append a new sample to the sliding window. If the buffer is not full, the sample is added to the next available position. If the buffer is full, the oldest sample is overwritten. Parameters: samples ( ndarray or list ) \u2013 Sample data to append. Must have exactly n_columns elements. timestamp ( float , default: None ) \u2013 Timestamp associated with the sample. If None, uses the current monotonic time. Raises: TypeError \u2013 If samples is not a numpy array or list. ValueError \u2013 If the sample shape doesn't match the expected number of columns. Examples: >>> window = SlidingWindow ( max_length = 10 , n_columns = 2 ) >>> window . append ([ 1.0 , 2.0 ]) >>> window . append ( np . array ([ 3.0 , 4.0 ]), timestamp = 1234567890.0 ) Source code in pyeyesweb/data_models/sliding_window.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def append ( self , samples : Union [ np . ndarray , list ], timestamp : Optional [ float ] = None ) -> None : \"\"\" Append a new sample to the sliding window. If the buffer is not full, the sample is added to the next available position. If the buffer is full, the oldest sample is overwritten. Parameters ---------- samples : np.ndarray or list Sample data to append. Must have exactly n_columns elements. timestamp : float, optional Timestamp associated with the sample. If None, uses the current monotonic time. Raises ------ TypeError If samples is not a numpy array or list. ValueError If the sample shape doesn't match the expected number of columns. Examples -------- >>> window = SlidingWindow(max_length=10, n_columns=2) >>> window.append([1.0, 2.0]) >>> window.append(np.array([3.0, 4.0]), timestamp=1234567890.0) \"\"\" with self . _lock : if not isinstance ( samples , ( np . ndarray , list )): raise TypeError ( \"Expected sample should be of type np.ndarray or list.\" ) value = np . asarray ( samples , dtype = np . float32 ) . reshape ( - 1 ) if value . shape [ 0 ] != self . _n_columns : raise ValueError ( f \"Expected shape ( { self . _n_columns } ,), got { value . shape } \" ) if timestamp is None : timestamp = time . monotonic () if self . _size < self . _max_length : idx = ( self . _start + self . _size ) % self . _max_length self . _size += 1 else : idx = self . _start self . _start = ( self . _start + 1 ) % self . _max_length if idx >= self . _buffer . shape [ 0 ]: raise RuntimeError ( f \"Internal error: idx= { idx } but buffer length= { self . _buffer . shape [ 0 ] } \" ) self . _buffer [ idx ] = value self . _timestamp [ idx ] = timestamp is_full () Check if the sliding window buffer is at maximum capacity. Returns: bool \u2013 True if the buffer contains max_length samples, False otherwise. Examples: >>> window = SlidingWindow ( max_length = 2 , n_columns = 1 ) >>> print ( window . is_full ()) # False >>> window . append ([ 1.0 ]) >>> window . append ([ 2.0 ]) >>> print ( window . is_full ()) # True Source code in pyeyesweb/data_models/sliding_window.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def is_full ( self ) -> bool : \"\"\" Check if the sliding window buffer is at maximum capacity. Returns ------- bool True if the buffer contains max_length samples, False otherwise. Examples -------- >>> window = SlidingWindow(max_length=2, n_columns=1) >>> print(window.is_full()) # False >>> window.append([1.0]) >>> window.append([2.0]) >>> print(window.is_full()) # True \"\"\" with self . _lock : return self . _size == self . _max_length reset () Reset the sliding window to empty state. Clears all samples and timestamps from the buffer and resets internal counters. The buffer arrays are filled with NaN values. Examples: >>> window = SlidingWindow ( max_length = 10 , n_columns = 2 ) >>> window . append ([ 1.0 , 2.0 ]) >>> print ( len ( window )) # 1 >>> window . reset () >>> print ( len ( window )) # 0 Source code in pyeyesweb/data_models/sliding_window.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def reset ( self ) -> None : \"\"\" Reset the sliding window to empty state. Clears all samples and timestamps from the buffer and resets internal counters. The buffer arrays are filled with NaN values. Examples -------- >>> window = SlidingWindow(max_length=10, n_columns=2) >>> window.append([1.0, 2.0]) >>> print(len(window)) # 1 >>> window.reset() >>> print(len(window)) # 0 \"\"\" with self . _lock : self . _start = 0 self . _size = 0 self . _buffer . fill ( np . nan ) self . _timestamp . fill ( np . nan ) to_array () Return the current contents of the sliding window as arrays. The returned arrays contain samples and timestamps in chronological order, with the oldest sample first and the newest sample last. Returns: samples ( ndarray ) \u2013 Array of shape (current_size, n_columns) containing all samples in the buffer in chronological order. timestamps ( ndarray ) \u2013 Array of shape (current_size,) containing timestamps corresponding to each sample in chronological order. Examples: >>> window = SlidingWindow ( max_length = 5 , n_columns = 2 ) >>> window . append ([ 1.0 , 2.0 ]) >>> window . append ([ 3.0 , 4.0 ]) >>> samples , timestamps = window . to_array () >>> print ( samples . shape ) # (2, 2) >>> print ( timestamps . shape ) # (2,) Source code in pyeyesweb/data_models/sliding_window.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def to_array ( self ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\" Return the current contents of the sliding window as arrays. The returned arrays contain samples and timestamps in chronological order, with the oldest sample first and the newest sample last. Returns ------- samples : np.ndarray Array of shape (current_size, n_columns) containing all samples in the buffer in chronological order. timestamps : np.ndarray Array of shape (current_size,) containing timestamps corresponding to each sample in chronological order. Examples -------- >>> window = SlidingWindow(max_length=5, n_columns=2) >>> window.append([1.0, 2.0]) >>> window.append([3.0, 4.0]) >>> samples, timestamps = window.to_array() >>> print(samples.shape) # (2, 2) >>> print(timestamps.shape) # (2,) \"\"\" with self . _lock : indices = ( self . _start + np . arange ( self . _size )) % self . _max_length return self . _buffer [ indices ] . copy (), self . _timestamp [ indices ] . copy ()","title":"Sliding Window"},{"location":"API/data_models/sliding_window/#sliding-window","text":"","title":"Sliding Window"},{"location":"API/data_models/sliding_window/#pyeyesweb.data_models.sliding_window.SlidingWindow","text":"A thread-safe sliding window buffer for storing samples with timestamps. This class implements a circular buffer that maintains a fixed-size window of the most recent samples. When the buffer is full, new samples overwrite the oldest ones. Each sample is associated with a timestamp. Parameters: max_length ( int ) \u2013 Maximum number of samples the window can hold. n_columns ( int ) \u2013 Number of columns (features) in each sample. Attributes: _lock ( Lock ) \u2013 Thread lock for thread-safe operations. _max_length ( int ) \u2013 Maximum capacity of the buffer. _n_columns ( int ) \u2013 Number of columns per sample. _buffer ( ndarray ) \u2013 Circular buffer storing the samples, shape (max_length, n_columns). _timestamp ( ndarray ) \u2013 Array storing timestamps for each sample, shape (max_length,). _start ( int ) \u2013 Index of the oldest sample in the buffer. _size ( int ) \u2013 Current number of samples in the buffer. Examples: >>> window = SlidingWindow ( max_length = 100 , n_columns = 3 ) >>> window . append ([ 1.0 , 2.0 , 3.0 ]) >>> window . append ( np . array ([ 4.0 , 5.0 , 6.0 ]), timestamp = 1234567890.0 ) >>> data , timestamps = window . to_array () >>> print ( f \"Buffer contains { len ( window ) } samples\" ) Source code in pyeyesweb/data_models/sliding_window.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 class SlidingWindow : \"\"\" A thread-safe sliding window buffer for storing samples with timestamps. This class implements a circular buffer that maintains a fixed-size window of the most recent samples. When the buffer is full, new samples overwrite the oldest ones. Each sample is associated with a timestamp. Parameters ---------- max_length : int Maximum number of samples the window can hold. n_columns : int Number of columns (features) in each sample. Attributes ---------- _lock : threading.Lock Thread lock for thread-safe operations. _max_length : int Maximum capacity of the buffer. _n_columns : int Number of columns per sample. _buffer : np.ndarray Circular buffer storing the samples, shape (max_length, n_columns). _timestamp : np.ndarray Array storing timestamps for each sample, shape (max_length,). _start : int Index of the oldest sample in the buffer. _size : int Current number of samples in the buffer. Examples -------- >>> window = SlidingWindow(max_length=100, n_columns=3) >>> window.append([1.0, 2.0, 3.0]) >>> window.append(np.array([4.0, 5.0, 6.0]), timestamp=1234567890.0) >>> data, timestamps = window.to_array() >>> print(f\"Buffer contains {len(window)} samples\") \"\"\" @property def max_length ( self ): return self . _max_length @max_length . setter def max_length ( self , value : int ): if value <= 0 : raise ValueError ( \"max_length must be positive.\" ) if value != self . _max_length : old_max_length = self . _max_length # keep old value self . _max_length = value self . _resize ( old_max_length ) def __init__ ( self , max_length : int , n_columns : int ): self . _max_length = validate_integer ( max_length , 'max_length' , min_val = 1 , max_val = 10_000_000 ) self . _n_columns = validate_integer ( n_columns , 'n_columns' , min_val = 1 , max_val = 10_000 ) self . _lock = threading . RLock () self . _buffer = np . empty (( self . _max_length , self . _n_columns ), dtype = np . float32 ) self . _timestamp = np . empty ( self . _max_length , dtype = np . float64 ) self . _start = 0 self . _size = 0 def __del__ ( self ): \"\"\"Clean up allocated numpy arrays when the object is destroyed. This helps ensure memory is released promptly rather than waiting for Python's garbage collector. \"\"\" # Explicitly delete numpy arrays to free memory if hasattr ( self , '_buffer' ): del self . _buffer if hasattr ( self , '_timestamp' ): del self . _timestamp def __len__ ( self ) -> int : \"\"\" Return the current number of samples in the sliding window. Returns ------- int Number of samples currently stored in the buffer. Examples -------- >>> window = SlidingWindow(max_length=10, n_columns=2) >>> print(len(window)) # 0 >>> window.append([1.0, 2.0]) >>> print(len(window)) # 1 \"\"\" with self . _lock : return self . _size def __repr__ ( self ) -> str : \"\"\" Return a concise representation showing max length, shape, and the samples with timestamps. \"\"\" data , timestamps = self . to_array () return f \"\"\"data= \\n { data } , \\n timestamps= \\n { timestamps } \"\"\" def _resize ( self , old_max_length : int ): # build indices with old_max_length, not the new one indices = ( self . _start + np . arange ( self . _size )) % old_max_length old_data = self . _buffer [ indices ] . copy () old_timestamps = self . _timestamp [ indices ] . copy () new_buffer = np . empty (( self . _max_length , self . _n_columns ), dtype = np . float32 ) new_timestamps = np . empty ( self . _max_length , dtype = np . float64 ) keep = min ( len ( old_data ), self . _max_length ) new_buffer [: keep , : self . _n_columns ] = old_data [ - keep :, : self . _n_columns ] new_timestamps [: keep ] = old_timestamps [ - keep :] self . _buffer = new_buffer self . _timestamp = new_timestamps self . _start = 0 self . _size = keep def append ( self , samples : Union [ np . ndarray , list ], timestamp : Optional [ float ] = None ) -> None : \"\"\" Append a new sample to the sliding window. If the buffer is not full, the sample is added to the next available position. If the buffer is full, the oldest sample is overwritten. Parameters ---------- samples : np.ndarray or list Sample data to append. Must have exactly n_columns elements. timestamp : float, optional Timestamp associated with the sample. If None, uses the current monotonic time. Raises ------ TypeError If samples is not a numpy array or list. ValueError If the sample shape doesn't match the expected number of columns. Examples -------- >>> window = SlidingWindow(max_length=10, n_columns=2) >>> window.append([1.0, 2.0]) >>> window.append(np.array([3.0, 4.0]), timestamp=1234567890.0) \"\"\" with self . _lock : if not isinstance ( samples , ( np . ndarray , list )): raise TypeError ( \"Expected sample should be of type np.ndarray or list.\" ) value = np . asarray ( samples , dtype = np . float32 ) . reshape ( - 1 ) if value . shape [ 0 ] != self . _n_columns : raise ValueError ( f \"Expected shape ( { self . _n_columns } ,), got { value . shape } \" ) if timestamp is None : timestamp = time . monotonic () if self . _size < self . _max_length : idx = ( self . _start + self . _size ) % self . _max_length self . _size += 1 else : idx = self . _start self . _start = ( self . _start + 1 ) % self . _max_length if idx >= self . _buffer . shape [ 0 ]: raise RuntimeError ( f \"Internal error: idx= { idx } but buffer length= { self . _buffer . shape [ 0 ] } \" ) self . _buffer [ idx ] = value self . _timestamp [ idx ] = timestamp def to_array ( self ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\" Return the current contents of the sliding window as arrays. The returned arrays contain samples and timestamps in chronological order, with the oldest sample first and the newest sample last. Returns ------- samples : np.ndarray Array of shape (current_size, n_columns) containing all samples in the buffer in chronological order. timestamps : np.ndarray Array of shape (current_size,) containing timestamps corresponding to each sample in chronological order. Examples -------- >>> window = SlidingWindow(max_length=5, n_columns=2) >>> window.append([1.0, 2.0]) >>> window.append([3.0, 4.0]) >>> samples, timestamps = window.to_array() >>> print(samples.shape) # (2, 2) >>> print(timestamps.shape) # (2,) \"\"\" with self . _lock : indices = ( self . _start + np . arange ( self . _size )) % self . _max_length return self . _buffer [ indices ] . copy (), self . _timestamp [ indices ] . copy () def reset ( self ) -> None : \"\"\" Reset the sliding window to empty state. Clears all samples and timestamps from the buffer and resets internal counters. The buffer arrays are filled with NaN values. Examples -------- >>> window = SlidingWindow(max_length=10, n_columns=2) >>> window.append([1.0, 2.0]) >>> print(len(window)) # 1 >>> window.reset() >>> print(len(window)) # 0 \"\"\" with self . _lock : self . _start = 0 self . _size = 0 self . _buffer . fill ( np . nan ) self . _timestamp . fill ( np . nan ) def is_full ( self ) -> bool : \"\"\" Check if the sliding window buffer is at maximum capacity. Returns ------- bool True if the buffer contains max_length samples, False otherwise. Examples -------- >>> window = SlidingWindow(max_length=2, n_columns=1) >>> print(window.is_full()) # False >>> window.append([1.0]) >>> window.append([2.0]) >>> print(window.is_full()) # True \"\"\" with self . _lock : return self . _size == self . _max_length","title":"SlidingWindow"},{"location":"API/data_models/sliding_window/#pyeyesweb.data_models.sliding_window.SlidingWindow.__del__","text":"Clean up allocated numpy arrays when the object is destroyed. This helps ensure memory is released promptly rather than waiting for Python's garbage collector. Source code in pyeyesweb/data_models/sliding_window.py 74 75 76 77 78 79 80 81 82 83 84 def __del__ ( self ): \"\"\"Clean up allocated numpy arrays when the object is destroyed. This helps ensure memory is released promptly rather than waiting for Python's garbage collector. \"\"\" # Explicitly delete numpy arrays to free memory if hasattr ( self , '_buffer' ): del self . _buffer if hasattr ( self , '_timestamp' ): del self . _timestamp","title":"__del__"},{"location":"API/data_models/sliding_window/#pyeyesweb.data_models.sliding_window.SlidingWindow.__len__","text":"Return the current number of samples in the sliding window. Returns: int \u2013 Number of samples currently stored in the buffer. Examples: >>> window = SlidingWindow ( max_length = 10 , n_columns = 2 ) >>> print ( len ( window )) # 0 >>> window . append ([ 1.0 , 2.0 ]) >>> print ( len ( window )) # 1 Source code in pyeyesweb/data_models/sliding_window.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __len__ ( self ) -> int : \"\"\" Return the current number of samples in the sliding window. Returns ------- int Number of samples currently stored in the buffer. Examples -------- >>> window = SlidingWindow(max_length=10, n_columns=2) >>> print(len(window)) # 0 >>> window.append([1.0, 2.0]) >>> print(len(window)) # 1 \"\"\" with self . _lock : return self . _size","title":"__len__"},{"location":"API/data_models/sliding_window/#pyeyesweb.data_models.sliding_window.SlidingWindow.__repr__","text":"Return a concise representation showing max length, shape, and the samples with timestamps. Source code in pyeyesweb/data_models/sliding_window.py 105 106 107 108 109 110 def __repr__ ( self ) -> str : \"\"\" Return a concise representation showing max length, shape, and the samples with timestamps. \"\"\" data , timestamps = self . to_array () return f \"\"\"data= \\n { data } , \\n timestamps= \\n { timestamps } \"\"\"","title":"__repr__"},{"location":"API/data_models/sliding_window/#pyeyesweb.data_models.sliding_window.SlidingWindow.append","text":"Append a new sample to the sliding window. If the buffer is not full, the sample is added to the next available position. If the buffer is full, the oldest sample is overwritten. Parameters: samples ( ndarray or list ) \u2013 Sample data to append. Must have exactly n_columns elements. timestamp ( float , default: None ) \u2013 Timestamp associated with the sample. If None, uses the current monotonic time. Raises: TypeError \u2013 If samples is not a numpy array or list. ValueError \u2013 If the sample shape doesn't match the expected number of columns. Examples: >>> window = SlidingWindow ( max_length = 10 , n_columns = 2 ) >>> window . append ([ 1.0 , 2.0 ]) >>> window . append ( np . array ([ 3.0 , 4.0 ]), timestamp = 1234567890.0 ) Source code in pyeyesweb/data_models/sliding_window.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def append ( self , samples : Union [ np . ndarray , list ], timestamp : Optional [ float ] = None ) -> None : \"\"\" Append a new sample to the sliding window. If the buffer is not full, the sample is added to the next available position. If the buffer is full, the oldest sample is overwritten. Parameters ---------- samples : np.ndarray or list Sample data to append. Must have exactly n_columns elements. timestamp : float, optional Timestamp associated with the sample. If None, uses the current monotonic time. Raises ------ TypeError If samples is not a numpy array or list. ValueError If the sample shape doesn't match the expected number of columns. Examples -------- >>> window = SlidingWindow(max_length=10, n_columns=2) >>> window.append([1.0, 2.0]) >>> window.append(np.array([3.0, 4.0]), timestamp=1234567890.0) \"\"\" with self . _lock : if not isinstance ( samples , ( np . ndarray , list )): raise TypeError ( \"Expected sample should be of type np.ndarray or list.\" ) value = np . asarray ( samples , dtype = np . float32 ) . reshape ( - 1 ) if value . shape [ 0 ] != self . _n_columns : raise ValueError ( f \"Expected shape ( { self . _n_columns } ,), got { value . shape } \" ) if timestamp is None : timestamp = time . monotonic () if self . _size < self . _max_length : idx = ( self . _start + self . _size ) % self . _max_length self . _size += 1 else : idx = self . _start self . _start = ( self . _start + 1 ) % self . _max_length if idx >= self . _buffer . shape [ 0 ]: raise RuntimeError ( f \"Internal error: idx= { idx } but buffer length= { self . _buffer . shape [ 0 ] } \" ) self . _buffer [ idx ] = value self . _timestamp [ idx ] = timestamp","title":"append"},{"location":"API/data_models/sliding_window/#pyeyesweb.data_models.sliding_window.SlidingWindow.is_full","text":"Check if the sliding window buffer is at maximum capacity. Returns: bool \u2013 True if the buffer contains max_length samples, False otherwise. Examples: >>> window = SlidingWindow ( max_length = 2 , n_columns = 1 ) >>> print ( window . is_full ()) # False >>> window . append ([ 1.0 ]) >>> window . append ([ 2.0 ]) >>> print ( window . is_full ()) # True Source code in pyeyesweb/data_models/sliding_window.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def is_full ( self ) -> bool : \"\"\" Check if the sliding window buffer is at maximum capacity. Returns ------- bool True if the buffer contains max_length samples, False otherwise. Examples -------- >>> window = SlidingWindow(max_length=2, n_columns=1) >>> print(window.is_full()) # False >>> window.append([1.0]) >>> window.append([2.0]) >>> print(window.is_full()) # True \"\"\" with self . _lock : return self . _size == self . _max_length","title":"is_full"},{"location":"API/data_models/sliding_window/#pyeyesweb.data_models.sliding_window.SlidingWindow.reset","text":"Reset the sliding window to empty state. Clears all samples and timestamps from the buffer and resets internal counters. The buffer arrays are filled with NaN values. Examples: >>> window = SlidingWindow ( max_length = 10 , n_columns = 2 ) >>> window . append ([ 1.0 , 2.0 ]) >>> print ( len ( window )) # 1 >>> window . reset () >>> print ( len ( window )) # 0 Source code in pyeyesweb/data_models/sliding_window.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def reset ( self ) -> None : \"\"\" Reset the sliding window to empty state. Clears all samples and timestamps from the buffer and resets internal counters. The buffer arrays are filled with NaN values. Examples -------- >>> window = SlidingWindow(max_length=10, n_columns=2) >>> window.append([1.0, 2.0]) >>> print(len(window)) # 1 >>> window.reset() >>> print(len(window)) # 0 \"\"\" with self . _lock : self . _start = 0 self . _size = 0 self . _buffer . fill ( np . nan ) self . _timestamp . fill ( np . nan )","title":"reset"},{"location":"API/data_models/sliding_window/#pyeyesweb.data_models.sliding_window.SlidingWindow.to_array","text":"Return the current contents of the sliding window as arrays. The returned arrays contain samples and timestamps in chronological order, with the oldest sample first and the newest sample last. Returns: samples ( ndarray ) \u2013 Array of shape (current_size, n_columns) containing all samples in the buffer in chronological order. timestamps ( ndarray ) \u2013 Array of shape (current_size,) containing timestamps corresponding to each sample in chronological order. Examples: >>> window = SlidingWindow ( max_length = 5 , n_columns = 2 ) >>> window . append ([ 1.0 , 2.0 ]) >>> window . append ([ 3.0 , 4.0 ]) >>> samples , timestamps = window . to_array () >>> print ( samples . shape ) # (2, 2) >>> print ( timestamps . shape ) # (2,) Source code in pyeyesweb/data_models/sliding_window.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def to_array ( self ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\" Return the current contents of the sliding window as arrays. The returned arrays contain samples and timestamps in chronological order, with the oldest sample first and the newest sample last. Returns ------- samples : np.ndarray Array of shape (current_size, n_columns) containing all samples in the buffer in chronological order. timestamps : np.ndarray Array of shape (current_size,) containing timestamps corresponding to each sample in chronological order. Examples -------- >>> window = SlidingWindow(max_length=5, n_columns=2) >>> window.append([1.0, 2.0]) >>> window.append([3.0, 4.0]) >>> samples, timestamps = window.to_array() >>> print(samples.shape) # (2, 2) >>> print(timestamps.shape) # (2,) \"\"\" with self . _lock : indices = ( self . _start + np . arange ( self . _size )) % self . _max_length return self . _buffer [ indices ] . copy (), self . _timestamp [ indices ] . copy ()","title":"to_array"},{"location":"API/data_models/thread_safe_buffer/","text":"Thread Safe Buffer Thread-safe history buffer for PyEyesWeb. This module provides a reusable thread-safe buffer implementation. ThreadSafeHistoryBuffer Thread-safe history buffer with deque backing. Parameters: maxlen ( int ) \u2013 Maximum size of the history buffer Examples: >>> buffer = ThreadSafeHistoryBuffer ( maxlen = 100 ) >>> buffer . append ( data ) >>> history = buffer . get_history () >>> length = len ( buffer ) Source code in pyeyesweb/data_models/thread_safe_buffer.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class ThreadSafeHistoryBuffer : \"\"\"Thread-safe history buffer with deque backing. Parameters ---------- maxlen : int Maximum size of the history buffer Examples -------- >>> buffer = ThreadSafeHistoryBuffer(maxlen=100) >>> buffer.append(data) >>> history = buffer.get_history() >>> length = len(buffer) \"\"\" def __init__ ( self , maxlen : int ): \"\"\"Initialize thread-safe history buffer. Parameters ---------- maxlen : int Maximum size of the buffer \"\"\" self . _history = deque ( maxlen = maxlen ) self . _lock = threading . RLock () def append ( self , item : Any ) -> None : \"\"\"Thread-safely append item to history. Parameters ---------- item : Any Item to append to the history buffer \"\"\" with self . _lock : self . _history . append ( item ) def get_history ( self ) -> List [ Any ]: \"\"\"Get a thread-safe copy of the history. Returns ------- list Copy of the current history \"\"\" with self . _lock : return list ( self . _history ) def get_array ( self ) -> np . ndarray : \"\"\"Get history as numpy array (thread-safe). Returns ------- np.ndarray History converted to numpy array \"\"\" with self . _lock : return np . array ( list ( self . _history )) def clear ( self ) -> None : \"\"\"Clear the history buffer (thread-safe).\"\"\" with self . _lock : self . _history . clear () def __len__ ( self ) -> int : \"\"\"Get current size of buffer (thread-safe). Returns ------- int Current number of items in buffer \"\"\" with self . _lock : return len ( self . _history ) def __repr__ ( self ) -> str : \"\"\"String representation of buffer.\"\"\" with self . _lock : return f \"ThreadSafeHistoryBuffer(maxlen= { self . _history . maxlen } , size= { len ( self . _history ) } )\" __init__ ( maxlen ) Initialize thread-safe history buffer. Parameters: maxlen ( int ) \u2013 Maximum size of the buffer Source code in pyeyesweb/data_models/thread_safe_buffer.py 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , maxlen : int ): \"\"\"Initialize thread-safe history buffer. Parameters ---------- maxlen : int Maximum size of the buffer \"\"\" self . _history = deque ( maxlen = maxlen ) self . _lock = threading . RLock () __len__ () Get current size of buffer (thread-safe). Returns: int \u2013 Current number of items in buffer Source code in pyeyesweb/data_models/thread_safe_buffer.py 77 78 79 80 81 82 83 84 85 86 def __len__ ( self ) -> int : \"\"\"Get current size of buffer (thread-safe). Returns ------- int Current number of items in buffer \"\"\" with self . _lock : return len ( self . _history ) __repr__ () String representation of buffer. Source code in pyeyesweb/data_models/thread_safe_buffer.py 88 89 90 91 def __repr__ ( self ) -> str : \"\"\"String representation of buffer.\"\"\" with self . _lock : return f \"ThreadSafeHistoryBuffer(maxlen= { self . _history . maxlen } , size= { len ( self . _history ) } )\" append ( item ) Thread-safely append item to history. Parameters: item ( Any ) \u2013 Item to append to the history buffer Source code in pyeyesweb/data_models/thread_safe_buffer.py 39 40 41 42 43 44 45 46 47 48 def append ( self , item : Any ) -> None : \"\"\"Thread-safely append item to history. Parameters ---------- item : Any Item to append to the history buffer \"\"\" with self . _lock : self . _history . append ( item ) clear () Clear the history buffer (thread-safe). Source code in pyeyesweb/data_models/thread_safe_buffer.py 72 73 74 75 def clear ( self ) -> None : \"\"\"Clear the history buffer (thread-safe).\"\"\" with self . _lock : self . _history . clear () get_array () Get history as numpy array (thread-safe). Returns: ndarray \u2013 History converted to numpy array Source code in pyeyesweb/data_models/thread_safe_buffer.py 61 62 63 64 65 66 67 68 69 70 def get_array ( self ) -> np . ndarray : \"\"\"Get history as numpy array (thread-safe). Returns ------- np.ndarray History converted to numpy array \"\"\" with self . _lock : return np . array ( list ( self . _history )) get_history () Get a thread-safe copy of the history. Returns: list \u2013 Copy of the current history Source code in pyeyesweb/data_models/thread_safe_buffer.py 50 51 52 53 54 55 56 57 58 59 def get_history ( self ) -> List [ Any ]: \"\"\"Get a thread-safe copy of the history. Returns ------- list Copy of the current history \"\"\" with self . _lock : return list ( self . _history )","title":"Thread Safe Buffer"},{"location":"API/data_models/thread_safe_buffer/#thread-safe-buffer","text":"Thread-safe history buffer for PyEyesWeb. This module provides a reusable thread-safe buffer implementation.","title":"Thread Safe Buffer"},{"location":"API/data_models/thread_safe_buffer/#pyeyesweb.data_models.thread_safe_buffer.ThreadSafeHistoryBuffer","text":"Thread-safe history buffer with deque backing. Parameters: maxlen ( int ) \u2013 Maximum size of the history buffer Examples: >>> buffer = ThreadSafeHistoryBuffer ( maxlen = 100 ) >>> buffer . append ( data ) >>> history = buffer . get_history () >>> length = len ( buffer ) Source code in pyeyesweb/data_models/thread_safe_buffer.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class ThreadSafeHistoryBuffer : \"\"\"Thread-safe history buffer with deque backing. Parameters ---------- maxlen : int Maximum size of the history buffer Examples -------- >>> buffer = ThreadSafeHistoryBuffer(maxlen=100) >>> buffer.append(data) >>> history = buffer.get_history() >>> length = len(buffer) \"\"\" def __init__ ( self , maxlen : int ): \"\"\"Initialize thread-safe history buffer. Parameters ---------- maxlen : int Maximum size of the buffer \"\"\" self . _history = deque ( maxlen = maxlen ) self . _lock = threading . RLock () def append ( self , item : Any ) -> None : \"\"\"Thread-safely append item to history. Parameters ---------- item : Any Item to append to the history buffer \"\"\" with self . _lock : self . _history . append ( item ) def get_history ( self ) -> List [ Any ]: \"\"\"Get a thread-safe copy of the history. Returns ------- list Copy of the current history \"\"\" with self . _lock : return list ( self . _history ) def get_array ( self ) -> np . ndarray : \"\"\"Get history as numpy array (thread-safe). Returns ------- np.ndarray History converted to numpy array \"\"\" with self . _lock : return np . array ( list ( self . _history )) def clear ( self ) -> None : \"\"\"Clear the history buffer (thread-safe).\"\"\" with self . _lock : self . _history . clear () def __len__ ( self ) -> int : \"\"\"Get current size of buffer (thread-safe). Returns ------- int Current number of items in buffer \"\"\" with self . _lock : return len ( self . _history ) def __repr__ ( self ) -> str : \"\"\"String representation of buffer.\"\"\" with self . _lock : return f \"ThreadSafeHistoryBuffer(maxlen= { self . _history . maxlen } , size= { len ( self . _history ) } )\"","title":"ThreadSafeHistoryBuffer"},{"location":"API/data_models/thread_safe_buffer/#pyeyesweb.data_models.thread_safe_buffer.ThreadSafeHistoryBuffer.__init__","text":"Initialize thread-safe history buffer. Parameters: maxlen ( int ) \u2013 Maximum size of the buffer Source code in pyeyesweb/data_models/thread_safe_buffer.py 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , maxlen : int ): \"\"\"Initialize thread-safe history buffer. Parameters ---------- maxlen : int Maximum size of the buffer \"\"\" self . _history = deque ( maxlen = maxlen ) self . _lock = threading . RLock ()","title":"__init__"},{"location":"API/data_models/thread_safe_buffer/#pyeyesweb.data_models.thread_safe_buffer.ThreadSafeHistoryBuffer.__len__","text":"Get current size of buffer (thread-safe). Returns: int \u2013 Current number of items in buffer Source code in pyeyesweb/data_models/thread_safe_buffer.py 77 78 79 80 81 82 83 84 85 86 def __len__ ( self ) -> int : \"\"\"Get current size of buffer (thread-safe). Returns ------- int Current number of items in buffer \"\"\" with self . _lock : return len ( self . _history )","title":"__len__"},{"location":"API/data_models/thread_safe_buffer/#pyeyesweb.data_models.thread_safe_buffer.ThreadSafeHistoryBuffer.__repr__","text":"String representation of buffer. Source code in pyeyesweb/data_models/thread_safe_buffer.py 88 89 90 91 def __repr__ ( self ) -> str : \"\"\"String representation of buffer.\"\"\" with self . _lock : return f \"ThreadSafeHistoryBuffer(maxlen= { self . _history . maxlen } , size= { len ( self . _history ) } )\"","title":"__repr__"},{"location":"API/data_models/thread_safe_buffer/#pyeyesweb.data_models.thread_safe_buffer.ThreadSafeHistoryBuffer.append","text":"Thread-safely append item to history. Parameters: item ( Any ) \u2013 Item to append to the history buffer Source code in pyeyesweb/data_models/thread_safe_buffer.py 39 40 41 42 43 44 45 46 47 48 def append ( self , item : Any ) -> None : \"\"\"Thread-safely append item to history. Parameters ---------- item : Any Item to append to the history buffer \"\"\" with self . _lock : self . _history . append ( item )","title":"append"},{"location":"API/data_models/thread_safe_buffer/#pyeyesweb.data_models.thread_safe_buffer.ThreadSafeHistoryBuffer.clear","text":"Clear the history buffer (thread-safe). Source code in pyeyesweb/data_models/thread_safe_buffer.py 72 73 74 75 def clear ( self ) -> None : \"\"\"Clear the history buffer (thread-safe).\"\"\" with self . _lock : self . _history . clear ()","title":"clear"},{"location":"API/data_models/thread_safe_buffer/#pyeyesweb.data_models.thread_safe_buffer.ThreadSafeHistoryBuffer.get_array","text":"Get history as numpy array (thread-safe). Returns: ndarray \u2013 History converted to numpy array Source code in pyeyesweb/data_models/thread_safe_buffer.py 61 62 63 64 65 66 67 68 69 70 def get_array ( self ) -> np . ndarray : \"\"\"Get history as numpy array (thread-safe). Returns ------- np.ndarray History converted to numpy array \"\"\" with self . _lock : return np . array ( list ( self . _history ))","title":"get_array"},{"location":"API/data_models/thread_safe_buffer/#pyeyesweb.data_models.thread_safe_buffer.ThreadSafeHistoryBuffer.get_history","text":"Get a thread-safe copy of the history. Returns: list \u2013 Copy of the current history Source code in pyeyesweb/data_models/thread_safe_buffer.py 50 51 52 53 54 55 56 57 58 59 def get_history ( self ) -> List [ Any ]: \"\"\"Get a thread-safe copy of the history. Returns ------- list Copy of the current history \"\"\" with self . _lock : return list ( self . _history )","title":"get_history"},{"location":"API/low_level/contraction_expansion/","text":"Contraction Expansion Contraction and expansion analysis for body movement patterns. This module provides optimized functions for analyzing contraction and expansion of body configurations in 2D and 3D space. It computes area (2D) or volume (3D) metrics for sets of body points and tracks changes relative to a baseline. The module uses Numba JIT compilation for performance optimization, making it suitable for real-time motion capture analysis. Key Features Fast area calculation using Shoelace formula (2D) Tetrahedron volume calculation using determinants (3D) Baseline-relative expansion/contraction indices Support for both single-frame and time-series analysis Automatic warmup for JIT compilation Typical Applications Dance movement analysis (body expansion/contraction) Gesture recognition (hand/arm configurations) Sports biomechanics (body positioning) Clinical movement assessment ContractionExpansion Analyze body movement contraction/expansion patterns. This class provides a standardized API for computing area (2D) or volume (3D) metrics for body point configurations and tracking expansion/contraction relative to a baseline. Parameters: mode ( ('2D', '3D', None) , default: \"2D\" ) \u2013 Analysis mode. If None, auto-detects from data dimensions. baseline_frame ( int , default: 0 ) \u2013 Frame index to use as baseline for time series (default: 0). Examples: >>> # Create analyzer >>> ce = ContractionExpansion ( mode = \"2D\" ) >>> >>> # Single frame analysis >>> points_2d = np . array ([[ 0 , 0 ], [ 1 , 0 ], [ 1 , 1 ], [ 0 , 1 ]]) >>> result = ce ( points_2d ) >>> print ( result [ 'metric' ]) # Area of square 1.0 >>> # Time series analysis >>> ce_3d = ContractionExpansion ( mode = \"3D\" , baseline_frame = 0 ) >>> frames = np . random . randn ( 100 , 4 , 3 ) >>> result = ce_3d ( frames ) >>> print ( result [ 'states' ][: 10 ]) # First 10 frame states Source code in pyeyesweb/low_level/contraction_expansion.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 class ContractionExpansion : \"\"\"Analyze body movement contraction/expansion patterns. This class provides a standardized API for computing area (2D) or volume (3D) metrics for body point configurations and tracking expansion/contraction relative to a baseline. Parameters ---------- mode : {\"2D\", \"3D\", None}, optional Analysis mode. If None, auto-detects from data dimensions. baseline_frame : int, optional Frame index to use as baseline for time series (default: 0). Examples -------- >>> # Create analyzer >>> ce = ContractionExpansion(mode=\"2D\") >>> >>> # Single frame analysis >>> points_2d = np.array([[0, 0], [1, 0], [1, 1], [0, 1]]) >>> result = ce(points_2d) >>> print(result['metric']) # Area of square 1.0 >>> # Time series analysis >>> ce_3d = ContractionExpansion(mode=\"3D\", baseline_frame=0) >>> frames = np.random.randn(100, 4, 3) >>> result = ce_3d(frames) >>> print(result['states'][:10]) # First 10 frame states \"\"\" def __init__ ( self , mode = None , baseline_frame = 0 ): self . mode = mode self . baseline_frame = baseline_frame def __call__ ( self , data ): \"\"\"Analyze movement data using the configured settings. Parameters ---------- data : ndarray Either single frame (4, 2) or (4, 3) for 2D/3D points, or time series (n_frames, 4, 2) or (n_frames, 4, 3). Returns ------- dict For single frame: - 'metric': area or volume value - 'dimension': \"2D\" or \"3D\" For time series: - 'metrics': array of area/volume values - 'indices': array of expansion indices relative to baseline - 'states': array of states (-1=contraction, 0=neutral, 1=expansion) - 'dimension': \"2D\" or \"3D\" \"\"\" return analyze_movement ( data , mode = self . mode , baseline_frame = self . baseline_frame ) __call__ ( data ) Analyze movement data using the configured settings. Parameters: data ( ndarray ) \u2013 Either single frame (4, 2) or (4, 3) for 2D/3D points, or time series (n_frames, 4, 2) or (n_frames, 4, 3). Returns: dict \u2013 For single frame: - 'metric': area or volume value - 'dimension': \"2D\" or \"3D\" For time series: - 'metrics': array of area/volume values - 'indices': array of expansion indices relative to baseline - 'states': array of states (-1=contraction, 0=neutral, 1=expansion) - 'dimension': \"2D\" or \"3D\" Source code in pyeyesweb/low_level/contraction_expansion.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def __call__ ( self , data ): \"\"\"Analyze movement data using the configured settings. Parameters ---------- data : ndarray Either single frame (4, 2) or (4, 3) for 2D/3D points, or time series (n_frames, 4, 2) or (n_frames, 4, 3). Returns ------- dict For single frame: - 'metric': area or volume value - 'dimension': \"2D\" or \"3D\" For time series: - 'metrics': array of area/volume values - 'indices': array of expansion indices relative to baseline - 'states': array of states (-1=contraction, 0=neutral, 1=expansion) - 'dimension': \"2D\" or \"3D\" \"\"\" return analyze_movement ( data , mode = self . mode , baseline_frame = self . baseline_frame ) analyze_movement ( data , mode = None , baseline_frame = 0 ) Analyze body movement contraction/expansion patterns. This function computes area (2D) or volume (3D) metrics for body point configurations and tracks expansion/contraction relative to a baseline. Parameters: data ( ndarray ) \u2013 Either single frame (4, 2) or (4, 3) for 2D/3D points, or time series (n_frames, 4, 2) or (n_frames, 4, 3). mode ( ('2D', '3D', None) , default: \"2D\" ) \u2013 Analysis mode. If None, auto-detects from data dimensions. baseline_frame ( int , default: 0 ) \u2013 Frame index to use as baseline for time series (default: 0). Returns: dict \u2013 For single frame: - 'metric': area or volume value - 'dimension': \"2D\" or \"3D\" For time series: - 'metrics': array of area/volume values - 'indices': array of expansion indices relative to baseline - 'states': array of states (-1=contraction, 0=neutral, 1=expansion) - 'dimension': \"2D\" or \"3D\" Raises: ValueError \u2013 If data shape is invalid or mode doesn't match data dimensions. Examples: >>> # Single frame 2D analysis >>> points_2d = np . array ([[ 0 , 0 ], [ 1 , 0 ], [ 1 , 1 ], [ 0 , 1 ]]) >>> result = analyze_movement ( points_2d , mode = \"2D\" ) >>> print ( result [ 'metric' ]) # Area of square 1.0 >>> # Time series 3D analysis >>> frames = np . random . randn ( 100 , 4 , 3 ) >>> result = analyze_movement ( frames , mode = \"3D\" , baseline_frame = 0 ) >>> print ( result [ 'states' ][: 10 ]) # First 10 frame states Source code in pyeyesweb/low_level/contraction_expansion.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 def analyze_movement ( data , mode = None , baseline_frame = 0 ): \"\"\"Analyze body movement contraction/expansion patterns. This function computes area (2D) or volume (3D) metrics for body point configurations and tracks expansion/contraction relative to a baseline. Parameters ---------- data : ndarray Either single frame (4, 2) or (4, 3) for 2D/3D points, or time series (n_frames, 4, 2) or (n_frames, 4, 3). mode : {\"2D\", \"3D\", None}, optional Analysis mode. If None, auto-detects from data dimensions. baseline_frame : int, optional Frame index to use as baseline for time series (default: 0). Returns ------- dict For single frame: - 'metric': area or volume value - 'dimension': \"2D\" or \"3D\" For time series: - 'metrics': array of area/volume values - 'indices': array of expansion indices relative to baseline - 'states': array of states (-1=contraction, 0=neutral, 1=expansion) - 'dimension': \"2D\" or \"3D\" Raises ------ ValueError If data shape is invalid or mode doesn't match data dimensions. Examples -------- >>> # Single frame 2D analysis >>> points_2d = np.array([[0, 0], [1, 0], [1, 1], [0, 1]]) >>> result = analyze_movement(points_2d, mode=\"2D\") >>> print(result['metric']) # Area of square 1.0 >>> # Time series 3D analysis >>> frames = np.random.randn(100, 4, 3) >>> result = analyze_movement(frames, mode=\"3D\", baseline_frame=0) >>> print(result['states'][:10]) # First 10 frame states \"\"\" if data . ndim == 2 : dims = data . shape [ 1 ] is_timeseries = False elif data . ndim == 3 : dims = data . shape [ 2 ] is_timeseries = True if data . shape [ 1 ] != 4 : raise ValueError ( \"Invalid shape: second dimension must be 4\" ) else : raise ValueError ( \"Invalid data dimensions\" ) if mode is None : mode = \"2D\" if dims == 2 else \"3D\" if dims == 3 else None if mode is None : raise ValueError ( \"Invalid coordinate dimensions\" ) expected_dims = 2 if mode == \"2D\" else 3 if dims != expected_dims : raise ValueError ( f \"Mode { mode } requires { expected_dims } D data\" ) if not is_timeseries : if mode == \"2D\" : metric = _area_2d_fast ( data ) else : metric = _volume_3d_fast ( data ) return { \"metric\" : metric , \"dimension\" : mode } if mode == \"2D\" : metrics , indices , states = _process_timeseries_2d ( data , baseline_frame ) else : metrics , indices , states = _process_timeseries_3d ( data , baseline_frame ) return { \"metrics\" : metrics , \"indices\" : indices , \"states\" : states , \"dimension\" : mode }","title":"Contraction Expansion"},{"location":"API/low_level/contraction_expansion/#contraction-expansion","text":"Contraction and expansion analysis for body movement patterns. This module provides optimized functions for analyzing contraction and expansion of body configurations in 2D and 3D space. It computes area (2D) or volume (3D) metrics for sets of body points and tracks changes relative to a baseline. The module uses Numba JIT compilation for performance optimization, making it suitable for real-time motion capture analysis. Key Features Fast area calculation using Shoelace formula (2D) Tetrahedron volume calculation using determinants (3D) Baseline-relative expansion/contraction indices Support for both single-frame and time-series analysis Automatic warmup for JIT compilation Typical Applications Dance movement analysis (body expansion/contraction) Gesture recognition (hand/arm configurations) Sports biomechanics (body positioning) Clinical movement assessment","title":"Contraction Expansion"},{"location":"API/low_level/contraction_expansion/#pyeyesweb.low_level.contraction_expansion.ContractionExpansion","text":"Analyze body movement contraction/expansion patterns. This class provides a standardized API for computing area (2D) or volume (3D) metrics for body point configurations and tracking expansion/contraction relative to a baseline. Parameters: mode ( ('2D', '3D', None) , default: \"2D\" ) \u2013 Analysis mode. If None, auto-detects from data dimensions. baseline_frame ( int , default: 0 ) \u2013 Frame index to use as baseline for time series (default: 0). Examples: >>> # Create analyzer >>> ce = ContractionExpansion ( mode = \"2D\" ) >>> >>> # Single frame analysis >>> points_2d = np . array ([[ 0 , 0 ], [ 1 , 0 ], [ 1 , 1 ], [ 0 , 1 ]]) >>> result = ce ( points_2d ) >>> print ( result [ 'metric' ]) # Area of square 1.0 >>> # Time series analysis >>> ce_3d = ContractionExpansion ( mode = \"3D\" , baseline_frame = 0 ) >>> frames = np . random . randn ( 100 , 4 , 3 ) >>> result = ce_3d ( frames ) >>> print ( result [ 'states' ][: 10 ]) # First 10 frame states Source code in pyeyesweb/low_level/contraction_expansion.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 class ContractionExpansion : \"\"\"Analyze body movement contraction/expansion patterns. This class provides a standardized API for computing area (2D) or volume (3D) metrics for body point configurations and tracking expansion/contraction relative to a baseline. Parameters ---------- mode : {\"2D\", \"3D\", None}, optional Analysis mode. If None, auto-detects from data dimensions. baseline_frame : int, optional Frame index to use as baseline for time series (default: 0). Examples -------- >>> # Create analyzer >>> ce = ContractionExpansion(mode=\"2D\") >>> >>> # Single frame analysis >>> points_2d = np.array([[0, 0], [1, 0], [1, 1], [0, 1]]) >>> result = ce(points_2d) >>> print(result['metric']) # Area of square 1.0 >>> # Time series analysis >>> ce_3d = ContractionExpansion(mode=\"3D\", baseline_frame=0) >>> frames = np.random.randn(100, 4, 3) >>> result = ce_3d(frames) >>> print(result['states'][:10]) # First 10 frame states \"\"\" def __init__ ( self , mode = None , baseline_frame = 0 ): self . mode = mode self . baseline_frame = baseline_frame def __call__ ( self , data ): \"\"\"Analyze movement data using the configured settings. Parameters ---------- data : ndarray Either single frame (4, 2) or (4, 3) for 2D/3D points, or time series (n_frames, 4, 2) or (n_frames, 4, 3). Returns ------- dict For single frame: - 'metric': area or volume value - 'dimension': \"2D\" or \"3D\" For time series: - 'metrics': array of area/volume values - 'indices': array of expansion indices relative to baseline - 'states': array of states (-1=contraction, 0=neutral, 1=expansion) - 'dimension': \"2D\" or \"3D\" \"\"\" return analyze_movement ( data , mode = self . mode , baseline_frame = self . baseline_frame )","title":"ContractionExpansion"},{"location":"API/low_level/contraction_expansion/#pyeyesweb.low_level.contraction_expansion.ContractionExpansion.__call__","text":"Analyze movement data using the configured settings. Parameters: data ( ndarray ) \u2013 Either single frame (4, 2) or (4, 3) for 2D/3D points, or time series (n_frames, 4, 2) or (n_frames, 4, 3). Returns: dict \u2013 For single frame: - 'metric': area or volume value - 'dimension': \"2D\" or \"3D\" For time series: - 'metrics': array of area/volume values - 'indices': array of expansion indices relative to baseline - 'states': array of states (-1=contraction, 0=neutral, 1=expansion) - 'dimension': \"2D\" or \"3D\" Source code in pyeyesweb/low_level/contraction_expansion.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def __call__ ( self , data ): \"\"\"Analyze movement data using the configured settings. Parameters ---------- data : ndarray Either single frame (4, 2) or (4, 3) for 2D/3D points, or time series (n_frames, 4, 2) or (n_frames, 4, 3). Returns ------- dict For single frame: - 'metric': area or volume value - 'dimension': \"2D\" or \"3D\" For time series: - 'metrics': array of area/volume values - 'indices': array of expansion indices relative to baseline - 'states': array of states (-1=contraction, 0=neutral, 1=expansion) - 'dimension': \"2D\" or \"3D\" \"\"\" return analyze_movement ( data , mode = self . mode , baseline_frame = self . baseline_frame )","title":"__call__"},{"location":"API/low_level/contraction_expansion/#pyeyesweb.low_level.contraction_expansion.analyze_movement","text":"Analyze body movement contraction/expansion patterns. This function computes area (2D) or volume (3D) metrics for body point configurations and tracks expansion/contraction relative to a baseline. Parameters: data ( ndarray ) \u2013 Either single frame (4, 2) or (4, 3) for 2D/3D points, or time series (n_frames, 4, 2) or (n_frames, 4, 3). mode ( ('2D', '3D', None) , default: \"2D\" ) \u2013 Analysis mode. If None, auto-detects from data dimensions. baseline_frame ( int , default: 0 ) \u2013 Frame index to use as baseline for time series (default: 0). Returns: dict \u2013 For single frame: - 'metric': area or volume value - 'dimension': \"2D\" or \"3D\" For time series: - 'metrics': array of area/volume values - 'indices': array of expansion indices relative to baseline - 'states': array of states (-1=contraction, 0=neutral, 1=expansion) - 'dimension': \"2D\" or \"3D\" Raises: ValueError \u2013 If data shape is invalid or mode doesn't match data dimensions. Examples: >>> # Single frame 2D analysis >>> points_2d = np . array ([[ 0 , 0 ], [ 1 , 0 ], [ 1 , 1 ], [ 0 , 1 ]]) >>> result = analyze_movement ( points_2d , mode = \"2D\" ) >>> print ( result [ 'metric' ]) # Area of square 1.0 >>> # Time series 3D analysis >>> frames = np . random . randn ( 100 , 4 , 3 ) >>> result = analyze_movement ( frames , mode = \"3D\" , baseline_frame = 0 ) >>> print ( result [ 'states' ][: 10 ]) # First 10 frame states Source code in pyeyesweb/low_level/contraction_expansion.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 def analyze_movement ( data , mode = None , baseline_frame = 0 ): \"\"\"Analyze body movement contraction/expansion patterns. This function computes area (2D) or volume (3D) metrics for body point configurations and tracks expansion/contraction relative to a baseline. Parameters ---------- data : ndarray Either single frame (4, 2) or (4, 3) for 2D/3D points, or time series (n_frames, 4, 2) or (n_frames, 4, 3). mode : {\"2D\", \"3D\", None}, optional Analysis mode. If None, auto-detects from data dimensions. baseline_frame : int, optional Frame index to use as baseline for time series (default: 0). Returns ------- dict For single frame: - 'metric': area or volume value - 'dimension': \"2D\" or \"3D\" For time series: - 'metrics': array of area/volume values - 'indices': array of expansion indices relative to baseline - 'states': array of states (-1=contraction, 0=neutral, 1=expansion) - 'dimension': \"2D\" or \"3D\" Raises ------ ValueError If data shape is invalid or mode doesn't match data dimensions. Examples -------- >>> # Single frame 2D analysis >>> points_2d = np.array([[0, 0], [1, 0], [1, 1], [0, 1]]) >>> result = analyze_movement(points_2d, mode=\"2D\") >>> print(result['metric']) # Area of square 1.0 >>> # Time series 3D analysis >>> frames = np.random.randn(100, 4, 3) >>> result = analyze_movement(frames, mode=\"3D\", baseline_frame=0) >>> print(result['states'][:10]) # First 10 frame states \"\"\" if data . ndim == 2 : dims = data . shape [ 1 ] is_timeseries = False elif data . ndim == 3 : dims = data . shape [ 2 ] is_timeseries = True if data . shape [ 1 ] != 4 : raise ValueError ( \"Invalid shape: second dimension must be 4\" ) else : raise ValueError ( \"Invalid data dimensions\" ) if mode is None : mode = \"2D\" if dims == 2 else \"3D\" if dims == 3 else None if mode is None : raise ValueError ( \"Invalid coordinate dimensions\" ) expected_dims = 2 if mode == \"2D\" else 3 if dims != expected_dims : raise ValueError ( f \"Mode { mode } requires { expected_dims } D data\" ) if not is_timeseries : if mode == \"2D\" : metric = _area_2d_fast ( data ) else : metric = _volume_3d_fast ( data ) return { \"metric\" : metric , \"dimension\" : mode } if mode == \"2D\" : metrics , indices , states = _process_timeseries_2d ( data , baseline_frame ) else : metrics , indices , states = _process_timeseries_3d ( data , baseline_frame ) return { \"metrics\" : metrics , \"indices\" : indices , \"states\" : states , \"dimension\" : mode }","title":"analyze_movement"},{"location":"API/low_level/equilibrium/","text":"Equilibrium Equilibrium Elliptical equilibrium evaluation between two feet and a barycenter. This class defines an elliptical region of interest (ROI) aligned with the line connecting the left and right foot. The ellipse is scaled by a margin in millimeters and can be weighted along the Y-axis to emphasize forward\u2013 backward sway more than lateral sway. A barycenter is evaluated against this ellipse to compute a normalized equilibrium value. Read more in the User Guide Parameters: margin_mm ( float , default: 100 ) \u2013 Extra margin in millimeters added around the rectangle spanned by the two feet (default: 100). y_weight ( float , default: 0.5 ) \u2013 Weighting factor applied to the ellipse height along the Y-axis. A value < 1 shrinks the ellipse in the forward/backward direction, emphasizing sway in that axis (default: 0.5). Examples: >>> import numpy as np >>> eq = Equilibrium ( margin_mm = 120 , y_weight = 0.6 ) Using 3D coordinates >>> left = np . array ([ 0 , 0 , 0 ]) >>> right = np . array ([ 400 , 0 , 0 ]) >>> barycenter = np . array ([ 200 , 50 , 0 ]) >>> result = eq ( left , right , barycenter ) >>> round ( result [ 'value' ], 2 ) 0.31 >>> round ( result [ 'angle' ], 1 ) 0.0 Using 2D coordinates (z is optional) >>> left_2d = np . array ([ 0 , 0 ]) >>> right_2d = np . array ([ 400 , 0 ]) >>> barycenter_2d = np . array ([ 200 , 50 ]) >>> result_2d = eq ( left_2d , right_2d , barycenter_2d ) >>> round ( result_2d [ 'value' ], 2 ) 0.31 >>> round ( result_2d [ 'angle' ], 1 ) 0.0 Source code in pyeyesweb/low_level/equilibrium.py 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class Equilibrium : \"\"\" Elliptical equilibrium evaluation between two feet and a barycenter. This class defines an elliptical region of interest (ROI) aligned with the line connecting the left and right foot. The ellipse is scaled by a margin in millimeters and can be weighted along the Y-axis to emphasize forward\u2013 backward sway more than lateral sway. A barycenter is evaluated against this ellipse to compute a normalized equilibrium value. Read more in the [User Guide](/PyEyesWeb/user_guide/theoretical_framework/low_level/postural_balance/) Parameters ---------- margin_mm : float, optional Extra margin in millimeters added around the rectangle spanned by the two feet (default: 100). y_weight : float, optional Weighting factor applied to the ellipse height along the Y-axis. A value < 1 shrinks the ellipse in the forward/backward direction, emphasizing sway in that axis (default: 0.5). Examples -------- >>> import numpy as np >>> eq = Equilibrium(margin_mm=120, y_weight=0.6) # Using 3D coordinates >>> left = np.array([0, 0, 0]) >>> right = np.array([400, 0, 0]) >>> barycenter = np.array([200, 50, 0]) >>> result = eq(left, right, barycenter) >>> round(result['value'], 2) 0.31 >>> round(result['angle'], 1) 0.0 # Using 2D coordinates (z is optional) >>> left_2d = np.array([0, 0]) >>> right_2d = np.array([400, 0]) >>> barycenter_2d = np.array([200, 50]) >>> result_2d = eq(left_2d, right_2d, barycenter_2d) >>> round(result_2d['value'], 2) 0.31 >>> round(result_2d['angle'], 1) 0.0 \"\"\" def __init__ ( self , margin_mm = 100 , y_weight = 0.5 ): self . margin = margin_mm self . y_weight = y_weight def __call__ ( self , left_foot : np . ndarray , right_foot : np . ndarray , barycenter : np . ndarray ) -> dict : \"\"\" Evaluate the equilibrium value and ellipse angle. Parameters ---------- left_foot : numpy.ndarray, shape (2,) or (3,) 2D coordinates (x, y) or 3D coordinates (x, y, z) of the left foot in millimeters. Only the x and y components are used. right_foot : numpy.ndarray, shape (2,) or (3,) 2D coordinates (x, y) or 3D coordinates (x, y, z) of the right foot in millimeters. Only the x and y components are used. barycenter : numpy.ndarray, shape (2,) or (3,) 2D coordinates (x, y) or 3D coordinates (x, y, z) of the barycenter in millimeters. Only the x and y components are used. Returns ------- dict Dictionary containing: - 'value': Equilibrium value in [0, 1]. 1 means the barycenter is perfectly at the ellipse center. 0 means the barycenter is outside the ellipse. - 'angle': Orientation of the ellipse in degrees, measured counter-clockwise from the X-axis (line connecting left and right foot). Notes ----- - The ellipse is aligned with the line connecting the two feet. - The ellipse width corresponds to the horizontal foot span + margin. - The ellipse height corresponds to the vertical span + margin, scaled by `y_weight`. - If 3D coordinates are provided, the z component is ignored. \"\"\" # Convert to numpy arrays and extract x,y components # Works for both 2D (x,y) and 3D (x,y,z) inputs ps = np . atleast_1d ( left_foot ) . flatten ()[: 2 ] pd = np . atleast_1d ( right_foot ) . flatten ()[: 2 ] bc = np . atleast_1d ( barycenter ) . flatten ()[: 2 ] min_xy = np . minimum ( ps , pd ) - self . margin max_xy = np . maximum ( ps , pd ) + self . margin center = ( min_xy + max_xy ) / 2 half_sizes = ( max_xy - min_xy ) / 2 a = half_sizes [ 0 ] b = half_sizes [ 1 ] * self . y_weight dx , dy = pd - ps angle = np . arctan2 ( dy , dx ) rel = bc - center rot_matrix = np . array ([ [ np . cos ( - angle ), - np . sin ( - angle )], [ np . sin ( - angle ), np . cos ( - angle )] ]) rel_rot = rot_matrix @ rel # Handle degenerate ellipse cases to avoid division by zero # Tolerance for considering a value as zero eps = 1e-10 if a < eps and b < eps : # Both axes are zero - ellipse is a point # Check if barycenter is at that point if np . linalg . norm ( rel ) < eps : value = 1.0 else : value = 0.0 elif a < eps : # Ellipse is a vertical line segment (a=0, b>0) # Only Y position matters if abs ( rel_rot [ 0 ]) > eps : # Barycenter is off the vertical line value = 0.0 else : # Check position along Y axis norm_y = ( rel_rot [ 1 ] / b ) ** 2 if norm_y <= 1.0 : value = 1.0 - np . sqrt ( norm_y ) else : value = 0.0 elif b < eps : # Ellipse is a horizontal line segment (a>0, b=0) # Only X position matters if abs ( rel_rot [ 1 ]) > eps : # Barycenter is off the horizontal line value = 0.0 else : # Check position along X axis norm_x = ( rel_rot [ 0 ] / a ) ** 2 if norm_x <= 1.0 : value = 1.0 - np . sqrt ( norm_x ) else : value = 0.0 else : # Normal ellipse case norm = ( rel_rot [ 0 ] / a ) ** 2 + ( rel_rot [ 1 ] / b ) ** 2 if norm <= 1.0 : value = 1.0 - np . sqrt ( norm ) else : value = 0.0 return { \"value\" : max ( 0.0 , value ), \"angle\" : np . degrees ( angle )} __call__ ( left_foot , right_foot , barycenter ) Evaluate the equilibrium value and ellipse angle. Parameters: left_foot ( ( ndarray , shape (2) or (3,)) ) \u2013 2D coordinates (x, y) or 3D coordinates (x, y, z) of the left foot in millimeters. Only the x and y components are used. right_foot ( ( ndarray , shape (2) or (3,)) ) \u2013 2D coordinates (x, y) or 3D coordinates (x, y, z) of the right foot in millimeters. Only the x and y components are used. barycenter ( ( ndarray , shape (2) or (3,)) ) \u2013 2D coordinates (x, y) or 3D coordinates (x, y, z) of the barycenter in millimeters. Only the x and y components are used. Returns: dict \u2013 Dictionary containing: - 'value': Equilibrium value in [0, 1]. 1 means the barycenter is perfectly at the ellipse center. 0 means the barycenter is outside the ellipse. - 'angle': Orientation of the ellipse in degrees, measured counter-clockwise from the X-axis (line connecting left and right foot). Notes The ellipse is aligned with the line connecting the two feet. The ellipse width corresponds to the horizontal foot span + margin. The ellipse height corresponds to the vertical span + margin, scaled by y_weight . If 3D coordinates are provided, the z component is ignored. Source code in pyeyesweb/low_level/equilibrium.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def __call__ ( self , left_foot : np . ndarray , right_foot : np . ndarray , barycenter : np . ndarray ) -> dict : \"\"\" Evaluate the equilibrium value and ellipse angle. Parameters ---------- left_foot : numpy.ndarray, shape (2,) or (3,) 2D coordinates (x, y) or 3D coordinates (x, y, z) of the left foot in millimeters. Only the x and y components are used. right_foot : numpy.ndarray, shape (2,) or (3,) 2D coordinates (x, y) or 3D coordinates (x, y, z) of the right foot in millimeters. Only the x and y components are used. barycenter : numpy.ndarray, shape (2,) or (3,) 2D coordinates (x, y) or 3D coordinates (x, y, z) of the barycenter in millimeters. Only the x and y components are used. Returns ------- dict Dictionary containing: - 'value': Equilibrium value in [0, 1]. 1 means the barycenter is perfectly at the ellipse center. 0 means the barycenter is outside the ellipse. - 'angle': Orientation of the ellipse in degrees, measured counter-clockwise from the X-axis (line connecting left and right foot). Notes ----- - The ellipse is aligned with the line connecting the two feet. - The ellipse width corresponds to the horizontal foot span + margin. - The ellipse height corresponds to the vertical span + margin, scaled by `y_weight`. - If 3D coordinates are provided, the z component is ignored. \"\"\" # Convert to numpy arrays and extract x,y components # Works for both 2D (x,y) and 3D (x,y,z) inputs ps = np . atleast_1d ( left_foot ) . flatten ()[: 2 ] pd = np . atleast_1d ( right_foot ) . flatten ()[: 2 ] bc = np . atleast_1d ( barycenter ) . flatten ()[: 2 ] min_xy = np . minimum ( ps , pd ) - self . margin max_xy = np . maximum ( ps , pd ) + self . margin center = ( min_xy + max_xy ) / 2 half_sizes = ( max_xy - min_xy ) / 2 a = half_sizes [ 0 ] b = half_sizes [ 1 ] * self . y_weight dx , dy = pd - ps angle = np . arctan2 ( dy , dx ) rel = bc - center rot_matrix = np . array ([ [ np . cos ( - angle ), - np . sin ( - angle )], [ np . sin ( - angle ), np . cos ( - angle )] ]) rel_rot = rot_matrix @ rel # Handle degenerate ellipse cases to avoid division by zero # Tolerance for considering a value as zero eps = 1e-10 if a < eps and b < eps : # Both axes are zero - ellipse is a point # Check if barycenter is at that point if np . linalg . norm ( rel ) < eps : value = 1.0 else : value = 0.0 elif a < eps : # Ellipse is a vertical line segment (a=0, b>0) # Only Y position matters if abs ( rel_rot [ 0 ]) > eps : # Barycenter is off the vertical line value = 0.0 else : # Check position along Y axis norm_y = ( rel_rot [ 1 ] / b ) ** 2 if norm_y <= 1.0 : value = 1.0 - np . sqrt ( norm_y ) else : value = 0.0 elif b < eps : # Ellipse is a horizontal line segment (a>0, b=0) # Only X position matters if abs ( rel_rot [ 1 ]) > eps : # Barycenter is off the horizontal line value = 0.0 else : # Check position along X axis norm_x = ( rel_rot [ 0 ] / a ) ** 2 if norm_x <= 1.0 : value = 1.0 - np . sqrt ( norm_x ) else : value = 0.0 else : # Normal ellipse case norm = ( rel_rot [ 0 ] / a ) ** 2 + ( rel_rot [ 1 ] / b ) ** 2 if norm <= 1.0 : value = 1.0 - np . sqrt ( norm ) else : value = 0.0 return { \"value\" : max ( 0.0 , value ), \"angle\" : np . degrees ( angle )}","title":"Equilibrium"},{"location":"API/low_level/equilibrium/#equilibrium","text":"","title":"Equilibrium"},{"location":"API/low_level/equilibrium/#pyeyesweb.low_level.equilibrium.Equilibrium","text":"Elliptical equilibrium evaluation between two feet and a barycenter. This class defines an elliptical region of interest (ROI) aligned with the line connecting the left and right foot. The ellipse is scaled by a margin in millimeters and can be weighted along the Y-axis to emphasize forward\u2013 backward sway more than lateral sway. A barycenter is evaluated against this ellipse to compute a normalized equilibrium value. Read more in the User Guide Parameters: margin_mm ( float , default: 100 ) \u2013 Extra margin in millimeters added around the rectangle spanned by the two feet (default: 100). y_weight ( float , default: 0.5 ) \u2013 Weighting factor applied to the ellipse height along the Y-axis. A value < 1 shrinks the ellipse in the forward/backward direction, emphasizing sway in that axis (default: 0.5). Examples: >>> import numpy as np >>> eq = Equilibrium ( margin_mm = 120 , y_weight = 0.6 )","title":"Equilibrium"},{"location":"API/low_level/equilibrium/#pyeyesweb.low_level.equilibrium.Equilibrium--using-3d-coordinates","text":">>> left = np . array ([ 0 , 0 , 0 ]) >>> right = np . array ([ 400 , 0 , 0 ]) >>> barycenter = np . array ([ 200 , 50 , 0 ]) >>> result = eq ( left , right , barycenter ) >>> round ( result [ 'value' ], 2 ) 0.31 >>> round ( result [ 'angle' ], 1 ) 0.0","title":"Using 3D coordinates"},{"location":"API/low_level/equilibrium/#pyeyesweb.low_level.equilibrium.Equilibrium--using-2d-coordinates-z-is-optional","text":">>> left_2d = np . array ([ 0 , 0 ]) >>> right_2d = np . array ([ 400 , 0 ]) >>> barycenter_2d = np . array ([ 200 , 50 ]) >>> result_2d = eq ( left_2d , right_2d , barycenter_2d ) >>> round ( result_2d [ 'value' ], 2 ) 0.31 >>> round ( result_2d [ 'angle' ], 1 ) 0.0 Source code in pyeyesweb/low_level/equilibrium.py 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class Equilibrium : \"\"\" Elliptical equilibrium evaluation between two feet and a barycenter. This class defines an elliptical region of interest (ROI) aligned with the line connecting the left and right foot. The ellipse is scaled by a margin in millimeters and can be weighted along the Y-axis to emphasize forward\u2013 backward sway more than lateral sway. A barycenter is evaluated against this ellipse to compute a normalized equilibrium value. Read more in the [User Guide](/PyEyesWeb/user_guide/theoretical_framework/low_level/postural_balance/) Parameters ---------- margin_mm : float, optional Extra margin in millimeters added around the rectangle spanned by the two feet (default: 100). y_weight : float, optional Weighting factor applied to the ellipse height along the Y-axis. A value < 1 shrinks the ellipse in the forward/backward direction, emphasizing sway in that axis (default: 0.5). Examples -------- >>> import numpy as np >>> eq = Equilibrium(margin_mm=120, y_weight=0.6) # Using 3D coordinates >>> left = np.array([0, 0, 0]) >>> right = np.array([400, 0, 0]) >>> barycenter = np.array([200, 50, 0]) >>> result = eq(left, right, barycenter) >>> round(result['value'], 2) 0.31 >>> round(result['angle'], 1) 0.0 # Using 2D coordinates (z is optional) >>> left_2d = np.array([0, 0]) >>> right_2d = np.array([400, 0]) >>> barycenter_2d = np.array([200, 50]) >>> result_2d = eq(left_2d, right_2d, barycenter_2d) >>> round(result_2d['value'], 2) 0.31 >>> round(result_2d['angle'], 1) 0.0 \"\"\" def __init__ ( self , margin_mm = 100 , y_weight = 0.5 ): self . margin = margin_mm self . y_weight = y_weight def __call__ ( self , left_foot : np . ndarray , right_foot : np . ndarray , barycenter : np . ndarray ) -> dict : \"\"\" Evaluate the equilibrium value and ellipse angle. Parameters ---------- left_foot : numpy.ndarray, shape (2,) or (3,) 2D coordinates (x, y) or 3D coordinates (x, y, z) of the left foot in millimeters. Only the x and y components are used. right_foot : numpy.ndarray, shape (2,) or (3,) 2D coordinates (x, y) or 3D coordinates (x, y, z) of the right foot in millimeters. Only the x and y components are used. barycenter : numpy.ndarray, shape (2,) or (3,) 2D coordinates (x, y) or 3D coordinates (x, y, z) of the barycenter in millimeters. Only the x and y components are used. Returns ------- dict Dictionary containing: - 'value': Equilibrium value in [0, 1]. 1 means the barycenter is perfectly at the ellipse center. 0 means the barycenter is outside the ellipse. - 'angle': Orientation of the ellipse in degrees, measured counter-clockwise from the X-axis (line connecting left and right foot). Notes ----- - The ellipse is aligned with the line connecting the two feet. - The ellipse width corresponds to the horizontal foot span + margin. - The ellipse height corresponds to the vertical span + margin, scaled by `y_weight`. - If 3D coordinates are provided, the z component is ignored. \"\"\" # Convert to numpy arrays and extract x,y components # Works for both 2D (x,y) and 3D (x,y,z) inputs ps = np . atleast_1d ( left_foot ) . flatten ()[: 2 ] pd = np . atleast_1d ( right_foot ) . flatten ()[: 2 ] bc = np . atleast_1d ( barycenter ) . flatten ()[: 2 ] min_xy = np . minimum ( ps , pd ) - self . margin max_xy = np . maximum ( ps , pd ) + self . margin center = ( min_xy + max_xy ) / 2 half_sizes = ( max_xy - min_xy ) / 2 a = half_sizes [ 0 ] b = half_sizes [ 1 ] * self . y_weight dx , dy = pd - ps angle = np . arctan2 ( dy , dx ) rel = bc - center rot_matrix = np . array ([ [ np . cos ( - angle ), - np . sin ( - angle )], [ np . sin ( - angle ), np . cos ( - angle )] ]) rel_rot = rot_matrix @ rel # Handle degenerate ellipse cases to avoid division by zero # Tolerance for considering a value as zero eps = 1e-10 if a < eps and b < eps : # Both axes are zero - ellipse is a point # Check if barycenter is at that point if np . linalg . norm ( rel ) < eps : value = 1.0 else : value = 0.0 elif a < eps : # Ellipse is a vertical line segment (a=0, b>0) # Only Y position matters if abs ( rel_rot [ 0 ]) > eps : # Barycenter is off the vertical line value = 0.0 else : # Check position along Y axis norm_y = ( rel_rot [ 1 ] / b ) ** 2 if norm_y <= 1.0 : value = 1.0 - np . sqrt ( norm_y ) else : value = 0.0 elif b < eps : # Ellipse is a horizontal line segment (a>0, b=0) # Only X position matters if abs ( rel_rot [ 1 ]) > eps : # Barycenter is off the horizontal line value = 0.0 else : # Check position along X axis norm_x = ( rel_rot [ 0 ] / a ) ** 2 if norm_x <= 1.0 : value = 1.0 - np . sqrt ( norm_x ) else : value = 0.0 else : # Normal ellipse case norm = ( rel_rot [ 0 ] / a ) ** 2 + ( rel_rot [ 1 ] / b ) ** 2 if norm <= 1.0 : value = 1.0 - np . sqrt ( norm ) else : value = 0.0 return { \"value\" : max ( 0.0 , value ), \"angle\" : np . degrees ( angle )}","title":"Using 2D coordinates (z is optional)"},{"location":"API/low_level/equilibrium/#pyeyesweb.low_level.equilibrium.Equilibrium.__call__","text":"Evaluate the equilibrium value and ellipse angle. Parameters: left_foot ( ( ndarray , shape (2) or (3,)) ) \u2013 2D coordinates (x, y) or 3D coordinates (x, y, z) of the left foot in millimeters. Only the x and y components are used. right_foot ( ( ndarray , shape (2) or (3,)) ) \u2013 2D coordinates (x, y) or 3D coordinates (x, y, z) of the right foot in millimeters. Only the x and y components are used. barycenter ( ( ndarray , shape (2) or (3,)) ) \u2013 2D coordinates (x, y) or 3D coordinates (x, y, z) of the barycenter in millimeters. Only the x and y components are used. Returns: dict \u2013 Dictionary containing: - 'value': Equilibrium value in [0, 1]. 1 means the barycenter is perfectly at the ellipse center. 0 means the barycenter is outside the ellipse. - 'angle': Orientation of the ellipse in degrees, measured counter-clockwise from the X-axis (line connecting left and right foot). Notes The ellipse is aligned with the line connecting the two feet. The ellipse width corresponds to the horizontal foot span + margin. The ellipse height corresponds to the vertical span + margin, scaled by y_weight . If 3D coordinates are provided, the z component is ignored. Source code in pyeyesweb/low_level/equilibrium.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def __call__ ( self , left_foot : np . ndarray , right_foot : np . ndarray , barycenter : np . ndarray ) -> dict : \"\"\" Evaluate the equilibrium value and ellipse angle. Parameters ---------- left_foot : numpy.ndarray, shape (2,) or (3,) 2D coordinates (x, y) or 3D coordinates (x, y, z) of the left foot in millimeters. Only the x and y components are used. right_foot : numpy.ndarray, shape (2,) or (3,) 2D coordinates (x, y) or 3D coordinates (x, y, z) of the right foot in millimeters. Only the x and y components are used. barycenter : numpy.ndarray, shape (2,) or (3,) 2D coordinates (x, y) or 3D coordinates (x, y, z) of the barycenter in millimeters. Only the x and y components are used. Returns ------- dict Dictionary containing: - 'value': Equilibrium value in [0, 1]. 1 means the barycenter is perfectly at the ellipse center. 0 means the barycenter is outside the ellipse. - 'angle': Orientation of the ellipse in degrees, measured counter-clockwise from the X-axis (line connecting left and right foot). Notes ----- - The ellipse is aligned with the line connecting the two feet. - The ellipse width corresponds to the horizontal foot span + margin. - The ellipse height corresponds to the vertical span + margin, scaled by `y_weight`. - If 3D coordinates are provided, the z component is ignored. \"\"\" # Convert to numpy arrays and extract x,y components # Works for both 2D (x,y) and 3D (x,y,z) inputs ps = np . atleast_1d ( left_foot ) . flatten ()[: 2 ] pd = np . atleast_1d ( right_foot ) . flatten ()[: 2 ] bc = np . atleast_1d ( barycenter ) . flatten ()[: 2 ] min_xy = np . minimum ( ps , pd ) - self . margin max_xy = np . maximum ( ps , pd ) + self . margin center = ( min_xy + max_xy ) / 2 half_sizes = ( max_xy - min_xy ) / 2 a = half_sizes [ 0 ] b = half_sizes [ 1 ] * self . y_weight dx , dy = pd - ps angle = np . arctan2 ( dy , dx ) rel = bc - center rot_matrix = np . array ([ [ np . cos ( - angle ), - np . sin ( - angle )], [ np . sin ( - angle ), np . cos ( - angle )] ]) rel_rot = rot_matrix @ rel # Handle degenerate ellipse cases to avoid division by zero # Tolerance for considering a value as zero eps = 1e-10 if a < eps and b < eps : # Both axes are zero - ellipse is a point # Check if barycenter is at that point if np . linalg . norm ( rel ) < eps : value = 1.0 else : value = 0.0 elif a < eps : # Ellipse is a vertical line segment (a=0, b>0) # Only Y position matters if abs ( rel_rot [ 0 ]) > eps : # Barycenter is off the vertical line value = 0.0 else : # Check position along Y axis norm_y = ( rel_rot [ 1 ] / b ) ** 2 if norm_y <= 1.0 : value = 1.0 - np . sqrt ( norm_y ) else : value = 0.0 elif b < eps : # Ellipse is a horizontal line segment (a>0, b=0) # Only X position matters if abs ( rel_rot [ 1 ]) > eps : # Barycenter is off the horizontal line value = 0.0 else : # Check position along X axis norm_x = ( rel_rot [ 0 ] / a ) ** 2 if norm_x <= 1.0 : value = 1.0 - np . sqrt ( norm_x ) else : value = 0.0 else : # Normal ellipse case norm = ( rel_rot [ 0 ] / a ) ** 2 + ( rel_rot [ 1 ] / b ) ** 2 if norm <= 1.0 : value = 1.0 - np . sqrt ( norm ) else : value = 0.0 return { \"value\" : max ( 0.0 , value ), \"angle\" : np . degrees ( angle )}","title":"__call__"},{"location":"API/low_level/kinetic_energy/","text":"Kinetic Energy KineticEnergy Source code in pyeyesweb/low_level/kinetic_energy.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 class KineticEnergy : def __init__ ( self , weights = 1.0 , labels = None ): \"\"\" weights: scalar mass or array of masses (one per joint) labels: list of joint labels (optional) \"\"\" # Store weight(s) self . setWeight ( weights ) # Store labels (optional, validated later) self . labels = labels def setWeight ( self , w ): \"\"\"Allow setting scalar weight or per-joint weights.\"\"\" if isinstance ( w , ( list , tuple , np . ndarray )): w = np . asarray ( w , dtype = float ) if np . any ( w < 0 ): raise ValueError ( \"Mass values cannot be negative.\" ) self . weights = w else : w = float ( w ) if w < 0 : raise ValueError ( \"Mass cannot be negative.\" ) self . weights = w def __call__ ( self , velocity_vectors ): \"\"\" velocity_vectors: - single vector (3,) - array of shape (N,3) for multiple joints \"\"\" # Convert input to array v = np . asarray ( velocity_vectors , dtype = float ) # Normalize to shape (N,3) if v . ndim == 1 : v = v . reshape ( 1 , - 1 ) N = v . shape [ 0 ] # Validate or expand weights if np . isscalar ( self . weights ): w = np . full ( N , self . weights ) else : w = np . asarray ( self . weights ) if w . size != N : raise ValueError ( f \"Weight array must match number of joints ( { N } ).\" ) # Validate labels if self . labels is not None : if len ( self . labels ) != N : raise ValueError ( f \"Labels must match number of joints ( { N } ).\" ) # Compute squared velocity components v_squared = v ** 2 # Component-wise kinetic energy: 1/2 * m_i * v^2 Ek_components = 0.5 * w [:, None ] * v_squared # Total KE per joint Ek_joint = Ek_components . sum ( axis = 1 ) # Total KE across all joints Ek_total = Ek_joint . sum () # Total per-axis component energy Ek_components_total = Ek_components . sum ( axis = 0 ) # Build dictionary optionally using labels if self . labels is None : joint_energy_dict = { i : { \"total\" : Ek_joint [ i ], \"components\" : Ek_components [ i ] } for i in range ( N ) } else : joint_energy_dict = { self . labels [ i ]: { \"total\" : Ek_joint [ i ], \"components\" : Ek_components [ i ] } for i in range ( N ) } return { \"total_energy\" : Ek_total , \"component_energy\" : Ek_components_total , # [Ex, Ey, Ez] \"joints\" : joint_energy_dict # dict indexed by labels or id } __call__ ( velocity_vectors ) velocity_vectors: - single vector (3,) - array of shape (N,3) for multiple joints Source code in pyeyesweb/low_level/kinetic_energy.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __call__ ( self , velocity_vectors ): \"\"\" velocity_vectors: - single vector (3,) - array of shape (N,3) for multiple joints \"\"\" # Convert input to array v = np . asarray ( velocity_vectors , dtype = float ) # Normalize to shape (N,3) if v . ndim == 1 : v = v . reshape ( 1 , - 1 ) N = v . shape [ 0 ] # Validate or expand weights if np . isscalar ( self . weights ): w = np . full ( N , self . weights ) else : w = np . asarray ( self . weights ) if w . size != N : raise ValueError ( f \"Weight array must match number of joints ( { N } ).\" ) # Validate labels if self . labels is not None : if len ( self . labels ) != N : raise ValueError ( f \"Labels must match number of joints ( { N } ).\" ) # Compute squared velocity components v_squared = v ** 2 # Component-wise kinetic energy: 1/2 * m_i * v^2 Ek_components = 0.5 * w [:, None ] * v_squared # Total KE per joint Ek_joint = Ek_components . sum ( axis = 1 ) # Total KE across all joints Ek_total = Ek_joint . sum () # Total per-axis component energy Ek_components_total = Ek_components . sum ( axis = 0 ) # Build dictionary optionally using labels if self . labels is None : joint_energy_dict = { i : { \"total\" : Ek_joint [ i ], \"components\" : Ek_components [ i ] } for i in range ( N ) } else : joint_energy_dict = { self . labels [ i ]: { \"total\" : Ek_joint [ i ], \"components\" : Ek_components [ i ] } for i in range ( N ) } return { \"total_energy\" : Ek_total , \"component_energy\" : Ek_components_total , # [Ex, Ey, Ez] \"joints\" : joint_energy_dict # dict indexed by labels or id } __init__ ( weights = 1.0 , labels = None ) weights: scalar mass or array of masses (one per joint) labels: list of joint labels (optional) Source code in pyeyesweb/low_level/kinetic_energy.py 6 7 8 9 10 11 12 13 14 15 16 def __init__ ( self , weights = 1.0 , labels = None ): \"\"\" weights: scalar mass or array of masses (one per joint) labels: list of joint labels (optional) \"\"\" # Store weight(s) self . setWeight ( weights ) # Store labels (optional, validated later) self . labels = labels setWeight ( w ) Allow setting scalar weight or per-joint weights. Source code in pyeyesweb/low_level/kinetic_energy.py 18 19 20 21 22 23 24 25 26 27 28 29 def setWeight ( self , w ): \"\"\"Allow setting scalar weight or per-joint weights.\"\"\" if isinstance ( w , ( list , tuple , np . ndarray )): w = np . asarray ( w , dtype = float ) if np . any ( w < 0 ): raise ValueError ( \"Mass values cannot be negative.\" ) self . weights = w else : w = float ( w ) if w < 0 : raise ValueError ( \"Mass cannot be negative.\" ) self . weights = w","title":"Kinetic Energy"},{"location":"API/low_level/kinetic_energy/#kinetic-energy","text":"","title":"Kinetic Energy"},{"location":"API/low_level/kinetic_energy/#pyeyesweb.low_level.kinetic_energy.KineticEnergy","text":"Source code in pyeyesweb/low_level/kinetic_energy.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 class KineticEnergy : def __init__ ( self , weights = 1.0 , labels = None ): \"\"\" weights: scalar mass or array of masses (one per joint) labels: list of joint labels (optional) \"\"\" # Store weight(s) self . setWeight ( weights ) # Store labels (optional, validated later) self . labels = labels def setWeight ( self , w ): \"\"\"Allow setting scalar weight or per-joint weights.\"\"\" if isinstance ( w , ( list , tuple , np . ndarray )): w = np . asarray ( w , dtype = float ) if np . any ( w < 0 ): raise ValueError ( \"Mass values cannot be negative.\" ) self . weights = w else : w = float ( w ) if w < 0 : raise ValueError ( \"Mass cannot be negative.\" ) self . weights = w def __call__ ( self , velocity_vectors ): \"\"\" velocity_vectors: - single vector (3,) - array of shape (N,3) for multiple joints \"\"\" # Convert input to array v = np . asarray ( velocity_vectors , dtype = float ) # Normalize to shape (N,3) if v . ndim == 1 : v = v . reshape ( 1 , - 1 ) N = v . shape [ 0 ] # Validate or expand weights if np . isscalar ( self . weights ): w = np . full ( N , self . weights ) else : w = np . asarray ( self . weights ) if w . size != N : raise ValueError ( f \"Weight array must match number of joints ( { N } ).\" ) # Validate labels if self . labels is not None : if len ( self . labels ) != N : raise ValueError ( f \"Labels must match number of joints ( { N } ).\" ) # Compute squared velocity components v_squared = v ** 2 # Component-wise kinetic energy: 1/2 * m_i * v^2 Ek_components = 0.5 * w [:, None ] * v_squared # Total KE per joint Ek_joint = Ek_components . sum ( axis = 1 ) # Total KE across all joints Ek_total = Ek_joint . sum () # Total per-axis component energy Ek_components_total = Ek_components . sum ( axis = 0 ) # Build dictionary optionally using labels if self . labels is None : joint_energy_dict = { i : { \"total\" : Ek_joint [ i ], \"components\" : Ek_components [ i ] } for i in range ( N ) } else : joint_energy_dict = { self . labels [ i ]: { \"total\" : Ek_joint [ i ], \"components\" : Ek_components [ i ] } for i in range ( N ) } return { \"total_energy\" : Ek_total , \"component_energy\" : Ek_components_total , # [Ex, Ey, Ez] \"joints\" : joint_energy_dict # dict indexed by labels or id }","title":"KineticEnergy"},{"location":"API/low_level/kinetic_energy/#pyeyesweb.low_level.kinetic_energy.KineticEnergy.__call__","text":"velocity_vectors: - single vector (3,) - array of shape (N,3) for multiple joints Source code in pyeyesweb/low_level/kinetic_energy.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __call__ ( self , velocity_vectors ): \"\"\" velocity_vectors: - single vector (3,) - array of shape (N,3) for multiple joints \"\"\" # Convert input to array v = np . asarray ( velocity_vectors , dtype = float ) # Normalize to shape (N,3) if v . ndim == 1 : v = v . reshape ( 1 , - 1 ) N = v . shape [ 0 ] # Validate or expand weights if np . isscalar ( self . weights ): w = np . full ( N , self . weights ) else : w = np . asarray ( self . weights ) if w . size != N : raise ValueError ( f \"Weight array must match number of joints ( { N } ).\" ) # Validate labels if self . labels is not None : if len ( self . labels ) != N : raise ValueError ( f \"Labels must match number of joints ( { N } ).\" ) # Compute squared velocity components v_squared = v ** 2 # Component-wise kinetic energy: 1/2 * m_i * v^2 Ek_components = 0.5 * w [:, None ] * v_squared # Total KE per joint Ek_joint = Ek_components . sum ( axis = 1 ) # Total KE across all joints Ek_total = Ek_joint . sum () # Total per-axis component energy Ek_components_total = Ek_components . sum ( axis = 0 ) # Build dictionary optionally using labels if self . labels is None : joint_energy_dict = { i : { \"total\" : Ek_joint [ i ], \"components\" : Ek_components [ i ] } for i in range ( N ) } else : joint_energy_dict = { self . labels [ i ]: { \"total\" : Ek_joint [ i ], \"components\" : Ek_components [ i ] } for i in range ( N ) } return { \"total_energy\" : Ek_total , \"component_energy\" : Ek_components_total , # [Ex, Ey, Ez] \"joints\" : joint_energy_dict # dict indexed by labels or id }","title":"__call__"},{"location":"API/low_level/kinetic_energy/#pyeyesweb.low_level.kinetic_energy.KineticEnergy.__init__","text":"weights: scalar mass or array of masses (one per joint) labels: list of joint labels (optional) Source code in pyeyesweb/low_level/kinetic_energy.py 6 7 8 9 10 11 12 13 14 15 16 def __init__ ( self , weights = 1.0 , labels = None ): \"\"\" weights: scalar mass or array of masses (one per joint) labels: list of joint labels (optional) \"\"\" # Store weight(s) self . setWeight ( weights ) # Store labels (optional, validated later) self . labels = labels","title":"__init__"},{"location":"API/low_level/kinetic_energy/#pyeyesweb.low_level.kinetic_energy.KineticEnergy.setWeight","text":"Allow setting scalar weight or per-joint weights. Source code in pyeyesweb/low_level/kinetic_energy.py 18 19 20 21 22 23 24 25 26 27 28 29 def setWeight ( self , w ): \"\"\"Allow setting scalar weight or per-joint weights.\"\"\" if isinstance ( w , ( list , tuple , np . ndarray )): w = np . asarray ( w , dtype = float ) if np . any ( w < 0 ): raise ValueError ( \"Mass values cannot be negative.\" ) self . weights = w else : w = float ( w ) if w < 0 : raise ValueError ( \"Mass cannot be negative.\" ) self . weights = w","title":"setWeight"},{"location":"API/low_level/smoothness/","text":"Smoothness Movement smoothness analysis module. This module provides tools for quantifying the smoothness of movement signals using multiple metrics including SPARC (Spectral Arc Length) and Jerk RMS. Designed for real-time analysis of motion capture or sensor data. The module can accept either position or velocity signals as input. When position is provided, velocity is automatically computed for SPARC analysis. Smoothness metrics are important indicators of movement quality in: 1. Motor control assessment 2. Rehabilitation monitoring 3. Skill learning evaluation 4. Neurological disorder diagnosis Smoothness Compute movement smoothness metrics from position or velocity signal data. This class analyzes movement smoothness using SPARC (Spectral Arc Length) and Jerk RMS metrics. It can accept either position or velocity signals as input. When position is provided, velocity is automatically computed. It can optionally apply Savitzky-Golay filtering to reduce noise before analysis. SPARC implementation is based on Balasubramanian et al. (2015) \"On the analysis of movement smoothness\" from Journal of NeuroEngineering and Rehabilitation. SPARC values are typically negative, with values closer to 0 indicating less smooth movements. Healthy reaching movements typically yield SPARC values around -1.4 to -1.6, while pathological movements may range from -3 to -10 or lower. Read more in the User Guide Parameters: rate_hz ( float , default: 50.0 ) \u2013 Sampling rate of the signal in Hz (default: 50.0). use_filter ( bool , default: True ) \u2013 Whether to apply Savitzky-Golay filtering before analysis (default: True). signal_type ( ( velocity , position ) , default: 'velocity' ) \u2013 Type of input signal (default: 'velocity'). - 'velocity': Direct velocity input - 'position': Position input (x, y coordinates or 1D position) Attributes: rate_hz ( float ) \u2013 Signal sampling rate. use_filter ( bool ) \u2013 Filter application flag. signal_type ( str ) \u2013 Type of input signal ('velocity' or 'position'). Examples: >>> from pyeyesweb.low_level.smoothness import Smoothness >>> from pyeyesweb.data_models.sliding_window import SlidingWindow >>> import numpy as np >>> >>> # Example 1: Using velocity data (traditional approach) >>> t = np . linspace ( 0 , 2 , 200 ) >>> velocity_data = np . sin ( 2 * np . pi * t ) + 0.1 * np . random . randn ( 200 ) >>> >>> smooth = Smoothness ( rate_hz = 100.0 , use_filter = True , signal_type = 'velocity' ) >>> window = SlidingWindow ( max_length = 200 , n_columns = 1 ) >>> >>> # Add velocity data to the window >>> for velocity in velocity_data : ... window . append ([ velocity ]) >>> >>> result = smooth ( window ) >>> print ( f \"SPARC: { result [ 'sparc' ] : .3f } , Jerk RMS: { result [ 'jerk_rms' ] : .3f } \" ) >>> >>> # Example 2: Using position data >>> # 2D position data (x, y coordinates) >>> t = np . linspace ( 0 , 2 , 200 ) >>> x_positions = 10 * np . sin ( 2 * np . pi * t ) >>> y_positions = 5 * np . cos ( 2 * np . pi * t ) >>> >>> smooth_pos = Smoothness ( rate_hz = 100.0 , use_filter = True , signal_type = 'position' ) >>> window_pos = SlidingWindow ( max_length = 200 , n_columns = 2 ) >>> >>> # Add position data to the window >>> for x , y in zip ( x_positions , y_positions ): ... window_pos . append ([ x , y ]) >>> >>> result = smooth_pos ( window_pos ) >>> print ( f \"SPARC: { result [ 'sparc' ] : .3f } , Jerk RMS: { result [ 'jerk_rms' ] : .3f } \" ) Notes SPARC: Values closer to 0 indicate smoother movement (less negative = smoother) Jerk RMS: Lower values indicate smoother movement Requires at least 5 samples for meaningful analysis References Balasubramanian, S., Melendez-Calderon, A., Roby-Brami, A., & Burdet, E. (2015). On the analysis of movement smoothness. Journal of NeuroEngineering and Rehabilitation, 12(1), 1-11. Source code in pyeyesweb/low_level/smoothness.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 class Smoothness : \"\"\"Compute movement smoothness metrics from position or velocity signal data. This class analyzes movement smoothness using SPARC (Spectral Arc Length) and Jerk RMS metrics. It can accept either position or velocity signals as input. When position is provided, velocity is automatically computed. It can optionally apply Savitzky-Golay filtering to reduce noise before analysis. SPARC implementation is based on Balasubramanian et al. (2015) \"On the analysis of movement smoothness\" from Journal of NeuroEngineering and Rehabilitation. SPARC values are typically negative, with values closer to 0 indicating less smooth movements. Healthy reaching movements typically yield SPARC values around -1.4 to -1.6, while pathological movements may range from -3 to -10 or lower. Read more in the [User Guide](/PyEyesWeb/user_guide/theoretical_framework/low_level/smoothness/) Parameters ---------- rate_hz : float, optional Sampling rate of the signal in Hz (default: 50.0). use_filter : bool, optional Whether to apply Savitzky-Golay filtering before analysis (default: True). signal_type : {'velocity', 'position'}, optional Type of input signal (default: 'velocity'). - 'velocity': Direct velocity input - 'position': Position input (x, y coordinates or 1D position) Attributes ---------- rate_hz : float Signal sampling rate. use_filter : bool Filter application flag. signal_type : str Type of input signal ('velocity' or 'position'). Examples -------- >>> from pyeyesweb.low_level.smoothness import Smoothness >>> from pyeyesweb.data_models.sliding_window import SlidingWindow >>> import numpy as np >>> >>> # Example 1: Using velocity data (traditional approach) >>> t = np.linspace(0, 2, 200) >>> velocity_data = np.sin(2 * np.pi * t) + 0.1 * np.random.randn(200) >>> >>> smooth = Smoothness(rate_hz=100.0, use_filter=True, signal_type='velocity') >>> window = SlidingWindow(max_length=200, n_columns=1) >>> >>> # Add velocity data to the window >>> for velocity in velocity_data: ... window.append([velocity]) >>> >>> result = smooth(window) >>> print(f\"SPARC: {result['sparc']:.3f}, Jerk RMS: {result['jerk_rms']:.3f}\") >>> >>> # Example 2: Using position data >>> # 2D position data (x, y coordinates) >>> t = np.linspace(0, 2, 200) >>> x_positions = 10 * np.sin(2 * np.pi * t) >>> y_positions = 5 * np.cos(2 * np.pi * t) >>> >>> smooth_pos = Smoothness(rate_hz=100.0, use_filter=True, signal_type='position') >>> window_pos = SlidingWindow(max_length=200, n_columns=2) >>> >>> # Add position data to the window >>> for x, y in zip(x_positions, y_positions): ... window_pos.append([x, y]) >>> >>> result = smooth_pos(window_pos) >>> print(f\"SPARC: {result['sparc']:.3f}, Jerk RMS: {result['jerk_rms']:.3f}\") Notes ----- 1. SPARC: Values closer to 0 indicate smoother movement (less negative = smoother) 2. Jerk RMS: Lower values indicate smoother movement 3. Requires at least 5 samples for meaningful analysis References ---------- Balasubramanian, S., Melendez-Calderon, A., Roby-Brami, A., & Burdet, E. (2015). On the analysis of movement smoothness. Journal of NeuroEngineering and Rehabilitation, 12(1), 1-11. \"\"\" def __init__ ( self , rate_hz = 50.0 , use_filter = True , signal_type = 'velocity' ): self . rate_hz = validate_numeric ( rate_hz , 'rate_hz' , min_val = 0.01 , max_val = 100000 ) self . use_filter = validate_boolean ( use_filter , 'use_filter' ) # Validate signal_type if signal_type not in [ 'velocity' , 'position' ]: raise ValueError ( f \"signal_type must be 'velocity' or 'position', got ' { signal_type } '\" ) self . signal_type = signal_type def _filter_signal ( self , signal ): \"\"\"Apply Savitzky-Golay filter if enabled and enough data. Parameters ---------- signal : array-like Input signal to filter. Returns ------- ndarray Filtered signal or original if filtering disabled/not possible. \"\"\" if not self . use_filter : return np . array ( signal ) return apply_savgol_filter ( signal , self . rate_hz ) def __call__ ( self , sliding_window : SlidingWindow ): \"\"\"Compute smoothness metrics from windowed signal data. Parameters ---------- sliding_window : SlidingWindow Buffer containing signal data to analyze. - If signal_type='velocity': expects velocity values - If signal_type='position': expects position values (1D or 2D) Returns ------- dict Dictionary containing: - 'sparc': Spectral Arc Length (closer to 0 = smoother). Returns NaN if insufficient data. - 'jerk_rms': RMS of jerk. Returns NaN if insufficient data. \"\"\" if len ( sliding_window ) < 5 : return { \"sparc\" : np . nan , \"jerk_rms\" : np . nan } signal , _ = sliding_window . to_array () # Extract or compute velocity based on signal type if self . signal_type == 'position' : # For position: compute velocity for SPARC velocity = extract_velocity_from_position ( signal , self . rate_hz ) # For jerk: use first dimension of position signal_for_jerk = signal [:, 0 ] if signal . ndim > 1 else signal . squeeze () else : # self.signal_type == 'velocity' # For velocity: use directly if signal . ndim > 1 and signal . shape [ 1 ] > 1 : velocity = signal [:, 0 ] else : velocity = signal . squeeze () signal_for_jerk = velocity # Apply filtering to velocity for SPARC filtered_velocity = self . _filter_signal ( velocity ) normalized = normalize_signal ( filtered_velocity ) # Compute SPARC (always uses velocity) sparc = compute_sparc ( normalized , self . rate_hz ) # Compute jerk with appropriate signal type filtered_for_jerk = self . _filter_signal ( signal_for_jerk ) jerk = compute_jerk_rms ( filtered_for_jerk , self . rate_hz , signal_type = self . signal_type ) return { \"sparc\" : sparc , \"jerk_rms\" : jerk } __call__ ( sliding_window ) Compute smoothness metrics from windowed signal data. Parameters: sliding_window ( SlidingWindow ) \u2013 Buffer containing signal data to analyze. - If signal_type='velocity': expects velocity values - If signal_type='position': expects position values (1D or 2D) Returns: dict \u2013 Dictionary containing: - 'sparc': Spectral Arc Length (closer to 0 = smoother). Returns NaN if insufficient data. - 'jerk_rms': RMS of jerk. Returns NaN if insufficient data. Source code in pyeyesweb/low_level/smoothness.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def __call__ ( self , sliding_window : SlidingWindow ): \"\"\"Compute smoothness metrics from windowed signal data. Parameters ---------- sliding_window : SlidingWindow Buffer containing signal data to analyze. - If signal_type='velocity': expects velocity values - If signal_type='position': expects position values (1D or 2D) Returns ------- dict Dictionary containing: - 'sparc': Spectral Arc Length (closer to 0 = smoother). Returns NaN if insufficient data. - 'jerk_rms': RMS of jerk. Returns NaN if insufficient data. \"\"\" if len ( sliding_window ) < 5 : return { \"sparc\" : np . nan , \"jerk_rms\" : np . nan } signal , _ = sliding_window . to_array () # Extract or compute velocity based on signal type if self . signal_type == 'position' : # For position: compute velocity for SPARC velocity = extract_velocity_from_position ( signal , self . rate_hz ) # For jerk: use first dimension of position signal_for_jerk = signal [:, 0 ] if signal . ndim > 1 else signal . squeeze () else : # self.signal_type == 'velocity' # For velocity: use directly if signal . ndim > 1 and signal . shape [ 1 ] > 1 : velocity = signal [:, 0 ] else : velocity = signal . squeeze () signal_for_jerk = velocity # Apply filtering to velocity for SPARC filtered_velocity = self . _filter_signal ( velocity ) normalized = normalize_signal ( filtered_velocity ) # Compute SPARC (always uses velocity) sparc = compute_sparc ( normalized , self . rate_hz ) # Compute jerk with appropriate signal type filtered_for_jerk = self . _filter_signal ( signal_for_jerk ) jerk = compute_jerk_rms ( filtered_for_jerk , self . rate_hz , signal_type = self . signal_type ) return { \"sparc\" : sparc , \"jerk_rms\" : jerk }","title":"Smoothness"},{"location":"API/low_level/smoothness/#smoothness","text":"Movement smoothness analysis module. This module provides tools for quantifying the smoothness of movement signals using multiple metrics including SPARC (Spectral Arc Length) and Jerk RMS. Designed for real-time analysis of motion capture or sensor data. The module can accept either position or velocity signals as input. When position is provided, velocity is automatically computed for SPARC analysis. Smoothness metrics are important indicators of movement quality in: 1. Motor control assessment 2. Rehabilitation monitoring 3. Skill learning evaluation 4. Neurological disorder diagnosis","title":"Smoothness"},{"location":"API/low_level/smoothness/#pyeyesweb.low_level.smoothness.Smoothness","text":"Compute movement smoothness metrics from position or velocity signal data. This class analyzes movement smoothness using SPARC (Spectral Arc Length) and Jerk RMS metrics. It can accept either position or velocity signals as input. When position is provided, velocity is automatically computed. It can optionally apply Savitzky-Golay filtering to reduce noise before analysis. SPARC implementation is based on Balasubramanian et al. (2015) \"On the analysis of movement smoothness\" from Journal of NeuroEngineering and Rehabilitation. SPARC values are typically negative, with values closer to 0 indicating less smooth movements. Healthy reaching movements typically yield SPARC values around -1.4 to -1.6, while pathological movements may range from -3 to -10 or lower. Read more in the User Guide Parameters: rate_hz ( float , default: 50.0 ) \u2013 Sampling rate of the signal in Hz (default: 50.0). use_filter ( bool , default: True ) \u2013 Whether to apply Savitzky-Golay filtering before analysis (default: True). signal_type ( ( velocity , position ) , default: 'velocity' ) \u2013 Type of input signal (default: 'velocity'). - 'velocity': Direct velocity input - 'position': Position input (x, y coordinates or 1D position) Attributes: rate_hz ( float ) \u2013 Signal sampling rate. use_filter ( bool ) \u2013 Filter application flag. signal_type ( str ) \u2013 Type of input signal ('velocity' or 'position'). Examples: >>> from pyeyesweb.low_level.smoothness import Smoothness >>> from pyeyesweb.data_models.sliding_window import SlidingWindow >>> import numpy as np >>> >>> # Example 1: Using velocity data (traditional approach) >>> t = np . linspace ( 0 , 2 , 200 ) >>> velocity_data = np . sin ( 2 * np . pi * t ) + 0.1 * np . random . randn ( 200 ) >>> >>> smooth = Smoothness ( rate_hz = 100.0 , use_filter = True , signal_type = 'velocity' ) >>> window = SlidingWindow ( max_length = 200 , n_columns = 1 ) >>> >>> # Add velocity data to the window >>> for velocity in velocity_data : ... window . append ([ velocity ]) >>> >>> result = smooth ( window ) >>> print ( f \"SPARC: { result [ 'sparc' ] : .3f } , Jerk RMS: { result [ 'jerk_rms' ] : .3f } \" ) >>> >>> # Example 2: Using position data >>> # 2D position data (x, y coordinates) >>> t = np . linspace ( 0 , 2 , 200 ) >>> x_positions = 10 * np . sin ( 2 * np . pi * t ) >>> y_positions = 5 * np . cos ( 2 * np . pi * t ) >>> >>> smooth_pos = Smoothness ( rate_hz = 100.0 , use_filter = True , signal_type = 'position' ) >>> window_pos = SlidingWindow ( max_length = 200 , n_columns = 2 ) >>> >>> # Add position data to the window >>> for x , y in zip ( x_positions , y_positions ): ... window_pos . append ([ x , y ]) >>> >>> result = smooth_pos ( window_pos ) >>> print ( f \"SPARC: { result [ 'sparc' ] : .3f } , Jerk RMS: { result [ 'jerk_rms' ] : .3f } \" ) Notes SPARC: Values closer to 0 indicate smoother movement (less negative = smoother) Jerk RMS: Lower values indicate smoother movement Requires at least 5 samples for meaningful analysis References Balasubramanian, S., Melendez-Calderon, A., Roby-Brami, A., & Burdet, E. (2015). On the analysis of movement smoothness. Journal of NeuroEngineering and Rehabilitation, 12(1), 1-11. Source code in pyeyesweb/low_level/smoothness.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 class Smoothness : \"\"\"Compute movement smoothness metrics from position or velocity signal data. This class analyzes movement smoothness using SPARC (Spectral Arc Length) and Jerk RMS metrics. It can accept either position or velocity signals as input. When position is provided, velocity is automatically computed. It can optionally apply Savitzky-Golay filtering to reduce noise before analysis. SPARC implementation is based on Balasubramanian et al. (2015) \"On the analysis of movement smoothness\" from Journal of NeuroEngineering and Rehabilitation. SPARC values are typically negative, with values closer to 0 indicating less smooth movements. Healthy reaching movements typically yield SPARC values around -1.4 to -1.6, while pathological movements may range from -3 to -10 or lower. Read more in the [User Guide](/PyEyesWeb/user_guide/theoretical_framework/low_level/smoothness/) Parameters ---------- rate_hz : float, optional Sampling rate of the signal in Hz (default: 50.0). use_filter : bool, optional Whether to apply Savitzky-Golay filtering before analysis (default: True). signal_type : {'velocity', 'position'}, optional Type of input signal (default: 'velocity'). - 'velocity': Direct velocity input - 'position': Position input (x, y coordinates or 1D position) Attributes ---------- rate_hz : float Signal sampling rate. use_filter : bool Filter application flag. signal_type : str Type of input signal ('velocity' or 'position'). Examples -------- >>> from pyeyesweb.low_level.smoothness import Smoothness >>> from pyeyesweb.data_models.sliding_window import SlidingWindow >>> import numpy as np >>> >>> # Example 1: Using velocity data (traditional approach) >>> t = np.linspace(0, 2, 200) >>> velocity_data = np.sin(2 * np.pi * t) + 0.1 * np.random.randn(200) >>> >>> smooth = Smoothness(rate_hz=100.0, use_filter=True, signal_type='velocity') >>> window = SlidingWindow(max_length=200, n_columns=1) >>> >>> # Add velocity data to the window >>> for velocity in velocity_data: ... window.append([velocity]) >>> >>> result = smooth(window) >>> print(f\"SPARC: {result['sparc']:.3f}, Jerk RMS: {result['jerk_rms']:.3f}\") >>> >>> # Example 2: Using position data >>> # 2D position data (x, y coordinates) >>> t = np.linspace(0, 2, 200) >>> x_positions = 10 * np.sin(2 * np.pi * t) >>> y_positions = 5 * np.cos(2 * np.pi * t) >>> >>> smooth_pos = Smoothness(rate_hz=100.0, use_filter=True, signal_type='position') >>> window_pos = SlidingWindow(max_length=200, n_columns=2) >>> >>> # Add position data to the window >>> for x, y in zip(x_positions, y_positions): ... window_pos.append([x, y]) >>> >>> result = smooth_pos(window_pos) >>> print(f\"SPARC: {result['sparc']:.3f}, Jerk RMS: {result['jerk_rms']:.3f}\") Notes ----- 1. SPARC: Values closer to 0 indicate smoother movement (less negative = smoother) 2. Jerk RMS: Lower values indicate smoother movement 3. Requires at least 5 samples for meaningful analysis References ---------- Balasubramanian, S., Melendez-Calderon, A., Roby-Brami, A., & Burdet, E. (2015). On the analysis of movement smoothness. Journal of NeuroEngineering and Rehabilitation, 12(1), 1-11. \"\"\" def __init__ ( self , rate_hz = 50.0 , use_filter = True , signal_type = 'velocity' ): self . rate_hz = validate_numeric ( rate_hz , 'rate_hz' , min_val = 0.01 , max_val = 100000 ) self . use_filter = validate_boolean ( use_filter , 'use_filter' ) # Validate signal_type if signal_type not in [ 'velocity' , 'position' ]: raise ValueError ( f \"signal_type must be 'velocity' or 'position', got ' { signal_type } '\" ) self . signal_type = signal_type def _filter_signal ( self , signal ): \"\"\"Apply Savitzky-Golay filter if enabled and enough data. Parameters ---------- signal : array-like Input signal to filter. Returns ------- ndarray Filtered signal or original if filtering disabled/not possible. \"\"\" if not self . use_filter : return np . array ( signal ) return apply_savgol_filter ( signal , self . rate_hz ) def __call__ ( self , sliding_window : SlidingWindow ): \"\"\"Compute smoothness metrics from windowed signal data. Parameters ---------- sliding_window : SlidingWindow Buffer containing signal data to analyze. - If signal_type='velocity': expects velocity values - If signal_type='position': expects position values (1D or 2D) Returns ------- dict Dictionary containing: - 'sparc': Spectral Arc Length (closer to 0 = smoother). Returns NaN if insufficient data. - 'jerk_rms': RMS of jerk. Returns NaN if insufficient data. \"\"\" if len ( sliding_window ) < 5 : return { \"sparc\" : np . nan , \"jerk_rms\" : np . nan } signal , _ = sliding_window . to_array () # Extract or compute velocity based on signal type if self . signal_type == 'position' : # For position: compute velocity for SPARC velocity = extract_velocity_from_position ( signal , self . rate_hz ) # For jerk: use first dimension of position signal_for_jerk = signal [:, 0 ] if signal . ndim > 1 else signal . squeeze () else : # self.signal_type == 'velocity' # For velocity: use directly if signal . ndim > 1 and signal . shape [ 1 ] > 1 : velocity = signal [:, 0 ] else : velocity = signal . squeeze () signal_for_jerk = velocity # Apply filtering to velocity for SPARC filtered_velocity = self . _filter_signal ( velocity ) normalized = normalize_signal ( filtered_velocity ) # Compute SPARC (always uses velocity) sparc = compute_sparc ( normalized , self . rate_hz ) # Compute jerk with appropriate signal type filtered_for_jerk = self . _filter_signal ( signal_for_jerk ) jerk = compute_jerk_rms ( filtered_for_jerk , self . rate_hz , signal_type = self . signal_type ) return { \"sparc\" : sparc , \"jerk_rms\" : jerk }","title":"Smoothness"},{"location":"API/low_level/smoothness/#pyeyesweb.low_level.smoothness.Smoothness.__call__","text":"Compute smoothness metrics from windowed signal data. Parameters: sliding_window ( SlidingWindow ) \u2013 Buffer containing signal data to analyze. - If signal_type='velocity': expects velocity values - If signal_type='position': expects position values (1D or 2D) Returns: dict \u2013 Dictionary containing: - 'sparc': Spectral Arc Length (closer to 0 = smoother). Returns NaN if insufficient data. - 'jerk_rms': RMS of jerk. Returns NaN if insufficient data. Source code in pyeyesweb/low_level/smoothness.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def __call__ ( self , sliding_window : SlidingWindow ): \"\"\"Compute smoothness metrics from windowed signal data. Parameters ---------- sliding_window : SlidingWindow Buffer containing signal data to analyze. - If signal_type='velocity': expects velocity values - If signal_type='position': expects position values (1D or 2D) Returns ------- dict Dictionary containing: - 'sparc': Spectral Arc Length (closer to 0 = smoother). Returns NaN if insufficient data. - 'jerk_rms': RMS of jerk. Returns NaN if insufficient data. \"\"\" if len ( sliding_window ) < 5 : return { \"sparc\" : np . nan , \"jerk_rms\" : np . nan } signal , _ = sliding_window . to_array () # Extract or compute velocity based on signal type if self . signal_type == 'position' : # For position: compute velocity for SPARC velocity = extract_velocity_from_position ( signal , self . rate_hz ) # For jerk: use first dimension of position signal_for_jerk = signal [:, 0 ] if signal . ndim > 1 else signal . squeeze () else : # self.signal_type == 'velocity' # For velocity: use directly if signal . ndim > 1 and signal . shape [ 1 ] > 1 : velocity = signal [:, 0 ] else : velocity = signal . squeeze () signal_for_jerk = velocity # Apply filtering to velocity for SPARC filtered_velocity = self . _filter_signal ( velocity ) normalized = normalize_signal ( filtered_velocity ) # Compute SPARC (always uses velocity) sparc = compute_sparc ( normalized , self . rate_hz ) # Compute jerk with appropriate signal type filtered_for_jerk = self . _filter_signal ( signal_for_jerk ) jerk = compute_jerk_rms ( filtered_for_jerk , self . rate_hz , signal_type = self . signal_type ) return { \"sparc\" : sparc , \"jerk_rms\" : jerk }","title":"__call__"},{"location":"API/mid_level/lightness/","text":"Lightness","title":"Lightness"},{"location":"API/mid_level/lightness/#lightness","text":"","title":"Lightness"},{"location":"API/utils/math_utils/","text":"Math Utils Mathematical utility functions for signal analysis. This module provides mathematical functions used throughout the PyEyesWeb library for signal processing, phase analysis, and movement metrics. center_signals ( sig ) Remove the mean from each signal to center the data. Centers signals by subtracting the mean, removing DC bias. Parameters: sig ( ndarray ) \u2013 Signal array of shape (n_samples, n_channels). Returns: ndarray \u2013 Centered signal with same shape as input. Source code in pyeyesweb/utils/math_utils.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def center_signals ( sig ): \"\"\"Remove the mean from each signal to center the data. Centers signals by subtracting the mean, removing DC bias. Parameters ---------- sig : ndarray Signal array of shape (n_samples, n_channels). Returns ------- ndarray Centered signal with same shape as input. \"\"\" return sig - np . mean ( sig , axis = 0 , keepdims = True ) compute_jerk_rms ( signal , rate_hz = 50.0 , signal_type = 'velocity' ) Compute RMS of jerk (rate of change of acceleration) from a signal. Jerk is the third derivative of position or the first derivative of acceleration. Lower RMS jerk values indicate smoother movement. Parameters: signal ( ndarray ) \u2013 1D movement signal. rate_hz ( float , default: 50.0 ) \u2013 Sampling rate in Hz (default: 50.0). signal_type ( str , default: 'velocity' ) \u2013 Type of input signal: 'position' or 'velocity' (default: 'velocity'). - 'position': Computes third derivative to get jerk - 'velocity': Computes second derivative to get jerk Returns: float \u2013 Root mean square of jerk. Returns NaN if signal has insufficient samples for the required derivatives. Notes Uses numpy.gradient for smooth derivative approximation with central differences where possible, providing better accuracy than forward differences. Source code in pyeyesweb/utils/math_utils.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def compute_jerk_rms ( signal , rate_hz = 50.0 , signal_type = 'velocity' ): \"\"\"Compute RMS of jerk (rate of change of acceleration) from a signal. Jerk is the third derivative of position or the first derivative of acceleration. Lower RMS jerk values indicate smoother movement. Parameters ---------- signal : ndarray 1D movement signal. rate_hz : float, optional Sampling rate in Hz (default: 50.0). signal_type : str, optional Type of input signal: 'position' or 'velocity' (default: 'velocity'). - 'position': Computes third derivative to get jerk - 'velocity': Computes second derivative to get jerk Returns ------- float Root mean square of jerk. Returns NaN if signal has insufficient samples for the required derivatives. Notes ----- Uses numpy.gradient for smooth derivative approximation with central differences where possible, providing better accuracy than forward differences. \"\"\" rate_hz = validate_numeric ( rate_hz , 'rate_hz' , min_val = 0.0001 ) # Define derivative orders needed for each signal type derivative_orders = { 'velocity' : 2 , # if signal type is velocity we can get velocity -> acceleration -> jerk 'position' : 3 # if signal type is position we can get position -> velocity -> acceleration -> jerk } if signal_type not in derivative_orders : raise ValueError ( f \"signal_type must be 'position' or 'velocity', got ' { signal_type } '\" ) n_derivatives = derivative_orders [ signal_type ] min_samples = n_derivatives + 1 if len ( signal ) < min_samples : return np . nan # Apply derivatives using numpy.gradient for better accuracy result = np . asarray ( signal ) for _ in range ( n_derivatives ): result = np . gradient ( result , 1.0 / rate_hz ) return np . sqrt ( np . mean ( result ** 2 )) compute_phase_locking_value ( phase1 , phase2 ) Compute the Phase Locking Value (PLV) from two phase arrays. PLV measures the inter-trial variability of the phase difference between two signals. A value of 1 indicates perfect phase locking, while 0 indicates no phase relationship. Parameters: phase1 ( ndarray ) \u2013 Phase values of first signal in radians. phase2 ( ndarray ) \u2013 Phase values of second signal in radians. Returns: float \u2013 Phase Locking Value between 0 and 1. References Lachaux et al. (1999). Measuring phase synchrony in brain signals. Source code in pyeyesweb/utils/math_utils.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def compute_phase_locking_value ( phase1 , phase2 ): \"\"\"Compute the Phase Locking Value (PLV) from two phase arrays. PLV measures the inter-trial variability of the phase difference between two signals. A value of 1 indicates perfect phase locking, while 0 indicates no phase relationship. Parameters ---------- phase1 : ndarray Phase values of first signal in radians. phase2 : ndarray Phase values of second signal in radians. Returns ------- float Phase Locking Value between 0 and 1. References ---------- Lachaux et al. (1999). Measuring phase synchrony in brain signals. \"\"\" phase_diff = phase1 - phase2 phase_diff_exp = np . exp ( 1 j * phase_diff ) plv = np . abs ( np . mean ( phase_diff_exp )) return plv compute_sparc ( signal , rate_hz = 50.0 ) Compute SPARC (Spectral Arc Length) from a signal. SPARC is a dimensionless smoothness metric that quantifies movement smoothness independent of movement amplitude and duration. More negative values indicate smoother movement. This implementation is based on the original algorithm by Balasubramanian et al. (2015). SPARC values are typically negative, with values closer to 0 indicating less smooth (more complex) movements. For healthy reaching movements, values around -1.4 to -1.6 are common. Pathological or very unsmooth movements may have values ranging from -3 to -10 or lower, depending on the degree of movement fragmentation. Parameters: signal ( ndarray ) \u2013 1D movement signal. rate_hz ( float , default: 50.0 ) \u2013 Sampling rate in Hz (default: 50.0). Returns: float \u2013 SPARC value (negative, more negative = smoother). Returns NaN if signal has less than 2 samples. References Balasubramanian, S., Melendez-Calderon, A., Roby-Brami, A., & Burdet, E. (2015). On the analysis of movement smoothness. Journal of NeuroEngineering and Rehabilitation, 12(1), 1-11. Source code in pyeyesweb/utils/math_utils.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def compute_sparc ( signal , rate_hz = 50.0 ): \"\"\"Compute SPARC (Spectral Arc Length) from a signal. SPARC is a dimensionless smoothness metric that quantifies movement smoothness independent of movement amplitude and duration. More negative values indicate smoother movement. This implementation is based on the original algorithm by Balasubramanian et al. (2015). SPARC values are typically negative, with values closer to 0 indicating less smooth (more complex) movements. For healthy reaching movements, values around -1.4 to -1.6 are common. Pathological or very unsmooth movements may have values ranging from -3 to -10 or lower, depending on the degree of movement fragmentation. Parameters ---------- signal : ndarray 1D movement signal. rate_hz : float, optional Sampling rate in Hz (default: 50.0). Returns ------- float SPARC value (negative, more negative = smoother). Returns NaN if signal has less than 2 samples. References ---------- Balasubramanian, S., Melendez-Calderon, A., Roby-Brami, A., & Burdet, E. (2015). On the analysis of movement smoothness. Journal of NeuroEngineering and Rehabilitation, 12(1), 1-11. \"\"\" rate_hz = validate_numeric ( rate_hz , 'rate_hz' , min_val = 0.0001 ) # Ensure signal is 1D signal = np . asarray ( signal ) if signal . ndim > 1 : # If 2D, check if it's a single column/row if signal . shape [ 0 ] == 1 : signal = signal . flatten () elif signal . shape [ 1 ] == 1 : signal = signal . flatten () else : raise ValueError ( f \"Signal must be 1D, got shape { signal . shape } \" ) N = len ( signal ) if N < 2 : return np . nan # Check if signal is constant (no movement) if np . allclose ( signal , signal [ 0 ]): # For constant signals, return NaN as SPARC is undefined # (no movement means smoothness is not applicable) return np . nan from scipy.fft import fft , fftfreq yf = np . abs ( fft ( signal ))[: N // 2 ] xf = fftfreq ( N , 1.0 / rate_hz )[: N // 2 ] # Normalize magnitude by maximum value max_yf = np . max ( yf ) if max_yf > 0 : yf /= max_yf else : # This should not happen after the constant signal check # but keep as safety fallback return np . nan # Compute arc length with normalized frequency differences # Following Balasubramanian et al. (2015) implementation freq_range = xf [ - 1 ] - xf [ 0 ] if freq_range <= 0 : return np . nan # Normalize frequency differences by the frequency range arc = np . sum ( np . sqrt (( np . diff ( xf ) / freq_range ) ** 2 + np . diff ( yf ) ** 2 )) return - arc extract_velocity_from_position ( position , rate_hz = 50.0 ) Extract velocity from position data. Computes velocity magnitude from position data of any dimensionality. For 1D position, returns absolute velocity. For multi-dimensional position, returns the Euclidean norm of the velocity vector. Parameters: position ( ndarray ) \u2013 Position data. Can be: - 1D array: single position coordinate - 2D array with shape (n_samples, n_dims): multi-dimensional positions rate_hz ( float , default: 50.0 ) \u2013 Sampling rate in Hz (default: 50.0). Returns: ndarray \u2013 1D array of velocity magnitudes. Examples: >>> # 1D position data >>> position_1d = np . array ([ 0 , 1 , 2 , 3 , 4 ]) >>> velocity = extract_velocity_from_position ( position_1d , rate_hz = 100 ) >>> # 2D position data (x, y coordinates) >>> position_2d = np . array ([[ 0 , 0 ], [ 1 , 0 ], [ 1 , 1 ], [ 2 , 1 ]]) >>> velocity = extract_velocity_from_position ( position_2d , rate_hz = 100 ) Source code in pyeyesweb/utils/math_utils.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def extract_velocity_from_position ( position , rate_hz = 50.0 ): \"\"\"Extract velocity from position data. Computes velocity magnitude from position data of any dimensionality. For 1D position, returns absolute velocity. For multi-dimensional position, returns the Euclidean norm of the velocity vector. Parameters ---------- position : ndarray Position data. Can be: - 1D array: single position coordinate - 2D array with shape (n_samples, n_dims): multi-dimensional positions rate_hz : float, optional Sampling rate in Hz (default: 50.0). Returns ------- ndarray 1D array of velocity magnitudes. Examples -------- >>> # 1D position data >>> position_1d = np.array([0, 1, 2, 3, 4]) >>> velocity = extract_velocity_from_position(position_1d, rate_hz=100) >>> # 2D position data (x, y coordinates) >>> position_2d = np.array([[0, 0], [1, 0], [1, 1], [2, 1]]) >>> velocity = extract_velocity_from_position(position_2d, rate_hz=100) \"\"\" rate_hz = validate_numeric ( rate_hz , 'rate_hz' , min_val = 0.0001 ) dt = 1.0 / rate_hz position = np . asarray ( position ) # Handle 1D position if position . ndim == 1 or ( position . ndim == 2 and position . shape [ 1 ] == 1 ): signal_1d = position . squeeze () return np . abs ( np . gradient ( signal_1d , dt )) # Handle multi-dimensional position if position . ndim == 2 : # Compute derivatives along time axis (axis=0) derivatives = np . gradient ( position , dt , axis = 0 ) # Return Euclidean norm of velocity vector return np . linalg . norm ( derivatives , axis = 1 ) raise ValueError ( f \"Position must be 1D or 2D array, got shape { position . shape } \" ) normalize_signal ( signal ) Normalize signal by its maximum absolute value. Scales the signal to the range [-1, 1] by dividing by the maximum absolute value. Parameters: signal ( ndarray ) \u2013 Input signal to normalize. Returns: ndarray \u2013 Normalized signal with same shape as input. Returns original signal if max absolute value is 0. Source code in pyeyesweb/utils/math_utils.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def normalize_signal ( signal ): \"\"\"Normalize signal by its maximum absolute value. Scales the signal to the range [-1, 1] by dividing by the maximum absolute value. Parameters ---------- signal : ndarray Input signal to normalize. Returns ------- ndarray Normalized signal with same shape as input. Returns original signal if max absolute value is 0. \"\"\" max_val = np . max ( np . abs ( signal )) return signal / max_val if max_val != 0 else signal","title":"Math Utils"},{"location":"API/utils/math_utils/#math-utils","text":"Mathematical utility functions for signal analysis. This module provides mathematical functions used throughout the PyEyesWeb library for signal processing, phase analysis, and movement metrics.","title":"Math Utils"},{"location":"API/utils/math_utils/#pyeyesweb.utils.math_utils.center_signals","text":"Remove the mean from each signal to center the data. Centers signals by subtracting the mean, removing DC bias. Parameters: sig ( ndarray ) \u2013 Signal array of shape (n_samples, n_channels). Returns: ndarray \u2013 Centered signal with same shape as input. Source code in pyeyesweb/utils/math_utils.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def center_signals ( sig ): \"\"\"Remove the mean from each signal to center the data. Centers signals by subtracting the mean, removing DC bias. Parameters ---------- sig : ndarray Signal array of shape (n_samples, n_channels). Returns ------- ndarray Centered signal with same shape as input. \"\"\" return sig - np . mean ( sig , axis = 0 , keepdims = True )","title":"center_signals"},{"location":"API/utils/math_utils/#pyeyesweb.utils.math_utils.compute_jerk_rms","text":"Compute RMS of jerk (rate of change of acceleration) from a signal. Jerk is the third derivative of position or the first derivative of acceleration. Lower RMS jerk values indicate smoother movement. Parameters: signal ( ndarray ) \u2013 1D movement signal. rate_hz ( float , default: 50.0 ) \u2013 Sampling rate in Hz (default: 50.0). signal_type ( str , default: 'velocity' ) \u2013 Type of input signal: 'position' or 'velocity' (default: 'velocity'). - 'position': Computes third derivative to get jerk - 'velocity': Computes second derivative to get jerk Returns: float \u2013 Root mean square of jerk. Returns NaN if signal has insufficient samples for the required derivatives. Notes Uses numpy.gradient for smooth derivative approximation with central differences where possible, providing better accuracy than forward differences. Source code in pyeyesweb/utils/math_utils.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def compute_jerk_rms ( signal , rate_hz = 50.0 , signal_type = 'velocity' ): \"\"\"Compute RMS of jerk (rate of change of acceleration) from a signal. Jerk is the third derivative of position or the first derivative of acceleration. Lower RMS jerk values indicate smoother movement. Parameters ---------- signal : ndarray 1D movement signal. rate_hz : float, optional Sampling rate in Hz (default: 50.0). signal_type : str, optional Type of input signal: 'position' or 'velocity' (default: 'velocity'). - 'position': Computes third derivative to get jerk - 'velocity': Computes second derivative to get jerk Returns ------- float Root mean square of jerk. Returns NaN if signal has insufficient samples for the required derivatives. Notes ----- Uses numpy.gradient for smooth derivative approximation with central differences where possible, providing better accuracy than forward differences. \"\"\" rate_hz = validate_numeric ( rate_hz , 'rate_hz' , min_val = 0.0001 ) # Define derivative orders needed for each signal type derivative_orders = { 'velocity' : 2 , # if signal type is velocity we can get velocity -> acceleration -> jerk 'position' : 3 # if signal type is position we can get position -> velocity -> acceleration -> jerk } if signal_type not in derivative_orders : raise ValueError ( f \"signal_type must be 'position' or 'velocity', got ' { signal_type } '\" ) n_derivatives = derivative_orders [ signal_type ] min_samples = n_derivatives + 1 if len ( signal ) < min_samples : return np . nan # Apply derivatives using numpy.gradient for better accuracy result = np . asarray ( signal ) for _ in range ( n_derivatives ): result = np . gradient ( result , 1.0 / rate_hz ) return np . sqrt ( np . mean ( result ** 2 ))","title":"compute_jerk_rms"},{"location":"API/utils/math_utils/#pyeyesweb.utils.math_utils.compute_phase_locking_value","text":"Compute the Phase Locking Value (PLV) from two phase arrays. PLV measures the inter-trial variability of the phase difference between two signals. A value of 1 indicates perfect phase locking, while 0 indicates no phase relationship. Parameters: phase1 ( ndarray ) \u2013 Phase values of first signal in radians. phase2 ( ndarray ) \u2013 Phase values of second signal in radians. Returns: float \u2013 Phase Locking Value between 0 and 1. References Lachaux et al. (1999). Measuring phase synchrony in brain signals. Source code in pyeyesweb/utils/math_utils.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def compute_phase_locking_value ( phase1 , phase2 ): \"\"\"Compute the Phase Locking Value (PLV) from two phase arrays. PLV measures the inter-trial variability of the phase difference between two signals. A value of 1 indicates perfect phase locking, while 0 indicates no phase relationship. Parameters ---------- phase1 : ndarray Phase values of first signal in radians. phase2 : ndarray Phase values of second signal in radians. Returns ------- float Phase Locking Value between 0 and 1. References ---------- Lachaux et al. (1999). Measuring phase synchrony in brain signals. \"\"\" phase_diff = phase1 - phase2 phase_diff_exp = np . exp ( 1 j * phase_diff ) plv = np . abs ( np . mean ( phase_diff_exp )) return plv","title":"compute_phase_locking_value"},{"location":"API/utils/math_utils/#pyeyesweb.utils.math_utils.compute_sparc","text":"Compute SPARC (Spectral Arc Length) from a signal. SPARC is a dimensionless smoothness metric that quantifies movement smoothness independent of movement amplitude and duration. More negative values indicate smoother movement. This implementation is based on the original algorithm by Balasubramanian et al. (2015). SPARC values are typically negative, with values closer to 0 indicating less smooth (more complex) movements. For healthy reaching movements, values around -1.4 to -1.6 are common. Pathological or very unsmooth movements may have values ranging from -3 to -10 or lower, depending on the degree of movement fragmentation. Parameters: signal ( ndarray ) \u2013 1D movement signal. rate_hz ( float , default: 50.0 ) \u2013 Sampling rate in Hz (default: 50.0). Returns: float \u2013 SPARC value (negative, more negative = smoother). Returns NaN if signal has less than 2 samples. References Balasubramanian, S., Melendez-Calderon, A., Roby-Brami, A., & Burdet, E. (2015). On the analysis of movement smoothness. Journal of NeuroEngineering and Rehabilitation, 12(1), 1-11. Source code in pyeyesweb/utils/math_utils.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def compute_sparc ( signal , rate_hz = 50.0 ): \"\"\"Compute SPARC (Spectral Arc Length) from a signal. SPARC is a dimensionless smoothness metric that quantifies movement smoothness independent of movement amplitude and duration. More negative values indicate smoother movement. This implementation is based on the original algorithm by Balasubramanian et al. (2015). SPARC values are typically negative, with values closer to 0 indicating less smooth (more complex) movements. For healthy reaching movements, values around -1.4 to -1.6 are common. Pathological or very unsmooth movements may have values ranging from -3 to -10 or lower, depending on the degree of movement fragmentation. Parameters ---------- signal : ndarray 1D movement signal. rate_hz : float, optional Sampling rate in Hz (default: 50.0). Returns ------- float SPARC value (negative, more negative = smoother). Returns NaN if signal has less than 2 samples. References ---------- Balasubramanian, S., Melendez-Calderon, A., Roby-Brami, A., & Burdet, E. (2015). On the analysis of movement smoothness. Journal of NeuroEngineering and Rehabilitation, 12(1), 1-11. \"\"\" rate_hz = validate_numeric ( rate_hz , 'rate_hz' , min_val = 0.0001 ) # Ensure signal is 1D signal = np . asarray ( signal ) if signal . ndim > 1 : # If 2D, check if it's a single column/row if signal . shape [ 0 ] == 1 : signal = signal . flatten () elif signal . shape [ 1 ] == 1 : signal = signal . flatten () else : raise ValueError ( f \"Signal must be 1D, got shape { signal . shape } \" ) N = len ( signal ) if N < 2 : return np . nan # Check if signal is constant (no movement) if np . allclose ( signal , signal [ 0 ]): # For constant signals, return NaN as SPARC is undefined # (no movement means smoothness is not applicable) return np . nan from scipy.fft import fft , fftfreq yf = np . abs ( fft ( signal ))[: N // 2 ] xf = fftfreq ( N , 1.0 / rate_hz )[: N // 2 ] # Normalize magnitude by maximum value max_yf = np . max ( yf ) if max_yf > 0 : yf /= max_yf else : # This should not happen after the constant signal check # but keep as safety fallback return np . nan # Compute arc length with normalized frequency differences # Following Balasubramanian et al. (2015) implementation freq_range = xf [ - 1 ] - xf [ 0 ] if freq_range <= 0 : return np . nan # Normalize frequency differences by the frequency range arc = np . sum ( np . sqrt (( np . diff ( xf ) / freq_range ) ** 2 + np . diff ( yf ) ** 2 )) return - arc","title":"compute_sparc"},{"location":"API/utils/math_utils/#pyeyesweb.utils.math_utils.extract_velocity_from_position","text":"Extract velocity from position data. Computes velocity magnitude from position data of any dimensionality. For 1D position, returns absolute velocity. For multi-dimensional position, returns the Euclidean norm of the velocity vector. Parameters: position ( ndarray ) \u2013 Position data. Can be: - 1D array: single position coordinate - 2D array with shape (n_samples, n_dims): multi-dimensional positions rate_hz ( float , default: 50.0 ) \u2013 Sampling rate in Hz (default: 50.0). Returns: ndarray \u2013 1D array of velocity magnitudes. Examples: >>> # 1D position data >>> position_1d = np . array ([ 0 , 1 , 2 , 3 , 4 ]) >>> velocity = extract_velocity_from_position ( position_1d , rate_hz = 100 ) >>> # 2D position data (x, y coordinates) >>> position_2d = np . array ([[ 0 , 0 ], [ 1 , 0 ], [ 1 , 1 ], [ 2 , 1 ]]) >>> velocity = extract_velocity_from_position ( position_2d , rate_hz = 100 ) Source code in pyeyesweb/utils/math_utils.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def extract_velocity_from_position ( position , rate_hz = 50.0 ): \"\"\"Extract velocity from position data. Computes velocity magnitude from position data of any dimensionality. For 1D position, returns absolute velocity. For multi-dimensional position, returns the Euclidean norm of the velocity vector. Parameters ---------- position : ndarray Position data. Can be: - 1D array: single position coordinate - 2D array with shape (n_samples, n_dims): multi-dimensional positions rate_hz : float, optional Sampling rate in Hz (default: 50.0). Returns ------- ndarray 1D array of velocity magnitudes. Examples -------- >>> # 1D position data >>> position_1d = np.array([0, 1, 2, 3, 4]) >>> velocity = extract_velocity_from_position(position_1d, rate_hz=100) >>> # 2D position data (x, y coordinates) >>> position_2d = np.array([[0, 0], [1, 0], [1, 1], [2, 1]]) >>> velocity = extract_velocity_from_position(position_2d, rate_hz=100) \"\"\" rate_hz = validate_numeric ( rate_hz , 'rate_hz' , min_val = 0.0001 ) dt = 1.0 / rate_hz position = np . asarray ( position ) # Handle 1D position if position . ndim == 1 or ( position . ndim == 2 and position . shape [ 1 ] == 1 ): signal_1d = position . squeeze () return np . abs ( np . gradient ( signal_1d , dt )) # Handle multi-dimensional position if position . ndim == 2 : # Compute derivatives along time axis (axis=0) derivatives = np . gradient ( position , dt , axis = 0 ) # Return Euclidean norm of velocity vector return np . linalg . norm ( derivatives , axis = 1 ) raise ValueError ( f \"Position must be 1D or 2D array, got shape { position . shape } \" )","title":"extract_velocity_from_position"},{"location":"API/utils/math_utils/#pyeyesweb.utils.math_utils.normalize_signal","text":"Normalize signal by its maximum absolute value. Scales the signal to the range [-1, 1] by dividing by the maximum absolute value. Parameters: signal ( ndarray ) \u2013 Input signal to normalize. Returns: ndarray \u2013 Normalized signal with same shape as input. Returns original signal if max absolute value is 0. Source code in pyeyesweb/utils/math_utils.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def normalize_signal ( signal ): \"\"\"Normalize signal by its maximum absolute value. Scales the signal to the range [-1, 1] by dividing by the maximum absolute value. Parameters ---------- signal : ndarray Input signal to normalize. Returns ------- ndarray Normalized signal with same shape as input. Returns original signal if max absolute value is 0. \"\"\" max_val = np . max ( np . abs ( signal )) return signal / max_val if max_val != 0 else signal","title":"normalize_signal"},{"location":"API/utils/signal_generators/","text":"Signal Generators Signal Generation Utilities for PyEyesWeb Testing This module provides signal generation capabilities for testing PyEyesWeb features. It includes various signal types from simple periodic waves to complex modulated and stochastic signals. Author: PyEyesWeb Development Team SignalGenerator Signal generator for testing and analysis. This class provides methods to generate various types of signals commonly used in signal processing, neuroscience, and movement analysis. Source code in pyeyesweb/utils/signal_generators.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 class SignalGenerator : \"\"\" Signal generator for testing and analysis. This class provides methods to generate various types of signals commonly used in signal processing, neuroscience, and movement analysis. \"\"\" # Registry of available signal generators _generators : Dict [ str , Callable ] = {} def __init__ ( self , sampling_rate : float = 100.0 ): \"\"\" Initialize the signal generator. Args: sampling_rate: Default sampling rate in Hz \"\"\" self . sampling_rate = sampling_rate self . _register_generators () def _create_time_array ( self , length : int ) -> np . ndarray : \"\"\"Create time array for signal generation. Args: length: Number of samples Returns: Time array from 0 to duration \"\"\" return np . linspace ( 0 , length / self . sampling_rate , length ) def _create_metadata ( self , signal_type : str , ** params ) -> Dict [ str , Any ]: \"\"\"Create metadata dictionary with common fields. Args: signal_type: Type of signal **params: Additional parameters to include Returns: Metadata dictionary with type, sampling_rate, and additional params \"\"\" metadata = { 'type' : signal_type , 'sampling_rate' : self . sampling_rate } metadata . update ( params ) return metadata def _register_generators ( self ): \"\"\"Register all available signal generators.\"\"\" self . _generators = { # Basic waveforms 'sine' : self . sine_wave , 'cosine' : self . cosine_wave , 'square' : self . square_wave , 'sawtooth' : self . sawtooth_wave , 'triangle' : self . triangle_wave , # Noise signals 'random' : self . random_signal , 'gaussian' : self . gaussian_noise , 'pink' : self . pink_noise , 'brown' : self . brownian_motion , 'white' : self . white_noise , # Complex signals 'chirp' : self . chirp_signal , 'chirp_exp' : self . exponential_chirp , 'chirp_hyperbolic' : self . hyperbolic_chirp , 'multisine' : self . multi_sine , 'complex' : self . complex_signal , # Modulated signals 'am' : self . amplitude_modulated , 'fm' : self . frequency_modulated , 'pm' : self . phase_modulated , 'pwm' : self . pulse_width_modulated , # Transient signals 'impulse' : self . impulse_train , 'step' : self . step_function , 'ramp' : self . ramp_function , 'exponential' : self . exponential_decay , 'damped_sine' : self . damped_sine , # Biological/Movement signals 'ecg' : self . ecg_like , 'emg' : self . emg_like , 'tremor' : self . tremor_signal , 'gait' : self . gait_pattern , 'breathing' : self . breathing_pattern , # Special signals 'lorenz' : self . lorenz_attractor , 'chaos' : self . logistic_map , 'fractal' : self . fractal_noise , 'burst' : self . burst_signal , 'spike_train' : self . spike_train , # Combined signals 'noisy_sine' : self . noisy_sine , 'drift_sine' : self . sine_with_drift , 'intermittent' : self . intermittent_signal , 'switched' : self . switched_signal , } def generate ( self , signal_type : str , length : int = 1000 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict [ str , Any ]]: \"\"\" Generate a signal based on type and parameters. Args: signal_type: Type of signal to generate length: Number of samples **kwargs: Additional parameters specific to each signal type Returns: Tuple of (time_array, signal_array, metadata_dict) \"\"\" if signal_type not in self . _generators : available = ', ' . join ( sorted ( self . _generators . keys ())) raise ValueError ( f \"Unknown signal type: ' { signal_type } '. Available types: { available } \" ) # Override sampling rate if provided if 'sampling_rate' in kwargs : self . sampling_rate = kwargs [ 'sampling_rate' ] return self . _generators [ signal_type ]( length , ** kwargs ) @property def available_signals ( self ) -> List [ str ]: \"\"\"Get list of available signal types.\"\"\" return sorted ( list ( self . _generators . keys ())) @property def signal_info ( self ) -> Dict [ str , Dict [ str , str ]]: \"\"\"Get detailed information about all signal types. Returns ------- dict Dictionary mapping signal names to their metadata including description and category. \"\"\" return { # Basic Waveforms 'sine' : { 'description' : 'Sine wave' , 'category' : 'Basic Waveforms' }, 'cosine' : { 'description' : 'Cosine wave' , 'category' : 'Basic Waveforms' }, 'square' : { 'description' : 'Square wave' , 'category' : 'Basic Waveforms' }, 'sawtooth' : { 'description' : 'Sawtooth wave' , 'category' : 'Basic Waveforms' }, 'triangle' : { 'description' : 'Triangle wave' , 'category' : 'Basic Waveforms' }, # Noise Signals 'random' : { 'description' : 'Random uniform noise' , 'category' : 'Noise Signals' }, 'gaussian' : { 'description' : 'Gaussian noise' , 'category' : 'Noise Signals' }, 'white' : { 'description' : 'White noise' , 'category' : 'Noise Signals' }, 'pink' : { 'description' : 'Pink (1/f) noise' , 'category' : 'Noise Signals' }, 'brown' : { 'description' : 'Brownian motion (random walk)' , 'category' : 'Noise Signals' }, # Chirp Signals 'chirp' : { 'description' : 'Linear chirp signal' , 'category' : 'Chirp Signals' }, 'chirp_exp' : { 'description' : 'Exponential chirp signal' , 'category' : 'Chirp Signals' }, 'chirp_hyperbolic' : { 'description' : 'Hyperbolic chirp signal' , 'category' : 'Chirp Signals' }, # Modulated 'am' : { 'description' : 'Amplitude modulated signal' , 'category' : 'Modulated' }, 'fm' : { 'description' : 'Frequency modulated signal' , 'category' : 'Modulated' }, 'pm' : { 'description' : 'Phase modulated signal' , 'category' : 'Modulated' }, 'pwm' : { 'description' : 'Pulse width modulated signal' , 'category' : 'Modulated' }, # Transient 'impulse' : { 'description' : 'Impulse train' , 'category' : 'Transient' }, 'step' : { 'description' : 'Step function' , 'category' : 'Transient' }, 'ramp' : { 'description' : 'Ramp function' , 'category' : 'Transient' }, 'exponential' : { 'description' : 'Exponential decay' , 'category' : 'Transient' }, 'damped_sine' : { 'description' : 'Damped sine wave' , 'category' : 'Transient' }, # Biological 'ecg' : { 'description' : 'ECG-like biological signal' , 'category' : 'Biological' }, 'emg' : { 'description' : 'EMG-like muscle activity signal' , 'category' : 'Biological' }, 'tremor' : { 'description' : 'Tremor signal (slow + fast oscillation)' , 'category' : 'Biological' }, 'gait' : { 'description' : 'Gait pattern signal' , 'category' : 'Biological' }, 'breathing' : { 'description' : 'Breathing pattern signal' , 'category' : 'Biological' }, # Complex 'complex' : { 'description' : 'Complex multi-frequency signal' , 'category' : 'Complex' }, 'multisine' : { 'description' : 'Multiple sine waves combined' , 'category' : 'Complex' }, 'noisy_sine' : { 'description' : 'Sine wave with noise' , 'category' : 'Complex' }, 'drift_sine' : { 'description' : 'Sine wave with linear drift' , 'category' : 'Complex' }, 'intermittent' : { 'description' : 'Intermittent signal' , 'category' : 'Complex' }, 'switched' : { 'description' : 'Frequency-switching signal' , 'category' : 'Complex' }, # Chaotic 'lorenz' : { 'description' : 'Lorenz attractor (chaotic)' , 'category' : 'Chaotic' }, 'chaos' : { 'description' : 'Logistic map (chaotic)' , 'category' : 'Chaotic' }, 'fractal' : { 'description' : 'Fractal noise (fBm)' , 'category' : 'Chaotic' }, 'burst' : { 'description' : 'Burst signal' , 'category' : 'Chaotic' }, 'spike_train' : { 'description' : 'Neural spike train' , 'category' : 'Chaotic' }, } def get_signals_by_category ( self ) -> Dict [ str , List [ str ]]: \"\"\"Get signals organized by category. Returns ------- dict Dictionary mapping category names to lists of signal types. \"\"\" categories = {} for signal_name , info in self . signal_info . items (): category = info [ 'category' ] if category not in categories : categories [ category ] = [] categories [ category ] . append ( signal_name ) return categories # ======================================================================== # Basic Waveforms # ======================================================================== def sine_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , phase : float = 0.0 , dc_offset : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sine wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * np . sin ( 2 * np . pi * freq * t + phase ) + dc_offset metadata = self . _create_metadata ( 'sine' , frequency = freq , amplitude = amplitude , phase = phase , dc_offset = dc_offset ) return t , signal , metadata def cosine_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , phase : float = 0.0 , dc_offset : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a cosine wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * np . cos ( 2 * np . pi * freq * t + phase ) + dc_offset metadata = { 'type' : 'cosine' , 'frequency' : freq , 'amplitude' : amplitude , 'phase' : phase , 'dc_offset' : dc_offset , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def square_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , duty_cycle : float = 0.5 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a square wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . square ( 2 * np . pi * freq * t , duty = duty_cycle ) metadata = { 'type' : 'square' , 'frequency' : freq , 'amplitude' : amplitude , 'duty_cycle' : duty_cycle , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def sawtooth_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , width : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sawtooth wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . sawtooth ( 2 * np . pi * freq * t , width = width ) metadata = { 'type' : 'sawtooth' , 'frequency' : freq , 'amplitude' : amplitude , 'width' : width , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def triangle_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a triangle wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . sawtooth ( 2 * np . pi * freq * t , width = 0.5 ) metadata = { 'type' : 'triangle' , 'frequency' : freq , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Noise Signals # ======================================================================== def random_signal ( self , length : int , seed : Optional [ int ] = None , min_val : float = - 1.0 , max_val : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate random uniform noise.\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) signal = np . random . uniform ( min_val , max_val , length ) metadata = { 'type' : 'random' , 'distribution' : 'uniform' , 'range' : [ min_val , max_val ], 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def gaussian_noise ( self , length : int , mean : float = 0.0 , std : float = 1.0 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate Gaussian (white) noise.\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) signal = np . random . normal ( mean , std , length ) metadata = { 'type' : 'gaussian' , 'mean' : mean , 'std' : std , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def white_noise ( self , length : int , power : float = 1.0 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate white noise with specified power.\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) signal = np . random . normal ( 0 , np . sqrt ( power ), length ) metadata = { 'type' : 'white_noise' , 'power' : power , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def pink_noise ( self , length : int , amplitude : float = 1.0 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate pink (1/f) noise.\"\"\" if seed is not None : np . random . seed ( seed ) # Generate white noise white = np . random . randn ( length ) # Apply 1/f filter in frequency domain fft = np . fft . rfft ( white ) freqs = np . fft . rfftfreq ( length ) freqs [ 0 ] = 1 # Avoid division by zero fft = fft / np . sqrt ( freqs ) signal = np . fft . irfft ( fft , length ) # Normalize signal = amplitude * signal / np . std ( signal ) t = self . _create_time_array ( length ) metadata = { 'type' : 'pink_noise' , 'amplitude' : amplitude , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def brownian_motion ( self , length : int , std : float = 0.1 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate Brownian motion (random walk).\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) steps = np . random . normal ( 0 , std , length ) signal = np . cumsum ( steps ) metadata = { 'type' : 'brownian_motion' , 'step_std' : std , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Complex Signals # ======================================================================== def chirp_signal ( self , length : int , freq_start : float = 1.0 , freq_end : float = 10.0 , amplitude : float = 1.0 , method : str = 'linear' , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a chirp signal (linear frequency sweep).\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . chirp ( t , freq_start , t [ - 1 ], freq_end , method = method ) metadata = { 'type' : 'chirp' , 'freq_start' : freq_start , 'freq_end' : freq_end , 'method' : method , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def exponential_chirp ( self , length : int , freq_start : float = 1.0 , freq_end : float = 10.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an exponential chirp signal.\"\"\" return self . chirp_signal ( length , freq_start , freq_end , amplitude , method = 'exponential' ) def hyperbolic_chirp ( self , length : int , freq_start : float = 1.0 , freq_end : float = 10.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a hyperbolic chirp signal.\"\"\" return self . chirp_signal ( length , freq_start , freq_end , amplitude , method = 'hyperbolic' ) def multi_sine ( self , length : int , frequencies : List [ float ] = None , amplitudes : List [ float ] = None , phases : List [ float ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate multiple sine waves combined.\"\"\" if frequencies is None : frequencies = [ 1.0 , 3.0 , 5.0 ] if amplitudes is None : amplitudes = [ 1.0 ] * len ( frequencies ) if phases is None : phases = [ 0.0 ] * len ( frequencies ) t = self . _create_time_array ( length ) signal = np . zeros ( length ) for freq , amp , phase in zip ( frequencies , amplitudes , phases ): signal += amp * np . sin ( 2 * np . pi * freq * t + phase ) metadata = { 'type' : 'multi_sine' , 'frequencies' : frequencies , 'amplitudes' : amplitudes , 'phases' : phases , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def complex_signal ( self , length : int , components : List [ Dict ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a complex signal with multiple frequency components.\"\"\" if components is None : components = [ { 'freq' : 5 , 'amp' : 1.0 , 'phase' : 0 }, { 'freq' : 10 , 'amp' : 0.5 , 'phase' : np . pi / 4 }, { 'freq' : 20 , 'amp' : 0.3 , 'phase' : np . pi / 2 } ] t = self . _create_time_array ( length ) signal = np . zeros ( length ) for comp in components : signal += comp [ 'amp' ] * np . sin ( 2 * np . pi * comp [ 'freq' ] * t + comp . get ( 'phase' , 0 )) metadata = { 'type' : 'complex' , 'components' : components , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Modulated Signals # ======================================================================== def amplitude_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , modulation_index : float = 0.5 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an amplitude modulated signal.\"\"\" t = self . _create_time_array ( length ) carrier = np . sin ( 2 * np . pi * carrier_freq * t ) modulator = 1 + modulation_index * np . sin ( 2 * np . pi * modulation_freq * t ) signal = modulator * carrier metadata = { 'type' : 'amplitude_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'modulation_index' : modulation_index , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def frequency_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , modulation_index : float = 5.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a frequency modulated signal.\"\"\" t = self . _create_time_array ( length ) modulator = modulation_index * np . sin ( 2 * np . pi * modulation_freq * t ) phase = 2 * np . pi * carrier_freq * t + modulator signal = np . sin ( phase ) metadata = { 'type' : 'frequency_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'modulation_index' : modulation_index , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def phase_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , modulation_index : float = np . pi , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a phase modulated signal.\"\"\" t = self . _create_time_array ( length ) phase_modulation = modulation_index * np . sin ( 2 * np . pi * modulation_freq * t ) signal = np . sin ( 2 * np . pi * carrier_freq * t + phase_modulation ) metadata = { 'type' : 'phase_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'modulation_index' : modulation_index , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def pulse_width_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a pulse width modulated signal.\"\"\" t = self . _create_time_array ( length ) duty_cycle = 0.5 + 0.4 * np . sin ( 2 * np . pi * modulation_freq * t ) signal = np . zeros ( length ) # Generate PWM signal phase = ( carrier_freq * t ) % 1 for i in range ( length ): signal [ i ] = 1 if phase [ i ] < duty_cycle [ i ] else - 1 metadata = { 'type' : 'pulse_width_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Transient Signals # ======================================================================== def impulse_train ( self , length : int , period : int = 100 , amplitude : float = 1.0 , jitter : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an impulse train with optional jitter.\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) if jitter > 0 : positions = np . arange ( 0 , length , period ) positions += np . random . uniform ( - jitter * period , jitter * period , len ( positions )) positions = np . clip ( positions . astype ( int ), 0 , length - 1 ) signal [ positions ] = amplitude else : signal [:: period ] = amplitude metadata = { 'type' : 'impulse_train' , 'period' : period , 'amplitude' : amplitude , 'jitter' : jitter , 'num_impulses' : np . sum ( signal != 0 ), 'sampling_rate' : self . sampling_rate } return t , signal , metadata def step_function ( self , length : int , step_time : float = 0.5 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a step function.\"\"\" t = np . linspace ( 0 , 1 , length ) signal = np . ones ( length ) * amplitude signal [ t < step_time ] = 0 metadata = { 'type' : 'step' , 'step_time' : step_time , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t * length / self . sampling_rate , signal , metadata def ramp_function ( self , length : int , slope : float = 1.0 , start_value : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a ramp function.\"\"\" t = self . _create_time_array ( length ) signal = start_value + slope * t metadata = { 'type' : 'ramp' , 'slope' : slope , 'start_value' : start_value , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def exponential_decay ( self , length : int , amplitude : float = 1.0 , decay_rate : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an exponential decay signal.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * np . exp ( - decay_rate * t ) metadata = { 'type' : 'exponential_decay' , 'amplitude' : amplitude , 'decay_rate' : decay_rate , 'time_constant' : 1 / decay_rate , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def damped_sine ( self , length : int , freq : float = 5.0 , amplitude : float = 1.0 , damping : float = 0.1 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a damped sine wave.\"\"\" t = self . _create_time_array ( length ) envelope = amplitude * np . exp ( - damping * t ) signal = envelope * np . sin ( 2 * np . pi * freq * t ) metadata = { 'type' : 'damped_sine' , 'frequency' : freq , 'amplitude' : amplitude , 'damping' : damping , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Biological/Movement Signals # ======================================================================== def ecg_like ( self , length : int , heart_rate : float = 60.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an ECG-like signal.\"\"\" t = self . _create_time_array ( length ) beat_period = 60.0 / heart_rate # Period in seconds n_beats = int ( t [ - 1 ] / beat_period ) signal = np . zeros ( length ) # Simple ECG model with P, QRS, T waves for beat in range ( n_beats ): beat_start = beat * beat_period beat_indices = np . where (( t >= beat_start ) & ( t < beat_start + beat_period ))[ 0 ] if len ( beat_indices ) > 0 : beat_t = t [ beat_indices ] - beat_start # P wave p_wave = 0.2 * amplitude * np . exp ( - (( beat_t - 0.15 ) ** 2 ) / ( 2 * 0.01 )) # QRS complex qrs = amplitude * np . exp ( - (( beat_t - 0.2 ) ** 2 ) / ( 2 * 0.001 )) # T wave t_wave = 0.3 * amplitude * np . exp ( - (( beat_t - 0.4 ) ** 2 ) / ( 2 * 0.02 )) signal [ beat_indices ] = p_wave + qrs + t_wave metadata = { 'type' : 'ecg_like' , 'heart_rate' : heart_rate , 'amplitude' : amplitude , 'n_beats' : n_beats , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def emg_like ( self , length : int , burst_frequency : float = 2.0 , burst_duration : float = 0.2 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an EMG-like signal with bursts of activity.\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) # Generate burst envelope burst_period = 1.0 / burst_frequency for burst_start in np . arange ( 0 , t [ - 1 ], burst_period ): burst_mask = ( t >= burst_start ) & ( t < burst_start + burst_duration ) # High-frequency noise during burst signal [ burst_mask ] = amplitude * np . random . normal ( 0 , 1 , np . sum ( burst_mask )) # Add baseline noise signal += 0.05 * amplitude * np . random . normal ( 0 , 1 , length ) metadata = { 'type' : 'emg_like' , 'burst_frequency' : burst_frequency , 'burst_duration' : burst_duration , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def tremor_signal ( self , length : int , tremor_freq : float = 5.0 , base_freq : float = 0.5 , tremor_amplitude : float = 0.3 , base_amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a tremor-like signal (slow movement with superimposed tremor).\"\"\" t = self . _create_time_array ( length ) base_movement = base_amplitude * np . sin ( 2 * np . pi * base_freq * t ) tremor = tremor_amplitude * np . sin ( 2 * np . pi * tremor_freq * t ) signal = base_movement + tremor metadata = { 'type' : 'tremor' , 'tremor_freq' : tremor_freq , 'base_freq' : base_freq , 'tremor_amplitude' : tremor_amplitude , 'base_amplitude' : base_amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def gait_pattern ( self , length : int , step_frequency : float = 2.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a gait-like pattern.\"\"\" t = self . _create_time_array ( length ) # Double bump pattern for heel strike and toe-off signal = amplitude * ( np . sin ( 2 * np . pi * step_frequency * t ) + 0.3 * np . sin ( 4 * np . pi * step_frequency * t )) # Add some variability signal += 0.05 * amplitude * np . random . normal ( 0 , 1 , length ) metadata = { 'type' : 'gait_pattern' , 'step_frequency' : step_frequency , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def breathing_pattern ( self , length : int , breathing_rate : float = 12.0 , inhale_ratio : float = 0.4 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a breathing-like pattern.\"\"\" t = self . _create_time_array ( length ) breathing_period = 60.0 / breathing_rate # Period in seconds signal = np . zeros ( length ) for breath_start in np . arange ( 0 , t [ - 1 ], breathing_period ): inhale_end = breath_start + inhale_ratio * breathing_period exhale_end = breath_start + breathing_period # Inhale phase (rising) inhale_mask = ( t >= breath_start ) & ( t < inhale_end ) if np . any ( inhale_mask ): inhale_t = ( t [ inhale_mask ] - breath_start ) / ( inhale_ratio * breathing_period ) signal [ inhale_mask ] = amplitude * ( 1 - np . cos ( np . pi * inhale_t )) / 2 # Exhale phase (falling) exhale_mask = ( t >= inhale_end ) & ( t < exhale_end ) if np . any ( exhale_mask ): exhale_t = ( t [ exhale_mask ] - inhale_end ) / (( 1 - inhale_ratio ) * breathing_period ) signal [ exhale_mask ] = amplitude * ( 1 + np . cos ( np . pi * exhale_t )) / 2 metadata = { 'type' : 'breathing_pattern' , 'breathing_rate' : breathing_rate , 'inhale_ratio' : inhale_ratio , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Special Signals # ======================================================================== def lorenz_attractor ( self , length : int , sigma : float = 10.0 , rho : float = 28.0 , beta : float = 8 / 3 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate signal from Lorenz attractor (chaotic system).\"\"\" dt = 0.01 t = np . arange ( 0 , length * dt , dt )[: length ] # Initialize x , y , z = 1.0 , 1.0 , 1.0 signal = np . zeros ( length ) # Integrate Lorenz equations for i in range ( length ): dx = sigma * ( y - x ) * dt dy = ( x * ( rho - z ) - y ) * dt dz = ( x * y - beta * z ) * dt x += dx y += dy z += dz signal [ i ] = x # Use x coordinate as signal # Normalize signal = ( signal - np . mean ( signal )) / np . std ( signal ) metadata = { 'type' : 'lorenz_attractor' , 'sigma' : sigma , 'rho' : rho , 'beta' : beta , 'sampling_rate' : self . sampling_rate } return t * self . sampling_rate , signal , metadata def logistic_map ( self , length : int , r : float = 3.8 , x0 : float = 0.5 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate signal from logistic map (chaotic for r > 3.57).\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) x = x0 for i in range ( length ): x = r * x * ( 1 - x ) signal [ i ] = x metadata = { 'type' : 'logistic_map' , 'r' : r , 'x0' : x0 , 'chaotic' : r > 3.57 , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def fractal_noise ( self , length : int , hurst : float = 0.5 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate fractal noise using fractional Brownian motion.\"\"\" from scipy.fft import fft , ifft , fftfreq # Generate frequencies freqs = fftfreq ( length , 1 / self . sampling_rate ) freqs [ 0 ] = 1 # Avoid division by zero # Generate random phases phases = np . random . uniform ( 0 , 2 * np . pi , length ) # Create power spectrum with 1/f^(2H+1) scaling power = np . abs ( freqs ) ** - ( 2 * hurst + 1 ) power [ 0 ] = 0 # Remove DC component # Generate signal in frequency domain fft_signal = np . sqrt ( power ) * np . exp ( 1 j * phases ) # Transform to time domain signal = np . real ( ifft ( fft_signal )) signal = amplitude * signal / np . std ( signal ) t = self . _create_time_array ( length ) metadata = { 'type' : 'fractal_noise' , 'hurst' : hurst , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def burst_signal ( self , length : int , burst_freq : float = 2.0 , burst_duration : float = 0.1 , carrier_freq : float = 20.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate burst signal (periodic bursts of oscillation).\"\"\" t = self . _create_time_array ( length ) carrier = amplitude * np . sin ( 2 * np . pi * carrier_freq * t ) # Create burst envelope burst_period = 1.0 / burst_freq envelope = np . zeros ( length ) for burst_start in np . arange ( 0 , t [ - 1 ], burst_period ): burst_mask = ( t >= burst_start ) & ( t < burst_start + burst_duration ) envelope [ burst_mask ] = 1.0 signal = carrier * envelope metadata = { 'type' : 'burst_signal' , 'burst_freq' : burst_freq , 'burst_duration' : burst_duration , 'carrier_freq' : carrier_freq , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def spike_train ( self , length : int , spike_rate : float = 10.0 , refractory_period : float = 0.002 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a neural spike train.\"\"\" t = self . _create_time_array ( length ) dt = 1.0 / self . sampling_rate signal = np . zeros ( length ) # Generate spikes with refractory period last_spike_time = - refractory_period spike_times = [] for i , time in enumerate ( t ): if time - last_spike_time > refractory_period : if np . random . random () < spike_rate * dt : signal [ i ] = amplitude last_spike_time = time spike_times . append ( time ) metadata = { 'type' : 'spike_train' , 'spike_rate' : spike_rate , 'refractory_period' : refractory_period , 'amplitude' : amplitude , 'num_spikes' : len ( spike_times ), 'actual_rate' : len ( spike_times ) / ( t [ - 1 ] - t [ 0 ]) if t [ - 1 ] > t [ 0 ] else 0 , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Combined Signals # ======================================================================== def noisy_sine ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , noise_level : float = 0.1 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sine wave with added noise.\"\"\" t , clean_signal , _ = self . sine_wave ( length , freq , amplitude ) noise = np . random . normal ( 0 , noise_level , length ) signal = clean_signal + noise metadata = { 'type' : 'noisy_sine' , 'frequency' : freq , 'amplitude' : amplitude , 'noise_level' : noise_level , 'snr' : 20 * np . log10 ( amplitude / noise_level ) if noise_level > 0 else np . inf , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def sine_with_drift ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , drift_rate : float = 0.01 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sine wave with linear drift.\"\"\" t , sine_signal , _ = self . sine_wave ( length , freq , amplitude ) drift = drift_rate * t signal = sine_signal + drift metadata = { 'type' : 'sine_with_drift' , 'frequency' : freq , 'amplitude' : amplitude , 'drift_rate' : drift_rate , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def intermittent_signal ( self , length : int , active_ratio : float = 0.3 , signal_freq : float = 5.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an intermittent signal (randomly switches on/off).\"\"\" t = self . _create_time_array ( length ) base_signal = amplitude * np . sin ( 2 * np . pi * signal_freq * t ) # Create random on/off pattern switch_period = int ( self . sampling_rate / 2 ) # Switch every 0.5 seconds n_switches = length // switch_period + 1 switches = np . random . random ( n_switches ) < active_ratio gate = np . repeat ( switches , switch_period )[: length ] signal = base_signal * gate metadata = { 'type' : 'intermittent_signal' , 'active_ratio' : active_ratio , 'signal_freq' : signal_freq , 'amplitude' : amplitude , 'actual_active_ratio' : np . mean ( gate ), 'sampling_rate' : self . sampling_rate } return t , signal , metadata def switched_signal ( self , length : int , switch_period : int = 200 , freq1 : float = 2.0 , freq2 : float = 8.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a signal that switches between two frequencies.\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) for i in range ( 0 , length , switch_period * 2 ): # First period: freq1 end1 = min ( i + switch_period , length ) signal [ i : end1 ] = amplitude * np . sin ( 2 * np . pi * freq1 * t [ i : end1 ]) # Second period: freq2 start2 = end1 end2 = min ( start2 + switch_period , length ) if start2 < length : signal [ start2 : end2 ] = amplitude * np . sin ( 2 * np . pi * freq2 * t [ start2 : end2 ]) metadata = { 'type' : 'switched_signal' , 'switch_period' : switch_period , 'freq1' : freq1 , 'freq2' : freq2 , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata available_signals property Get list of available signal types. signal_info property Get detailed information about all signal types. Returns: dict \u2013 Dictionary mapping signal names to their metadata including description and category. __init__ ( sampling_rate = 100.0 ) Initialize the signal generator. Args: sampling_rate: Default sampling rate in Hz Source code in pyeyesweb/utils/signal_generators.py 28 29 30 31 32 33 34 35 36 def __init__ ( self , sampling_rate : float = 100.0 ): \"\"\" Initialize the signal generator. Args: sampling_rate: Default sampling rate in Hz \"\"\" self . sampling_rate = sampling_rate self . _register_generators () amplitude_modulated ( length , carrier_freq = 10.0 , modulation_freq = 1.0 , modulation_index = 0.5 , ** kwargs ) Generate an amplitude modulated signal. Source code in pyeyesweb/utils/signal_generators.py 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 def amplitude_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , modulation_index : float = 0.5 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an amplitude modulated signal.\"\"\" t = self . _create_time_array ( length ) carrier = np . sin ( 2 * np . pi * carrier_freq * t ) modulator = 1 + modulation_index * np . sin ( 2 * np . pi * modulation_freq * t ) signal = modulator * carrier metadata = { 'type' : 'amplitude_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'modulation_index' : modulation_index , 'sampling_rate' : self . sampling_rate } return t , signal , metadata breathing_pattern ( length , breathing_rate = 12.0 , inhale_ratio = 0.4 , amplitude = 1.0 , ** kwargs ) Generate a breathing-like pattern. Source code in pyeyesweb/utils/signal_generators.py 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 def breathing_pattern ( self , length : int , breathing_rate : float = 12.0 , inhale_ratio : float = 0.4 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a breathing-like pattern.\"\"\" t = self . _create_time_array ( length ) breathing_period = 60.0 / breathing_rate # Period in seconds signal = np . zeros ( length ) for breath_start in np . arange ( 0 , t [ - 1 ], breathing_period ): inhale_end = breath_start + inhale_ratio * breathing_period exhale_end = breath_start + breathing_period # Inhale phase (rising) inhale_mask = ( t >= breath_start ) & ( t < inhale_end ) if np . any ( inhale_mask ): inhale_t = ( t [ inhale_mask ] - breath_start ) / ( inhale_ratio * breathing_period ) signal [ inhale_mask ] = amplitude * ( 1 - np . cos ( np . pi * inhale_t )) / 2 # Exhale phase (falling) exhale_mask = ( t >= inhale_end ) & ( t < exhale_end ) if np . any ( exhale_mask ): exhale_t = ( t [ exhale_mask ] - inhale_end ) / (( 1 - inhale_ratio ) * breathing_period ) signal [ exhale_mask ] = amplitude * ( 1 + np . cos ( np . pi * exhale_t )) / 2 metadata = { 'type' : 'breathing_pattern' , 'breathing_rate' : breathing_rate , 'inhale_ratio' : inhale_ratio , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata brownian_motion ( length , std = 0.1 , seed = None , ** kwargs ) Generate Brownian motion (random walk). Source code in pyeyesweb/utils/signal_generators.py 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 def brownian_motion ( self , length : int , std : float = 0.1 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate Brownian motion (random walk).\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) steps = np . random . normal ( 0 , std , length ) signal = np . cumsum ( steps ) metadata = { 'type' : 'brownian_motion' , 'step_std' : std , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata burst_signal ( length , burst_freq = 2.0 , burst_duration = 0.1 , carrier_freq = 20.0 , amplitude = 1.0 , ** kwargs ) Generate burst signal (periodic bursts of oscillation). Source code in pyeyesweb/utils/signal_generators.py 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 def burst_signal ( self , length : int , burst_freq : float = 2.0 , burst_duration : float = 0.1 , carrier_freq : float = 20.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate burst signal (periodic bursts of oscillation).\"\"\" t = self . _create_time_array ( length ) carrier = amplitude * np . sin ( 2 * np . pi * carrier_freq * t ) # Create burst envelope burst_period = 1.0 / burst_freq envelope = np . zeros ( length ) for burst_start in np . arange ( 0 , t [ - 1 ], burst_period ): burst_mask = ( t >= burst_start ) & ( t < burst_start + burst_duration ) envelope [ burst_mask ] = 1.0 signal = carrier * envelope metadata = { 'type' : 'burst_signal' , 'burst_freq' : burst_freq , 'burst_duration' : burst_duration , 'carrier_freq' : carrier_freq , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata chirp_signal ( length , freq_start = 1.0 , freq_end = 10.0 , amplitude = 1.0 , method = 'linear' , ** kwargs ) Generate a chirp signal (linear frequency sweep). Source code in pyeyesweb/utils/signal_generators.py 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 def chirp_signal ( self , length : int , freq_start : float = 1.0 , freq_end : float = 10.0 , amplitude : float = 1.0 , method : str = 'linear' , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a chirp signal (linear frequency sweep).\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . chirp ( t , freq_start , t [ - 1 ], freq_end , method = method ) metadata = { 'type' : 'chirp' , 'freq_start' : freq_start , 'freq_end' : freq_end , 'method' : method , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata complex_signal ( length , components = None , ** kwargs ) Generate a complex signal with multiple frequency components. Source code in pyeyesweb/utils/signal_generators.py 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 def complex_signal ( self , length : int , components : List [ Dict ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a complex signal with multiple frequency components.\"\"\" if components is None : components = [ { 'freq' : 5 , 'amp' : 1.0 , 'phase' : 0 }, { 'freq' : 10 , 'amp' : 0.5 , 'phase' : np . pi / 4 }, { 'freq' : 20 , 'amp' : 0.3 , 'phase' : np . pi / 2 } ] t = self . _create_time_array ( length ) signal = np . zeros ( length ) for comp in components : signal += comp [ 'amp' ] * np . sin ( 2 * np . pi * comp [ 'freq' ] * t + comp . get ( 'phase' , 0 )) metadata = { 'type' : 'complex' , 'components' : components , 'sampling_rate' : self . sampling_rate } return t , signal , metadata cosine_wave ( length , freq = 1.0 , amplitude = 1.0 , phase = 0.0 , dc_offset = 0.0 , ** kwargs ) Generate a cosine wave. Source code in pyeyesweb/utils/signal_generators.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def cosine_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , phase : float = 0.0 , dc_offset : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a cosine wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * np . cos ( 2 * np . pi * freq * t + phase ) + dc_offset metadata = { 'type' : 'cosine' , 'frequency' : freq , 'amplitude' : amplitude , 'phase' : phase , 'dc_offset' : dc_offset , 'sampling_rate' : self . sampling_rate } return t , signal , metadata damped_sine ( length , freq = 5.0 , amplitude = 1.0 , damping = 0.1 , ** kwargs ) Generate a damped sine wave. Source code in pyeyesweb/utils/signal_generators.py 624 625 626 627 628 629 630 631 632 633 634 635 636 637 def damped_sine ( self , length : int , freq : float = 5.0 , amplitude : float = 1.0 , damping : float = 0.1 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a damped sine wave.\"\"\" t = self . _create_time_array ( length ) envelope = amplitude * np . exp ( - damping * t ) signal = envelope * np . sin ( 2 * np . pi * freq * t ) metadata = { 'type' : 'damped_sine' , 'frequency' : freq , 'amplitude' : amplitude , 'damping' : damping , 'sampling_rate' : self . sampling_rate } return t , signal , metadata ecg_like ( length , heart_rate = 60.0 , amplitude = 1.0 , ** kwargs ) Generate an ECG-like signal. Source code in pyeyesweb/utils/signal_generators.py 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 def ecg_like ( self , length : int , heart_rate : float = 60.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an ECG-like signal.\"\"\" t = self . _create_time_array ( length ) beat_period = 60.0 / heart_rate # Period in seconds n_beats = int ( t [ - 1 ] / beat_period ) signal = np . zeros ( length ) # Simple ECG model with P, QRS, T waves for beat in range ( n_beats ): beat_start = beat * beat_period beat_indices = np . where (( t >= beat_start ) & ( t < beat_start + beat_period ))[ 0 ] if len ( beat_indices ) > 0 : beat_t = t [ beat_indices ] - beat_start # P wave p_wave = 0.2 * amplitude * np . exp ( - (( beat_t - 0.15 ) ** 2 ) / ( 2 * 0.01 )) # QRS complex qrs = amplitude * np . exp ( - (( beat_t - 0.2 ) ** 2 ) / ( 2 * 0.001 )) # T wave t_wave = 0.3 * amplitude * np . exp ( - (( beat_t - 0.4 ) ** 2 ) / ( 2 * 0.02 )) signal [ beat_indices ] = p_wave + qrs + t_wave metadata = { 'type' : 'ecg_like' , 'heart_rate' : heart_rate , 'amplitude' : amplitude , 'n_beats' : n_beats , 'sampling_rate' : self . sampling_rate } return t , signal , metadata emg_like ( length , burst_frequency = 2.0 , burst_duration = 0.2 , amplitude = 1.0 , ** kwargs ) Generate an EMG-like signal with bursts of activity. Source code in pyeyesweb/utils/signal_generators.py 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 def emg_like ( self , length : int , burst_frequency : float = 2.0 , burst_duration : float = 0.2 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an EMG-like signal with bursts of activity.\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) # Generate burst envelope burst_period = 1.0 / burst_frequency for burst_start in np . arange ( 0 , t [ - 1 ], burst_period ): burst_mask = ( t >= burst_start ) & ( t < burst_start + burst_duration ) # High-frequency noise during burst signal [ burst_mask ] = amplitude * np . random . normal ( 0 , 1 , np . sum ( burst_mask )) # Add baseline noise signal += 0.05 * amplitude * np . random . normal ( 0 , 1 , length ) metadata = { 'type' : 'emg_like' , 'burst_frequency' : burst_frequency , 'burst_duration' : burst_duration , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata exponential_chirp ( length , freq_start = 1.0 , freq_end = 10.0 , amplitude = 1.0 , ** kwargs ) Generate an exponential chirp signal. Source code in pyeyesweb/utils/signal_generators.py 422 423 424 425 def exponential_chirp ( self , length : int , freq_start : float = 1.0 , freq_end : float = 10.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an exponential chirp signal.\"\"\" return self . chirp_signal ( length , freq_start , freq_end , amplitude , method = 'exponential' ) exponential_decay ( length , amplitude = 1.0 , decay_rate = 1.0 , ** kwargs ) Generate an exponential decay signal. Source code in pyeyesweb/utils/signal_generators.py 610 611 612 613 614 615 616 617 618 619 620 621 622 def exponential_decay ( self , length : int , amplitude : float = 1.0 , decay_rate : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an exponential decay signal.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * np . exp ( - decay_rate * t ) metadata = { 'type' : 'exponential_decay' , 'amplitude' : amplitude , 'decay_rate' : decay_rate , 'time_constant' : 1 / decay_rate , 'sampling_rate' : self . sampling_rate } return t , signal , metadata fractal_noise ( length , hurst = 0.5 , amplitude = 1.0 , ** kwargs ) Generate fractal noise using fractional Brownian motion. Source code in pyeyesweb/utils/signal_generators.py 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 def fractal_noise ( self , length : int , hurst : float = 0.5 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate fractal noise using fractional Brownian motion.\"\"\" from scipy.fft import fft , ifft , fftfreq # Generate frequencies freqs = fftfreq ( length , 1 / self . sampling_rate ) freqs [ 0 ] = 1 # Avoid division by zero # Generate random phases phases = np . random . uniform ( 0 , 2 * np . pi , length ) # Create power spectrum with 1/f^(2H+1) scaling power = np . abs ( freqs ) ** - ( 2 * hurst + 1 ) power [ 0 ] = 0 # Remove DC component # Generate signal in frequency domain fft_signal = np . sqrt ( power ) * np . exp ( 1 j * phases ) # Transform to time domain signal = np . real ( ifft ( fft_signal )) signal = amplitude * signal / np . std ( signal ) t = self . _create_time_array ( length ) metadata = { 'type' : 'fractal_noise' , 'hurst' : hurst , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata frequency_modulated ( length , carrier_freq = 10.0 , modulation_freq = 1.0 , modulation_index = 5.0 , ** kwargs ) Generate a frequency modulated signal. Source code in pyeyesweb/utils/signal_generators.py 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 def frequency_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , modulation_index : float = 5.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a frequency modulated signal.\"\"\" t = self . _create_time_array ( length ) modulator = modulation_index * np . sin ( 2 * np . pi * modulation_freq * t ) phase = 2 * np . pi * carrier_freq * t + modulator signal = np . sin ( phase ) metadata = { 'type' : 'frequency_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'modulation_index' : modulation_index , 'sampling_rate' : self . sampling_rate } return t , signal , metadata gait_pattern ( length , step_frequency = 2.0 , amplitude = 1.0 , ** kwargs ) Generate a gait-like pattern. Source code in pyeyesweb/utils/signal_generators.py 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 def gait_pattern ( self , length : int , step_frequency : float = 2.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a gait-like pattern.\"\"\" t = self . _create_time_array ( length ) # Double bump pattern for heel strike and toe-off signal = amplitude * ( np . sin ( 2 * np . pi * step_frequency * t ) + 0.3 * np . sin ( 4 * np . pi * step_frequency * t )) # Add some variability signal += 0.05 * amplitude * np . random . normal ( 0 , 1 , length ) metadata = { 'type' : 'gait_pattern' , 'step_frequency' : step_frequency , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata gaussian_noise ( length , mean = 0.0 , std = 1.0 , seed = None , ** kwargs ) Generate Gaussian (white) noise. Source code in pyeyesweb/utils/signal_generators.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def gaussian_noise ( self , length : int , mean : float = 0.0 , std : float = 1.0 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate Gaussian (white) noise.\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) signal = np . random . normal ( mean , std , length ) metadata = { 'type' : 'gaussian' , 'mean' : mean , 'std' : std , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata generate ( signal_type , length = 1000 , ** kwargs ) Generate a signal based on type and parameters. Args: signal_type: Type of signal to generate length: Number of samples **kwargs: Additional parameters specific to each signal type Returns: Tuple of (time_array, signal_array, metadata_dict) Source code in pyeyesweb/utils/signal_generators.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def generate ( self , signal_type : str , length : int = 1000 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict [ str , Any ]]: \"\"\" Generate a signal based on type and parameters. Args: signal_type: Type of signal to generate length: Number of samples **kwargs: Additional parameters specific to each signal type Returns: Tuple of (time_array, signal_array, metadata_dict) \"\"\" if signal_type not in self . _generators : available = ', ' . join ( sorted ( self . _generators . keys ())) raise ValueError ( f \"Unknown signal type: ' { signal_type } '. Available types: { available } \" ) # Override sampling rate if provided if 'sampling_rate' in kwargs : self . sampling_rate = kwargs [ 'sampling_rate' ] return self . _generators [ signal_type ]( length , ** kwargs ) get_signals_by_category () Get signals organized by category. Returns: dict \u2013 Dictionary mapping category names to lists of signal types. Source code in pyeyesweb/utils/signal_generators.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def get_signals_by_category ( self ) -> Dict [ str , List [ str ]]: \"\"\"Get signals organized by category. Returns ------- dict Dictionary mapping category names to lists of signal types. \"\"\" categories = {} for signal_name , info in self . signal_info . items (): category = info [ 'category' ] if category not in categories : categories [ category ] = [] categories [ category ] . append ( signal_name ) return categories hyperbolic_chirp ( length , freq_start = 1.0 , freq_end = 10.0 , amplitude = 1.0 , ** kwargs ) Generate a hyperbolic chirp signal. Source code in pyeyesweb/utils/signal_generators.py 427 428 429 430 def hyperbolic_chirp ( self , length : int , freq_start : float = 1.0 , freq_end : float = 10.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a hyperbolic chirp signal.\"\"\" return self . chirp_signal ( length , freq_start , freq_end , amplitude , method = 'hyperbolic' ) impulse_train ( length , period = 100 , amplitude = 1.0 , jitter = 0.0 , ** kwargs ) Generate an impulse train with optional jitter. Source code in pyeyesweb/utils/signal_generators.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 def impulse_train ( self , length : int , period : int = 100 , amplitude : float = 1.0 , jitter : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an impulse train with optional jitter.\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) if jitter > 0 : positions = np . arange ( 0 , length , period ) positions += np . random . uniform ( - jitter * period , jitter * period , len ( positions )) positions = np . clip ( positions . astype ( int ), 0 , length - 1 ) signal [ positions ] = amplitude else : signal [:: period ] = amplitude metadata = { 'type' : 'impulse_train' , 'period' : period , 'amplitude' : amplitude , 'jitter' : jitter , 'num_impulses' : np . sum ( signal != 0 ), 'sampling_rate' : self . sampling_rate } return t , signal , metadata intermittent_signal ( length , active_ratio = 0.3 , signal_freq = 5.0 , amplitude = 1.0 , ** kwargs ) Generate an intermittent signal (randomly switches on/off). Source code in pyeyesweb/utils/signal_generators.py 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 def intermittent_signal ( self , length : int , active_ratio : float = 0.3 , signal_freq : float = 5.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an intermittent signal (randomly switches on/off).\"\"\" t = self . _create_time_array ( length ) base_signal = amplitude * np . sin ( 2 * np . pi * signal_freq * t ) # Create random on/off pattern switch_period = int ( self . sampling_rate / 2 ) # Switch every 0.5 seconds n_switches = length // switch_period + 1 switches = np . random . random ( n_switches ) < active_ratio gate = np . repeat ( switches , switch_period )[: length ] signal = base_signal * gate metadata = { 'type' : 'intermittent_signal' , 'active_ratio' : active_ratio , 'signal_freq' : signal_freq , 'amplitude' : amplitude , 'actual_active_ratio' : np . mean ( gate ), 'sampling_rate' : self . sampling_rate } return t , signal , metadata logistic_map ( length , r = 3.8 , x0 = 0.5 , ** kwargs ) Generate signal from logistic map (chaotic for r > 3.57). Source code in pyeyesweb/utils/signal_generators.py 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 def logistic_map ( self , length : int , r : float = 3.8 , x0 : float = 0.5 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate signal from logistic map (chaotic for r > 3.57).\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) x = x0 for i in range ( length ): x = r * x * ( 1 - x ) signal [ i ] = x metadata = { 'type' : 'logistic_map' , 'r' : r , 'x0' : x0 , 'chaotic' : r > 3.57 , 'sampling_rate' : self . sampling_rate } return t , signal , metadata lorenz_attractor ( length , sigma = 10.0 , rho = 28.0 , beta = 8 / 3 , ** kwargs ) Generate signal from Lorenz attractor (chaotic system). Source code in pyeyesweb/utils/signal_generators.py 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 def lorenz_attractor ( self , length : int , sigma : float = 10.0 , rho : float = 28.0 , beta : float = 8 / 3 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate signal from Lorenz attractor (chaotic system).\"\"\" dt = 0.01 t = np . arange ( 0 , length * dt , dt )[: length ] # Initialize x , y , z = 1.0 , 1.0 , 1.0 signal = np . zeros ( length ) # Integrate Lorenz equations for i in range ( length ): dx = sigma * ( y - x ) * dt dy = ( x * ( rho - z ) - y ) * dt dz = ( x * y - beta * z ) * dt x += dx y += dy z += dz signal [ i ] = x # Use x coordinate as signal # Normalize signal = ( signal - np . mean ( signal )) / np . std ( signal ) metadata = { 'type' : 'lorenz_attractor' , 'sigma' : sigma , 'rho' : rho , 'beta' : beta , 'sampling_rate' : self . sampling_rate } return t * self . sampling_rate , signal , metadata multi_sine ( length , frequencies = None , amplitudes = None , phases = None , ** kwargs ) Generate multiple sine waves combined. Source code in pyeyesweb/utils/signal_generators.py 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 def multi_sine ( self , length : int , frequencies : List [ float ] = None , amplitudes : List [ float ] = None , phases : List [ float ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate multiple sine waves combined.\"\"\" if frequencies is None : frequencies = [ 1.0 , 3.0 , 5.0 ] if amplitudes is None : amplitudes = [ 1.0 ] * len ( frequencies ) if phases is None : phases = [ 0.0 ] * len ( frequencies ) t = self . _create_time_array ( length ) signal = np . zeros ( length ) for freq , amp , phase in zip ( frequencies , amplitudes , phases ): signal += amp * np . sin ( 2 * np . pi * freq * t + phase ) metadata = { 'type' : 'multi_sine' , 'frequencies' : frequencies , 'amplitudes' : amplitudes , 'phases' : phases , 'sampling_rate' : self . sampling_rate } return t , signal , metadata noisy_sine ( length , freq = 1.0 , amplitude = 1.0 , noise_level = 0.1 , ** kwargs ) Generate a sine wave with added noise. Source code in pyeyesweb/utils/signal_generators.py 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 def noisy_sine ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , noise_level : float = 0.1 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sine wave with added noise.\"\"\" t , clean_signal , _ = self . sine_wave ( length , freq , amplitude ) noise = np . random . normal ( 0 , noise_level , length ) signal = clean_signal + noise metadata = { 'type' : 'noisy_sine' , 'frequency' : freq , 'amplitude' : amplitude , 'noise_level' : noise_level , 'snr' : 20 * np . log10 ( amplitude / noise_level ) if noise_level > 0 else np . inf , 'sampling_rate' : self . sampling_rate } return t , signal , metadata phase_modulated ( length , carrier_freq = 10.0 , modulation_freq = 1.0 , modulation_index = np . pi , ** kwargs ) Generate a phase modulated signal. Source code in pyeyesweb/utils/signal_generators.py 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 def phase_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , modulation_index : float = np . pi , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a phase modulated signal.\"\"\" t = self . _create_time_array ( length ) phase_modulation = modulation_index * np . sin ( 2 * np . pi * modulation_freq * t ) signal = np . sin ( 2 * np . pi * carrier_freq * t + phase_modulation ) metadata = { 'type' : 'phase_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'modulation_index' : modulation_index , 'sampling_rate' : self . sampling_rate } return t , signal , metadata pink_noise ( length , amplitude = 1.0 , seed = None , ** kwargs ) Generate pink (1/f) noise. Source code in pyeyesweb/utils/signal_generators.py 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 def pink_noise ( self , length : int , amplitude : float = 1.0 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate pink (1/f) noise.\"\"\" if seed is not None : np . random . seed ( seed ) # Generate white noise white = np . random . randn ( length ) # Apply 1/f filter in frequency domain fft = np . fft . rfft ( white ) freqs = np . fft . rfftfreq ( length ) freqs [ 0 ] = 1 # Avoid division by zero fft = fft / np . sqrt ( freqs ) signal = np . fft . irfft ( fft , length ) # Normalize signal = amplitude * signal / np . std ( signal ) t = self . _create_time_array ( length ) metadata = { 'type' : 'pink_noise' , 'amplitude' : amplitude , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata pulse_width_modulated ( length , carrier_freq = 10.0 , modulation_freq = 1.0 , ** kwargs ) Generate a pulse width modulated signal. Source code in pyeyesweb/utils/signal_generators.py 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 def pulse_width_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a pulse width modulated signal.\"\"\" t = self . _create_time_array ( length ) duty_cycle = 0.5 + 0.4 * np . sin ( 2 * np . pi * modulation_freq * t ) signal = np . zeros ( length ) # Generate PWM signal phase = ( carrier_freq * t ) % 1 for i in range ( length ): signal [ i ] = 1 if phase [ i ] < duty_cycle [ i ] else - 1 metadata = { 'type' : 'pulse_width_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'sampling_rate' : self . sampling_rate } return t , signal , metadata ramp_function ( length , slope = 1.0 , start_value = 0.0 , ** kwargs ) Generate a ramp function. Source code in pyeyesweb/utils/signal_generators.py 597 598 599 600 601 602 603 604 605 606 607 608 def ramp_function ( self , length : int , slope : float = 1.0 , start_value : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a ramp function.\"\"\" t = self . _create_time_array ( length ) signal = start_value + slope * t metadata = { 'type' : 'ramp' , 'slope' : slope , 'start_value' : start_value , 'sampling_rate' : self . sampling_rate } return t , signal , metadata random_signal ( length , seed = None , min_val =- 1.0 , max_val = 1.0 , ** kwargs ) Generate random uniform noise. Source code in pyeyesweb/utils/signal_generators.py 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def random_signal ( self , length : int , seed : Optional [ int ] = None , min_val : float = - 1.0 , max_val : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate random uniform noise.\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) signal = np . random . uniform ( min_val , max_val , length ) metadata = { 'type' : 'random' , 'distribution' : 'uniform' , 'range' : [ min_val , max_val ], 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata sawtooth_wave ( length , freq = 1.0 , amplitude = 1.0 , width = 1.0 , ** kwargs ) Generate a sawtooth wave. Source code in pyeyesweb/utils/signal_generators.py 280 281 282 283 284 285 286 287 288 289 290 291 292 def sawtooth_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , width : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sawtooth wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . sawtooth ( 2 * np . pi * freq * t , width = width ) metadata = { 'type' : 'sawtooth' , 'frequency' : freq , 'amplitude' : amplitude , 'width' : width , 'sampling_rate' : self . sampling_rate } return t , signal , metadata sine_wave ( length , freq = 1.0 , amplitude = 1.0 , phase = 0.0 , dc_offset = 0.0 , ** kwargs ) Generate a sine wave. Source code in pyeyesweb/utils/signal_generators.py 237 238 239 240 241 242 243 244 245 246 247 248 249 def sine_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , phase : float = 0.0 , dc_offset : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sine wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * np . sin ( 2 * np . pi * freq * t + phase ) + dc_offset metadata = self . _create_metadata ( 'sine' , frequency = freq , amplitude = amplitude , phase = phase , dc_offset = dc_offset ) return t , signal , metadata sine_with_drift ( length , freq = 1.0 , amplitude = 1.0 , drift_rate = 0.01 , ** kwargs ) Generate a sine wave with linear drift. Source code in pyeyesweb/utils/signal_generators.py 931 932 933 934 935 936 937 938 939 940 941 942 943 944 def sine_with_drift ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , drift_rate : float = 0.01 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sine wave with linear drift.\"\"\" t , sine_signal , _ = self . sine_wave ( length , freq , amplitude ) drift = drift_rate * t signal = sine_signal + drift metadata = { 'type' : 'sine_with_drift' , 'frequency' : freq , 'amplitude' : amplitude , 'drift_rate' : drift_rate , 'sampling_rate' : self . sampling_rate } return t , signal , metadata spike_train ( length , spike_rate = 10.0 , refractory_period = 0.002 , amplitude = 1.0 , ** kwargs ) Generate a neural spike train. Source code in pyeyesweb/utils/signal_generators.py 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 def spike_train ( self , length : int , spike_rate : float = 10.0 , refractory_period : float = 0.002 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a neural spike train.\"\"\" t = self . _create_time_array ( length ) dt = 1.0 / self . sampling_rate signal = np . zeros ( length ) # Generate spikes with refractory period last_spike_time = - refractory_period spike_times = [] for i , time in enumerate ( t ): if time - last_spike_time > refractory_period : if np . random . random () < spike_rate * dt : signal [ i ] = amplitude last_spike_time = time spike_times . append ( time ) metadata = { 'type' : 'spike_train' , 'spike_rate' : spike_rate , 'refractory_period' : refractory_period , 'amplitude' : amplitude , 'num_spikes' : len ( spike_times ), 'actual_rate' : len ( spike_times ) / ( t [ - 1 ] - t [ 0 ]) if t [ - 1 ] > t [ 0 ] else 0 , 'sampling_rate' : self . sampling_rate } return t , signal , metadata square_wave ( length , freq = 1.0 , amplitude = 1.0 , duty_cycle = 0.5 , ** kwargs ) Generate a square wave. Source code in pyeyesweb/utils/signal_generators.py 266 267 268 269 270 271 272 273 274 275 276 277 278 def square_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , duty_cycle : float = 0.5 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a square wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . square ( 2 * np . pi * freq * t , duty = duty_cycle ) metadata = { 'type' : 'square' , 'frequency' : freq , 'amplitude' : amplitude , 'duty_cycle' : duty_cycle , 'sampling_rate' : self . sampling_rate } return t , signal , metadata step_function ( length , step_time = 0.5 , amplitude = 1.0 , ** kwargs ) Generate a step function. Source code in pyeyesweb/utils/signal_generators.py 583 584 585 586 587 588 589 590 591 592 593 594 595 def step_function ( self , length : int , step_time : float = 0.5 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a step function.\"\"\" t = np . linspace ( 0 , 1 , length ) signal = np . ones ( length ) * amplitude signal [ t < step_time ] = 0 metadata = { 'type' : 'step' , 'step_time' : step_time , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t * length / self . sampling_rate , signal , metadata switched_signal ( length , switch_period = 200 , freq1 = 2.0 , freq2 = 8.0 , amplitude = 1.0 , ** kwargs ) Generate a signal that switches between two frequencies. Source code in pyeyesweb/utils/signal_generators.py 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 def switched_signal ( self , length : int , switch_period : int = 200 , freq1 : float = 2.0 , freq2 : float = 8.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a signal that switches between two frequencies.\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) for i in range ( 0 , length , switch_period * 2 ): # First period: freq1 end1 = min ( i + switch_period , length ) signal [ i : end1 ] = amplitude * np . sin ( 2 * np . pi * freq1 * t [ i : end1 ]) # Second period: freq2 start2 = end1 end2 = min ( start2 + switch_period , length ) if start2 < length : signal [ start2 : end2 ] = amplitude * np . sin ( 2 * np . pi * freq2 * t [ start2 : end2 ]) metadata = { 'type' : 'switched_signal' , 'switch_period' : switch_period , 'freq1' : freq1 , 'freq2' : freq2 , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata tremor_signal ( length , tremor_freq = 5.0 , base_freq = 0.5 , tremor_amplitude = 0.3 , base_amplitude = 1.0 , ** kwargs ) Generate a tremor-like signal (slow movement with superimposed tremor). Source code in pyeyesweb/utils/signal_generators.py 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 def tremor_signal ( self , length : int , tremor_freq : float = 5.0 , base_freq : float = 0.5 , tremor_amplitude : float = 0.3 , base_amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a tremor-like signal (slow movement with superimposed tremor).\"\"\" t = self . _create_time_array ( length ) base_movement = base_amplitude * np . sin ( 2 * np . pi * base_freq * t ) tremor = tremor_amplitude * np . sin ( 2 * np . pi * tremor_freq * t ) signal = base_movement + tremor metadata = { 'type' : 'tremor' , 'tremor_freq' : tremor_freq , 'base_freq' : base_freq , 'tremor_amplitude' : tremor_amplitude , 'base_amplitude' : base_amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata triangle_wave ( length , freq = 1.0 , amplitude = 1.0 , ** kwargs ) Generate a triangle wave. Source code in pyeyesweb/utils/signal_generators.py 294 295 296 297 298 299 300 301 302 303 304 305 def triangle_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a triangle wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . sawtooth ( 2 * np . pi * freq * t , width = 0.5 ) metadata = { 'type' : 'triangle' , 'frequency' : freq , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata white_noise ( length , power = 1.0 , seed = None , ** kwargs ) Generate white noise with specified power. Source code in pyeyesweb/utils/signal_generators.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def white_noise ( self , length : int , power : float = 1.0 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate white noise with specified power.\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) signal = np . random . normal ( 0 , np . sqrt ( power ), length ) metadata = { 'type' : 'white_noise' , 'power' : power , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata generate_signal ( signal_type , length = 1000 , ** kwargs ) Quick signal generation without instantiating the class. Args: signal_type: Type of signal to generate length: Number of samples **kwargs: Additional parameters Returns: Tuple of (time_array, signal_array, metadata_dict) Source code in pyeyesweb/utils/signal_generators.py 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 def generate_signal ( signal_type : str , length : int = 1000 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\" Quick signal generation without instantiating the class. Args: signal_type: Type of signal to generate length: Number of samples **kwargs: Additional parameters Returns: Tuple of (time_array, signal_array, metadata_dict) \"\"\" generator = SignalGenerator () return generator . generate ( signal_type , length , ** kwargs ) list_available_signals () Get list of all available signal types. Source code in pyeyesweb/utils/signal_generators.py 1017 1018 1019 1020 def list_available_signals () -> List [ str ]: \"\"\"Get list of all available signal types.\"\"\" generator = SignalGenerator () return generator . available_signals","title":"Signal Generators"},{"location":"API/utils/signal_generators/#signal-generators","text":"Signal Generation Utilities for PyEyesWeb Testing This module provides signal generation capabilities for testing PyEyesWeb features. It includes various signal types from simple periodic waves to complex modulated and stochastic signals. Author: PyEyesWeb Development Team","title":"Signal Generators"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator","text":"Signal generator for testing and analysis. This class provides methods to generate various types of signals commonly used in signal processing, neuroscience, and movement analysis. Source code in pyeyesweb/utils/signal_generators.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 class SignalGenerator : \"\"\" Signal generator for testing and analysis. This class provides methods to generate various types of signals commonly used in signal processing, neuroscience, and movement analysis. \"\"\" # Registry of available signal generators _generators : Dict [ str , Callable ] = {} def __init__ ( self , sampling_rate : float = 100.0 ): \"\"\" Initialize the signal generator. Args: sampling_rate: Default sampling rate in Hz \"\"\" self . sampling_rate = sampling_rate self . _register_generators () def _create_time_array ( self , length : int ) -> np . ndarray : \"\"\"Create time array for signal generation. Args: length: Number of samples Returns: Time array from 0 to duration \"\"\" return np . linspace ( 0 , length / self . sampling_rate , length ) def _create_metadata ( self , signal_type : str , ** params ) -> Dict [ str , Any ]: \"\"\"Create metadata dictionary with common fields. Args: signal_type: Type of signal **params: Additional parameters to include Returns: Metadata dictionary with type, sampling_rate, and additional params \"\"\" metadata = { 'type' : signal_type , 'sampling_rate' : self . sampling_rate } metadata . update ( params ) return metadata def _register_generators ( self ): \"\"\"Register all available signal generators.\"\"\" self . _generators = { # Basic waveforms 'sine' : self . sine_wave , 'cosine' : self . cosine_wave , 'square' : self . square_wave , 'sawtooth' : self . sawtooth_wave , 'triangle' : self . triangle_wave , # Noise signals 'random' : self . random_signal , 'gaussian' : self . gaussian_noise , 'pink' : self . pink_noise , 'brown' : self . brownian_motion , 'white' : self . white_noise , # Complex signals 'chirp' : self . chirp_signal , 'chirp_exp' : self . exponential_chirp , 'chirp_hyperbolic' : self . hyperbolic_chirp , 'multisine' : self . multi_sine , 'complex' : self . complex_signal , # Modulated signals 'am' : self . amplitude_modulated , 'fm' : self . frequency_modulated , 'pm' : self . phase_modulated , 'pwm' : self . pulse_width_modulated , # Transient signals 'impulse' : self . impulse_train , 'step' : self . step_function , 'ramp' : self . ramp_function , 'exponential' : self . exponential_decay , 'damped_sine' : self . damped_sine , # Biological/Movement signals 'ecg' : self . ecg_like , 'emg' : self . emg_like , 'tremor' : self . tremor_signal , 'gait' : self . gait_pattern , 'breathing' : self . breathing_pattern , # Special signals 'lorenz' : self . lorenz_attractor , 'chaos' : self . logistic_map , 'fractal' : self . fractal_noise , 'burst' : self . burst_signal , 'spike_train' : self . spike_train , # Combined signals 'noisy_sine' : self . noisy_sine , 'drift_sine' : self . sine_with_drift , 'intermittent' : self . intermittent_signal , 'switched' : self . switched_signal , } def generate ( self , signal_type : str , length : int = 1000 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict [ str , Any ]]: \"\"\" Generate a signal based on type and parameters. Args: signal_type: Type of signal to generate length: Number of samples **kwargs: Additional parameters specific to each signal type Returns: Tuple of (time_array, signal_array, metadata_dict) \"\"\" if signal_type not in self . _generators : available = ', ' . join ( sorted ( self . _generators . keys ())) raise ValueError ( f \"Unknown signal type: ' { signal_type } '. Available types: { available } \" ) # Override sampling rate if provided if 'sampling_rate' in kwargs : self . sampling_rate = kwargs [ 'sampling_rate' ] return self . _generators [ signal_type ]( length , ** kwargs ) @property def available_signals ( self ) -> List [ str ]: \"\"\"Get list of available signal types.\"\"\" return sorted ( list ( self . _generators . keys ())) @property def signal_info ( self ) -> Dict [ str , Dict [ str , str ]]: \"\"\"Get detailed information about all signal types. Returns ------- dict Dictionary mapping signal names to their metadata including description and category. \"\"\" return { # Basic Waveforms 'sine' : { 'description' : 'Sine wave' , 'category' : 'Basic Waveforms' }, 'cosine' : { 'description' : 'Cosine wave' , 'category' : 'Basic Waveforms' }, 'square' : { 'description' : 'Square wave' , 'category' : 'Basic Waveforms' }, 'sawtooth' : { 'description' : 'Sawtooth wave' , 'category' : 'Basic Waveforms' }, 'triangle' : { 'description' : 'Triangle wave' , 'category' : 'Basic Waveforms' }, # Noise Signals 'random' : { 'description' : 'Random uniform noise' , 'category' : 'Noise Signals' }, 'gaussian' : { 'description' : 'Gaussian noise' , 'category' : 'Noise Signals' }, 'white' : { 'description' : 'White noise' , 'category' : 'Noise Signals' }, 'pink' : { 'description' : 'Pink (1/f) noise' , 'category' : 'Noise Signals' }, 'brown' : { 'description' : 'Brownian motion (random walk)' , 'category' : 'Noise Signals' }, # Chirp Signals 'chirp' : { 'description' : 'Linear chirp signal' , 'category' : 'Chirp Signals' }, 'chirp_exp' : { 'description' : 'Exponential chirp signal' , 'category' : 'Chirp Signals' }, 'chirp_hyperbolic' : { 'description' : 'Hyperbolic chirp signal' , 'category' : 'Chirp Signals' }, # Modulated 'am' : { 'description' : 'Amplitude modulated signal' , 'category' : 'Modulated' }, 'fm' : { 'description' : 'Frequency modulated signal' , 'category' : 'Modulated' }, 'pm' : { 'description' : 'Phase modulated signal' , 'category' : 'Modulated' }, 'pwm' : { 'description' : 'Pulse width modulated signal' , 'category' : 'Modulated' }, # Transient 'impulse' : { 'description' : 'Impulse train' , 'category' : 'Transient' }, 'step' : { 'description' : 'Step function' , 'category' : 'Transient' }, 'ramp' : { 'description' : 'Ramp function' , 'category' : 'Transient' }, 'exponential' : { 'description' : 'Exponential decay' , 'category' : 'Transient' }, 'damped_sine' : { 'description' : 'Damped sine wave' , 'category' : 'Transient' }, # Biological 'ecg' : { 'description' : 'ECG-like biological signal' , 'category' : 'Biological' }, 'emg' : { 'description' : 'EMG-like muscle activity signal' , 'category' : 'Biological' }, 'tremor' : { 'description' : 'Tremor signal (slow + fast oscillation)' , 'category' : 'Biological' }, 'gait' : { 'description' : 'Gait pattern signal' , 'category' : 'Biological' }, 'breathing' : { 'description' : 'Breathing pattern signal' , 'category' : 'Biological' }, # Complex 'complex' : { 'description' : 'Complex multi-frequency signal' , 'category' : 'Complex' }, 'multisine' : { 'description' : 'Multiple sine waves combined' , 'category' : 'Complex' }, 'noisy_sine' : { 'description' : 'Sine wave with noise' , 'category' : 'Complex' }, 'drift_sine' : { 'description' : 'Sine wave with linear drift' , 'category' : 'Complex' }, 'intermittent' : { 'description' : 'Intermittent signal' , 'category' : 'Complex' }, 'switched' : { 'description' : 'Frequency-switching signal' , 'category' : 'Complex' }, # Chaotic 'lorenz' : { 'description' : 'Lorenz attractor (chaotic)' , 'category' : 'Chaotic' }, 'chaos' : { 'description' : 'Logistic map (chaotic)' , 'category' : 'Chaotic' }, 'fractal' : { 'description' : 'Fractal noise (fBm)' , 'category' : 'Chaotic' }, 'burst' : { 'description' : 'Burst signal' , 'category' : 'Chaotic' }, 'spike_train' : { 'description' : 'Neural spike train' , 'category' : 'Chaotic' }, } def get_signals_by_category ( self ) -> Dict [ str , List [ str ]]: \"\"\"Get signals organized by category. Returns ------- dict Dictionary mapping category names to lists of signal types. \"\"\" categories = {} for signal_name , info in self . signal_info . items (): category = info [ 'category' ] if category not in categories : categories [ category ] = [] categories [ category ] . append ( signal_name ) return categories # ======================================================================== # Basic Waveforms # ======================================================================== def sine_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , phase : float = 0.0 , dc_offset : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sine wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * np . sin ( 2 * np . pi * freq * t + phase ) + dc_offset metadata = self . _create_metadata ( 'sine' , frequency = freq , amplitude = amplitude , phase = phase , dc_offset = dc_offset ) return t , signal , metadata def cosine_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , phase : float = 0.0 , dc_offset : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a cosine wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * np . cos ( 2 * np . pi * freq * t + phase ) + dc_offset metadata = { 'type' : 'cosine' , 'frequency' : freq , 'amplitude' : amplitude , 'phase' : phase , 'dc_offset' : dc_offset , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def square_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , duty_cycle : float = 0.5 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a square wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . square ( 2 * np . pi * freq * t , duty = duty_cycle ) metadata = { 'type' : 'square' , 'frequency' : freq , 'amplitude' : amplitude , 'duty_cycle' : duty_cycle , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def sawtooth_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , width : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sawtooth wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . sawtooth ( 2 * np . pi * freq * t , width = width ) metadata = { 'type' : 'sawtooth' , 'frequency' : freq , 'amplitude' : amplitude , 'width' : width , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def triangle_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a triangle wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . sawtooth ( 2 * np . pi * freq * t , width = 0.5 ) metadata = { 'type' : 'triangle' , 'frequency' : freq , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Noise Signals # ======================================================================== def random_signal ( self , length : int , seed : Optional [ int ] = None , min_val : float = - 1.0 , max_val : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate random uniform noise.\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) signal = np . random . uniform ( min_val , max_val , length ) metadata = { 'type' : 'random' , 'distribution' : 'uniform' , 'range' : [ min_val , max_val ], 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def gaussian_noise ( self , length : int , mean : float = 0.0 , std : float = 1.0 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate Gaussian (white) noise.\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) signal = np . random . normal ( mean , std , length ) metadata = { 'type' : 'gaussian' , 'mean' : mean , 'std' : std , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def white_noise ( self , length : int , power : float = 1.0 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate white noise with specified power.\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) signal = np . random . normal ( 0 , np . sqrt ( power ), length ) metadata = { 'type' : 'white_noise' , 'power' : power , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def pink_noise ( self , length : int , amplitude : float = 1.0 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate pink (1/f) noise.\"\"\" if seed is not None : np . random . seed ( seed ) # Generate white noise white = np . random . randn ( length ) # Apply 1/f filter in frequency domain fft = np . fft . rfft ( white ) freqs = np . fft . rfftfreq ( length ) freqs [ 0 ] = 1 # Avoid division by zero fft = fft / np . sqrt ( freqs ) signal = np . fft . irfft ( fft , length ) # Normalize signal = amplitude * signal / np . std ( signal ) t = self . _create_time_array ( length ) metadata = { 'type' : 'pink_noise' , 'amplitude' : amplitude , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def brownian_motion ( self , length : int , std : float = 0.1 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate Brownian motion (random walk).\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) steps = np . random . normal ( 0 , std , length ) signal = np . cumsum ( steps ) metadata = { 'type' : 'brownian_motion' , 'step_std' : std , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Complex Signals # ======================================================================== def chirp_signal ( self , length : int , freq_start : float = 1.0 , freq_end : float = 10.0 , amplitude : float = 1.0 , method : str = 'linear' , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a chirp signal (linear frequency sweep).\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . chirp ( t , freq_start , t [ - 1 ], freq_end , method = method ) metadata = { 'type' : 'chirp' , 'freq_start' : freq_start , 'freq_end' : freq_end , 'method' : method , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def exponential_chirp ( self , length : int , freq_start : float = 1.0 , freq_end : float = 10.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an exponential chirp signal.\"\"\" return self . chirp_signal ( length , freq_start , freq_end , amplitude , method = 'exponential' ) def hyperbolic_chirp ( self , length : int , freq_start : float = 1.0 , freq_end : float = 10.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a hyperbolic chirp signal.\"\"\" return self . chirp_signal ( length , freq_start , freq_end , amplitude , method = 'hyperbolic' ) def multi_sine ( self , length : int , frequencies : List [ float ] = None , amplitudes : List [ float ] = None , phases : List [ float ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate multiple sine waves combined.\"\"\" if frequencies is None : frequencies = [ 1.0 , 3.0 , 5.0 ] if amplitudes is None : amplitudes = [ 1.0 ] * len ( frequencies ) if phases is None : phases = [ 0.0 ] * len ( frequencies ) t = self . _create_time_array ( length ) signal = np . zeros ( length ) for freq , amp , phase in zip ( frequencies , amplitudes , phases ): signal += amp * np . sin ( 2 * np . pi * freq * t + phase ) metadata = { 'type' : 'multi_sine' , 'frequencies' : frequencies , 'amplitudes' : amplitudes , 'phases' : phases , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def complex_signal ( self , length : int , components : List [ Dict ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a complex signal with multiple frequency components.\"\"\" if components is None : components = [ { 'freq' : 5 , 'amp' : 1.0 , 'phase' : 0 }, { 'freq' : 10 , 'amp' : 0.5 , 'phase' : np . pi / 4 }, { 'freq' : 20 , 'amp' : 0.3 , 'phase' : np . pi / 2 } ] t = self . _create_time_array ( length ) signal = np . zeros ( length ) for comp in components : signal += comp [ 'amp' ] * np . sin ( 2 * np . pi * comp [ 'freq' ] * t + comp . get ( 'phase' , 0 )) metadata = { 'type' : 'complex' , 'components' : components , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Modulated Signals # ======================================================================== def amplitude_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , modulation_index : float = 0.5 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an amplitude modulated signal.\"\"\" t = self . _create_time_array ( length ) carrier = np . sin ( 2 * np . pi * carrier_freq * t ) modulator = 1 + modulation_index * np . sin ( 2 * np . pi * modulation_freq * t ) signal = modulator * carrier metadata = { 'type' : 'amplitude_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'modulation_index' : modulation_index , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def frequency_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , modulation_index : float = 5.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a frequency modulated signal.\"\"\" t = self . _create_time_array ( length ) modulator = modulation_index * np . sin ( 2 * np . pi * modulation_freq * t ) phase = 2 * np . pi * carrier_freq * t + modulator signal = np . sin ( phase ) metadata = { 'type' : 'frequency_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'modulation_index' : modulation_index , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def phase_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , modulation_index : float = np . pi , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a phase modulated signal.\"\"\" t = self . _create_time_array ( length ) phase_modulation = modulation_index * np . sin ( 2 * np . pi * modulation_freq * t ) signal = np . sin ( 2 * np . pi * carrier_freq * t + phase_modulation ) metadata = { 'type' : 'phase_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'modulation_index' : modulation_index , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def pulse_width_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a pulse width modulated signal.\"\"\" t = self . _create_time_array ( length ) duty_cycle = 0.5 + 0.4 * np . sin ( 2 * np . pi * modulation_freq * t ) signal = np . zeros ( length ) # Generate PWM signal phase = ( carrier_freq * t ) % 1 for i in range ( length ): signal [ i ] = 1 if phase [ i ] < duty_cycle [ i ] else - 1 metadata = { 'type' : 'pulse_width_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Transient Signals # ======================================================================== def impulse_train ( self , length : int , period : int = 100 , amplitude : float = 1.0 , jitter : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an impulse train with optional jitter.\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) if jitter > 0 : positions = np . arange ( 0 , length , period ) positions += np . random . uniform ( - jitter * period , jitter * period , len ( positions )) positions = np . clip ( positions . astype ( int ), 0 , length - 1 ) signal [ positions ] = amplitude else : signal [:: period ] = amplitude metadata = { 'type' : 'impulse_train' , 'period' : period , 'amplitude' : amplitude , 'jitter' : jitter , 'num_impulses' : np . sum ( signal != 0 ), 'sampling_rate' : self . sampling_rate } return t , signal , metadata def step_function ( self , length : int , step_time : float = 0.5 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a step function.\"\"\" t = np . linspace ( 0 , 1 , length ) signal = np . ones ( length ) * amplitude signal [ t < step_time ] = 0 metadata = { 'type' : 'step' , 'step_time' : step_time , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t * length / self . sampling_rate , signal , metadata def ramp_function ( self , length : int , slope : float = 1.0 , start_value : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a ramp function.\"\"\" t = self . _create_time_array ( length ) signal = start_value + slope * t metadata = { 'type' : 'ramp' , 'slope' : slope , 'start_value' : start_value , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def exponential_decay ( self , length : int , amplitude : float = 1.0 , decay_rate : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an exponential decay signal.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * np . exp ( - decay_rate * t ) metadata = { 'type' : 'exponential_decay' , 'amplitude' : amplitude , 'decay_rate' : decay_rate , 'time_constant' : 1 / decay_rate , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def damped_sine ( self , length : int , freq : float = 5.0 , amplitude : float = 1.0 , damping : float = 0.1 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a damped sine wave.\"\"\" t = self . _create_time_array ( length ) envelope = amplitude * np . exp ( - damping * t ) signal = envelope * np . sin ( 2 * np . pi * freq * t ) metadata = { 'type' : 'damped_sine' , 'frequency' : freq , 'amplitude' : amplitude , 'damping' : damping , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Biological/Movement Signals # ======================================================================== def ecg_like ( self , length : int , heart_rate : float = 60.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an ECG-like signal.\"\"\" t = self . _create_time_array ( length ) beat_period = 60.0 / heart_rate # Period in seconds n_beats = int ( t [ - 1 ] / beat_period ) signal = np . zeros ( length ) # Simple ECG model with P, QRS, T waves for beat in range ( n_beats ): beat_start = beat * beat_period beat_indices = np . where (( t >= beat_start ) & ( t < beat_start + beat_period ))[ 0 ] if len ( beat_indices ) > 0 : beat_t = t [ beat_indices ] - beat_start # P wave p_wave = 0.2 * amplitude * np . exp ( - (( beat_t - 0.15 ) ** 2 ) / ( 2 * 0.01 )) # QRS complex qrs = amplitude * np . exp ( - (( beat_t - 0.2 ) ** 2 ) / ( 2 * 0.001 )) # T wave t_wave = 0.3 * amplitude * np . exp ( - (( beat_t - 0.4 ) ** 2 ) / ( 2 * 0.02 )) signal [ beat_indices ] = p_wave + qrs + t_wave metadata = { 'type' : 'ecg_like' , 'heart_rate' : heart_rate , 'amplitude' : amplitude , 'n_beats' : n_beats , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def emg_like ( self , length : int , burst_frequency : float = 2.0 , burst_duration : float = 0.2 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an EMG-like signal with bursts of activity.\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) # Generate burst envelope burst_period = 1.0 / burst_frequency for burst_start in np . arange ( 0 , t [ - 1 ], burst_period ): burst_mask = ( t >= burst_start ) & ( t < burst_start + burst_duration ) # High-frequency noise during burst signal [ burst_mask ] = amplitude * np . random . normal ( 0 , 1 , np . sum ( burst_mask )) # Add baseline noise signal += 0.05 * amplitude * np . random . normal ( 0 , 1 , length ) metadata = { 'type' : 'emg_like' , 'burst_frequency' : burst_frequency , 'burst_duration' : burst_duration , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def tremor_signal ( self , length : int , tremor_freq : float = 5.0 , base_freq : float = 0.5 , tremor_amplitude : float = 0.3 , base_amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a tremor-like signal (slow movement with superimposed tremor).\"\"\" t = self . _create_time_array ( length ) base_movement = base_amplitude * np . sin ( 2 * np . pi * base_freq * t ) tremor = tremor_amplitude * np . sin ( 2 * np . pi * tremor_freq * t ) signal = base_movement + tremor metadata = { 'type' : 'tremor' , 'tremor_freq' : tremor_freq , 'base_freq' : base_freq , 'tremor_amplitude' : tremor_amplitude , 'base_amplitude' : base_amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def gait_pattern ( self , length : int , step_frequency : float = 2.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a gait-like pattern.\"\"\" t = self . _create_time_array ( length ) # Double bump pattern for heel strike and toe-off signal = amplitude * ( np . sin ( 2 * np . pi * step_frequency * t ) + 0.3 * np . sin ( 4 * np . pi * step_frequency * t )) # Add some variability signal += 0.05 * amplitude * np . random . normal ( 0 , 1 , length ) metadata = { 'type' : 'gait_pattern' , 'step_frequency' : step_frequency , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def breathing_pattern ( self , length : int , breathing_rate : float = 12.0 , inhale_ratio : float = 0.4 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a breathing-like pattern.\"\"\" t = self . _create_time_array ( length ) breathing_period = 60.0 / breathing_rate # Period in seconds signal = np . zeros ( length ) for breath_start in np . arange ( 0 , t [ - 1 ], breathing_period ): inhale_end = breath_start + inhale_ratio * breathing_period exhale_end = breath_start + breathing_period # Inhale phase (rising) inhale_mask = ( t >= breath_start ) & ( t < inhale_end ) if np . any ( inhale_mask ): inhale_t = ( t [ inhale_mask ] - breath_start ) / ( inhale_ratio * breathing_period ) signal [ inhale_mask ] = amplitude * ( 1 - np . cos ( np . pi * inhale_t )) / 2 # Exhale phase (falling) exhale_mask = ( t >= inhale_end ) & ( t < exhale_end ) if np . any ( exhale_mask ): exhale_t = ( t [ exhale_mask ] - inhale_end ) / (( 1 - inhale_ratio ) * breathing_period ) signal [ exhale_mask ] = amplitude * ( 1 + np . cos ( np . pi * exhale_t )) / 2 metadata = { 'type' : 'breathing_pattern' , 'breathing_rate' : breathing_rate , 'inhale_ratio' : inhale_ratio , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Special Signals # ======================================================================== def lorenz_attractor ( self , length : int , sigma : float = 10.0 , rho : float = 28.0 , beta : float = 8 / 3 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate signal from Lorenz attractor (chaotic system).\"\"\" dt = 0.01 t = np . arange ( 0 , length * dt , dt )[: length ] # Initialize x , y , z = 1.0 , 1.0 , 1.0 signal = np . zeros ( length ) # Integrate Lorenz equations for i in range ( length ): dx = sigma * ( y - x ) * dt dy = ( x * ( rho - z ) - y ) * dt dz = ( x * y - beta * z ) * dt x += dx y += dy z += dz signal [ i ] = x # Use x coordinate as signal # Normalize signal = ( signal - np . mean ( signal )) / np . std ( signal ) metadata = { 'type' : 'lorenz_attractor' , 'sigma' : sigma , 'rho' : rho , 'beta' : beta , 'sampling_rate' : self . sampling_rate } return t * self . sampling_rate , signal , metadata def logistic_map ( self , length : int , r : float = 3.8 , x0 : float = 0.5 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate signal from logistic map (chaotic for r > 3.57).\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) x = x0 for i in range ( length ): x = r * x * ( 1 - x ) signal [ i ] = x metadata = { 'type' : 'logistic_map' , 'r' : r , 'x0' : x0 , 'chaotic' : r > 3.57 , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def fractal_noise ( self , length : int , hurst : float = 0.5 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate fractal noise using fractional Brownian motion.\"\"\" from scipy.fft import fft , ifft , fftfreq # Generate frequencies freqs = fftfreq ( length , 1 / self . sampling_rate ) freqs [ 0 ] = 1 # Avoid division by zero # Generate random phases phases = np . random . uniform ( 0 , 2 * np . pi , length ) # Create power spectrum with 1/f^(2H+1) scaling power = np . abs ( freqs ) ** - ( 2 * hurst + 1 ) power [ 0 ] = 0 # Remove DC component # Generate signal in frequency domain fft_signal = np . sqrt ( power ) * np . exp ( 1 j * phases ) # Transform to time domain signal = np . real ( ifft ( fft_signal )) signal = amplitude * signal / np . std ( signal ) t = self . _create_time_array ( length ) metadata = { 'type' : 'fractal_noise' , 'hurst' : hurst , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def burst_signal ( self , length : int , burst_freq : float = 2.0 , burst_duration : float = 0.1 , carrier_freq : float = 20.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate burst signal (periodic bursts of oscillation).\"\"\" t = self . _create_time_array ( length ) carrier = amplitude * np . sin ( 2 * np . pi * carrier_freq * t ) # Create burst envelope burst_period = 1.0 / burst_freq envelope = np . zeros ( length ) for burst_start in np . arange ( 0 , t [ - 1 ], burst_period ): burst_mask = ( t >= burst_start ) & ( t < burst_start + burst_duration ) envelope [ burst_mask ] = 1.0 signal = carrier * envelope metadata = { 'type' : 'burst_signal' , 'burst_freq' : burst_freq , 'burst_duration' : burst_duration , 'carrier_freq' : carrier_freq , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def spike_train ( self , length : int , spike_rate : float = 10.0 , refractory_period : float = 0.002 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a neural spike train.\"\"\" t = self . _create_time_array ( length ) dt = 1.0 / self . sampling_rate signal = np . zeros ( length ) # Generate spikes with refractory period last_spike_time = - refractory_period spike_times = [] for i , time in enumerate ( t ): if time - last_spike_time > refractory_period : if np . random . random () < spike_rate * dt : signal [ i ] = amplitude last_spike_time = time spike_times . append ( time ) metadata = { 'type' : 'spike_train' , 'spike_rate' : spike_rate , 'refractory_period' : refractory_period , 'amplitude' : amplitude , 'num_spikes' : len ( spike_times ), 'actual_rate' : len ( spike_times ) / ( t [ - 1 ] - t [ 0 ]) if t [ - 1 ] > t [ 0 ] else 0 , 'sampling_rate' : self . sampling_rate } return t , signal , metadata # ======================================================================== # Combined Signals # ======================================================================== def noisy_sine ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , noise_level : float = 0.1 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sine wave with added noise.\"\"\" t , clean_signal , _ = self . sine_wave ( length , freq , amplitude ) noise = np . random . normal ( 0 , noise_level , length ) signal = clean_signal + noise metadata = { 'type' : 'noisy_sine' , 'frequency' : freq , 'amplitude' : amplitude , 'noise_level' : noise_level , 'snr' : 20 * np . log10 ( amplitude / noise_level ) if noise_level > 0 else np . inf , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def sine_with_drift ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , drift_rate : float = 0.01 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sine wave with linear drift.\"\"\" t , sine_signal , _ = self . sine_wave ( length , freq , amplitude ) drift = drift_rate * t signal = sine_signal + drift metadata = { 'type' : 'sine_with_drift' , 'frequency' : freq , 'amplitude' : amplitude , 'drift_rate' : drift_rate , 'sampling_rate' : self . sampling_rate } return t , signal , metadata def intermittent_signal ( self , length : int , active_ratio : float = 0.3 , signal_freq : float = 5.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an intermittent signal (randomly switches on/off).\"\"\" t = self . _create_time_array ( length ) base_signal = amplitude * np . sin ( 2 * np . pi * signal_freq * t ) # Create random on/off pattern switch_period = int ( self . sampling_rate / 2 ) # Switch every 0.5 seconds n_switches = length // switch_period + 1 switches = np . random . random ( n_switches ) < active_ratio gate = np . repeat ( switches , switch_period )[: length ] signal = base_signal * gate metadata = { 'type' : 'intermittent_signal' , 'active_ratio' : active_ratio , 'signal_freq' : signal_freq , 'amplitude' : amplitude , 'actual_active_ratio' : np . mean ( gate ), 'sampling_rate' : self . sampling_rate } return t , signal , metadata def switched_signal ( self , length : int , switch_period : int = 200 , freq1 : float = 2.0 , freq2 : float = 8.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a signal that switches between two frequencies.\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) for i in range ( 0 , length , switch_period * 2 ): # First period: freq1 end1 = min ( i + switch_period , length ) signal [ i : end1 ] = amplitude * np . sin ( 2 * np . pi * freq1 * t [ i : end1 ]) # Second period: freq2 start2 = end1 end2 = min ( start2 + switch_period , length ) if start2 < length : signal [ start2 : end2 ] = amplitude * np . sin ( 2 * np . pi * freq2 * t [ start2 : end2 ]) metadata = { 'type' : 'switched_signal' , 'switch_period' : switch_period , 'freq1' : freq1 , 'freq2' : freq2 , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"SignalGenerator"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.available_signals","text":"Get list of available signal types.","title":"available_signals"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.signal_info","text":"Get detailed information about all signal types. Returns: dict \u2013 Dictionary mapping signal names to their metadata including description and category.","title":"signal_info"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.__init__","text":"Initialize the signal generator. Args: sampling_rate: Default sampling rate in Hz Source code in pyeyesweb/utils/signal_generators.py 28 29 30 31 32 33 34 35 36 def __init__ ( self , sampling_rate : float = 100.0 ): \"\"\" Initialize the signal generator. Args: sampling_rate: Default sampling rate in Hz \"\"\" self . sampling_rate = sampling_rate self . _register_generators ()","title":"__init__"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.amplitude_modulated","text":"Generate an amplitude modulated signal. Source code in pyeyesweb/utils/signal_generators.py 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 def amplitude_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , modulation_index : float = 0.5 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an amplitude modulated signal.\"\"\" t = self . _create_time_array ( length ) carrier = np . sin ( 2 * np . pi * carrier_freq * t ) modulator = 1 + modulation_index * np . sin ( 2 * np . pi * modulation_freq * t ) signal = modulator * carrier metadata = { 'type' : 'amplitude_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'modulation_index' : modulation_index , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"amplitude_modulated"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.breathing_pattern","text":"Generate a breathing-like pattern. Source code in pyeyesweb/utils/signal_generators.py 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 def breathing_pattern ( self , length : int , breathing_rate : float = 12.0 , inhale_ratio : float = 0.4 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a breathing-like pattern.\"\"\" t = self . _create_time_array ( length ) breathing_period = 60.0 / breathing_rate # Period in seconds signal = np . zeros ( length ) for breath_start in np . arange ( 0 , t [ - 1 ], breathing_period ): inhale_end = breath_start + inhale_ratio * breathing_period exhale_end = breath_start + breathing_period # Inhale phase (rising) inhale_mask = ( t >= breath_start ) & ( t < inhale_end ) if np . any ( inhale_mask ): inhale_t = ( t [ inhale_mask ] - breath_start ) / ( inhale_ratio * breathing_period ) signal [ inhale_mask ] = amplitude * ( 1 - np . cos ( np . pi * inhale_t )) / 2 # Exhale phase (falling) exhale_mask = ( t >= inhale_end ) & ( t < exhale_end ) if np . any ( exhale_mask ): exhale_t = ( t [ exhale_mask ] - inhale_end ) / (( 1 - inhale_ratio ) * breathing_period ) signal [ exhale_mask ] = amplitude * ( 1 + np . cos ( np . pi * exhale_t )) / 2 metadata = { 'type' : 'breathing_pattern' , 'breathing_rate' : breathing_rate , 'inhale_ratio' : inhale_ratio , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"breathing_pattern"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.brownian_motion","text":"Generate Brownian motion (random walk). Source code in pyeyesweb/utils/signal_generators.py 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 def brownian_motion ( self , length : int , std : float = 0.1 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate Brownian motion (random walk).\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) steps = np . random . normal ( 0 , std , length ) signal = np . cumsum ( steps ) metadata = { 'type' : 'brownian_motion' , 'step_std' : std , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"brownian_motion"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.burst_signal","text":"Generate burst signal (periodic bursts of oscillation). Source code in pyeyesweb/utils/signal_generators.py 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 def burst_signal ( self , length : int , burst_freq : float = 2.0 , burst_duration : float = 0.1 , carrier_freq : float = 20.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate burst signal (periodic bursts of oscillation).\"\"\" t = self . _create_time_array ( length ) carrier = amplitude * np . sin ( 2 * np . pi * carrier_freq * t ) # Create burst envelope burst_period = 1.0 / burst_freq envelope = np . zeros ( length ) for burst_start in np . arange ( 0 , t [ - 1 ], burst_period ): burst_mask = ( t >= burst_start ) & ( t < burst_start + burst_duration ) envelope [ burst_mask ] = 1.0 signal = carrier * envelope metadata = { 'type' : 'burst_signal' , 'burst_freq' : burst_freq , 'burst_duration' : burst_duration , 'carrier_freq' : carrier_freq , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"burst_signal"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.chirp_signal","text":"Generate a chirp signal (linear frequency sweep). Source code in pyeyesweb/utils/signal_generators.py 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 def chirp_signal ( self , length : int , freq_start : float = 1.0 , freq_end : float = 10.0 , amplitude : float = 1.0 , method : str = 'linear' , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a chirp signal (linear frequency sweep).\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . chirp ( t , freq_start , t [ - 1 ], freq_end , method = method ) metadata = { 'type' : 'chirp' , 'freq_start' : freq_start , 'freq_end' : freq_end , 'method' : method , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"chirp_signal"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.complex_signal","text":"Generate a complex signal with multiple frequency components. Source code in pyeyesweb/utils/signal_generators.py 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 def complex_signal ( self , length : int , components : List [ Dict ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a complex signal with multiple frequency components.\"\"\" if components is None : components = [ { 'freq' : 5 , 'amp' : 1.0 , 'phase' : 0 }, { 'freq' : 10 , 'amp' : 0.5 , 'phase' : np . pi / 4 }, { 'freq' : 20 , 'amp' : 0.3 , 'phase' : np . pi / 2 } ] t = self . _create_time_array ( length ) signal = np . zeros ( length ) for comp in components : signal += comp [ 'amp' ] * np . sin ( 2 * np . pi * comp [ 'freq' ] * t + comp . get ( 'phase' , 0 )) metadata = { 'type' : 'complex' , 'components' : components , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"complex_signal"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.cosine_wave","text":"Generate a cosine wave. Source code in pyeyesweb/utils/signal_generators.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def cosine_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , phase : float = 0.0 , dc_offset : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a cosine wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * np . cos ( 2 * np . pi * freq * t + phase ) + dc_offset metadata = { 'type' : 'cosine' , 'frequency' : freq , 'amplitude' : amplitude , 'phase' : phase , 'dc_offset' : dc_offset , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"cosine_wave"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.damped_sine","text":"Generate a damped sine wave. Source code in pyeyesweb/utils/signal_generators.py 624 625 626 627 628 629 630 631 632 633 634 635 636 637 def damped_sine ( self , length : int , freq : float = 5.0 , amplitude : float = 1.0 , damping : float = 0.1 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a damped sine wave.\"\"\" t = self . _create_time_array ( length ) envelope = amplitude * np . exp ( - damping * t ) signal = envelope * np . sin ( 2 * np . pi * freq * t ) metadata = { 'type' : 'damped_sine' , 'frequency' : freq , 'amplitude' : amplitude , 'damping' : damping , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"damped_sine"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.ecg_like","text":"Generate an ECG-like signal. Source code in pyeyesweb/utils/signal_generators.py 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 def ecg_like ( self , length : int , heart_rate : float = 60.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an ECG-like signal.\"\"\" t = self . _create_time_array ( length ) beat_period = 60.0 / heart_rate # Period in seconds n_beats = int ( t [ - 1 ] / beat_period ) signal = np . zeros ( length ) # Simple ECG model with P, QRS, T waves for beat in range ( n_beats ): beat_start = beat * beat_period beat_indices = np . where (( t >= beat_start ) & ( t < beat_start + beat_period ))[ 0 ] if len ( beat_indices ) > 0 : beat_t = t [ beat_indices ] - beat_start # P wave p_wave = 0.2 * amplitude * np . exp ( - (( beat_t - 0.15 ) ** 2 ) / ( 2 * 0.01 )) # QRS complex qrs = amplitude * np . exp ( - (( beat_t - 0.2 ) ** 2 ) / ( 2 * 0.001 )) # T wave t_wave = 0.3 * amplitude * np . exp ( - (( beat_t - 0.4 ) ** 2 ) / ( 2 * 0.02 )) signal [ beat_indices ] = p_wave + qrs + t_wave metadata = { 'type' : 'ecg_like' , 'heart_rate' : heart_rate , 'amplitude' : amplitude , 'n_beats' : n_beats , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"ecg_like"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.emg_like","text":"Generate an EMG-like signal with bursts of activity. Source code in pyeyesweb/utils/signal_generators.py 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 def emg_like ( self , length : int , burst_frequency : float = 2.0 , burst_duration : float = 0.2 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an EMG-like signal with bursts of activity.\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) # Generate burst envelope burst_period = 1.0 / burst_frequency for burst_start in np . arange ( 0 , t [ - 1 ], burst_period ): burst_mask = ( t >= burst_start ) & ( t < burst_start + burst_duration ) # High-frequency noise during burst signal [ burst_mask ] = amplitude * np . random . normal ( 0 , 1 , np . sum ( burst_mask )) # Add baseline noise signal += 0.05 * amplitude * np . random . normal ( 0 , 1 , length ) metadata = { 'type' : 'emg_like' , 'burst_frequency' : burst_frequency , 'burst_duration' : burst_duration , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"emg_like"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.exponential_chirp","text":"Generate an exponential chirp signal. Source code in pyeyesweb/utils/signal_generators.py 422 423 424 425 def exponential_chirp ( self , length : int , freq_start : float = 1.0 , freq_end : float = 10.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an exponential chirp signal.\"\"\" return self . chirp_signal ( length , freq_start , freq_end , amplitude , method = 'exponential' )","title":"exponential_chirp"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.exponential_decay","text":"Generate an exponential decay signal. Source code in pyeyesweb/utils/signal_generators.py 610 611 612 613 614 615 616 617 618 619 620 621 622 def exponential_decay ( self , length : int , amplitude : float = 1.0 , decay_rate : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an exponential decay signal.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * np . exp ( - decay_rate * t ) metadata = { 'type' : 'exponential_decay' , 'amplitude' : amplitude , 'decay_rate' : decay_rate , 'time_constant' : 1 / decay_rate , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"exponential_decay"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.fractal_noise","text":"Generate fractal noise using fractional Brownian motion. Source code in pyeyesweb/utils/signal_generators.py 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 def fractal_noise ( self , length : int , hurst : float = 0.5 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate fractal noise using fractional Brownian motion.\"\"\" from scipy.fft import fft , ifft , fftfreq # Generate frequencies freqs = fftfreq ( length , 1 / self . sampling_rate ) freqs [ 0 ] = 1 # Avoid division by zero # Generate random phases phases = np . random . uniform ( 0 , 2 * np . pi , length ) # Create power spectrum with 1/f^(2H+1) scaling power = np . abs ( freqs ) ** - ( 2 * hurst + 1 ) power [ 0 ] = 0 # Remove DC component # Generate signal in frequency domain fft_signal = np . sqrt ( power ) * np . exp ( 1 j * phases ) # Transform to time domain signal = np . real ( ifft ( fft_signal )) signal = amplitude * signal / np . std ( signal ) t = self . _create_time_array ( length ) metadata = { 'type' : 'fractal_noise' , 'hurst' : hurst , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"fractal_noise"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.frequency_modulated","text":"Generate a frequency modulated signal. Source code in pyeyesweb/utils/signal_generators.py 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 def frequency_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , modulation_index : float = 5.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a frequency modulated signal.\"\"\" t = self . _create_time_array ( length ) modulator = modulation_index * np . sin ( 2 * np . pi * modulation_freq * t ) phase = 2 * np . pi * carrier_freq * t + modulator signal = np . sin ( phase ) metadata = { 'type' : 'frequency_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'modulation_index' : modulation_index , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"frequency_modulated"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.gait_pattern","text":"Generate a gait-like pattern. Source code in pyeyesweb/utils/signal_generators.py 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 def gait_pattern ( self , length : int , step_frequency : float = 2.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a gait-like pattern.\"\"\" t = self . _create_time_array ( length ) # Double bump pattern for heel strike and toe-off signal = amplitude * ( np . sin ( 2 * np . pi * step_frequency * t ) + 0.3 * np . sin ( 4 * np . pi * step_frequency * t )) # Add some variability signal += 0.05 * amplitude * np . random . normal ( 0 , 1 , length ) metadata = { 'type' : 'gait_pattern' , 'step_frequency' : step_frequency , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"gait_pattern"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.gaussian_noise","text":"Generate Gaussian (white) noise. Source code in pyeyesweb/utils/signal_generators.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def gaussian_noise ( self , length : int , mean : float = 0.0 , std : float = 1.0 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate Gaussian (white) noise.\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) signal = np . random . normal ( mean , std , length ) metadata = { 'type' : 'gaussian' , 'mean' : mean , 'std' : std , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"gaussian_noise"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.generate","text":"Generate a signal based on type and parameters. Args: signal_type: Type of signal to generate length: Number of samples **kwargs: Additional parameters specific to each signal type Returns: Tuple of (time_array, signal_array, metadata_dict) Source code in pyeyesweb/utils/signal_generators.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def generate ( self , signal_type : str , length : int = 1000 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict [ str , Any ]]: \"\"\" Generate a signal based on type and parameters. Args: signal_type: Type of signal to generate length: Number of samples **kwargs: Additional parameters specific to each signal type Returns: Tuple of (time_array, signal_array, metadata_dict) \"\"\" if signal_type not in self . _generators : available = ', ' . join ( sorted ( self . _generators . keys ())) raise ValueError ( f \"Unknown signal type: ' { signal_type } '. Available types: { available } \" ) # Override sampling rate if provided if 'sampling_rate' in kwargs : self . sampling_rate = kwargs [ 'sampling_rate' ] return self . _generators [ signal_type ]( length , ** kwargs )","title":"generate"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.get_signals_by_category","text":"Get signals organized by category. Returns: dict \u2013 Dictionary mapping category names to lists of signal types. Source code in pyeyesweb/utils/signal_generators.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def get_signals_by_category ( self ) -> Dict [ str , List [ str ]]: \"\"\"Get signals organized by category. Returns ------- dict Dictionary mapping category names to lists of signal types. \"\"\" categories = {} for signal_name , info in self . signal_info . items (): category = info [ 'category' ] if category not in categories : categories [ category ] = [] categories [ category ] . append ( signal_name ) return categories","title":"get_signals_by_category"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.hyperbolic_chirp","text":"Generate a hyperbolic chirp signal. Source code in pyeyesweb/utils/signal_generators.py 427 428 429 430 def hyperbolic_chirp ( self , length : int , freq_start : float = 1.0 , freq_end : float = 10.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a hyperbolic chirp signal.\"\"\" return self . chirp_signal ( length , freq_start , freq_end , amplitude , method = 'hyperbolic' )","title":"hyperbolic_chirp"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.impulse_train","text":"Generate an impulse train with optional jitter. Source code in pyeyesweb/utils/signal_generators.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 def impulse_train ( self , length : int , period : int = 100 , amplitude : float = 1.0 , jitter : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an impulse train with optional jitter.\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) if jitter > 0 : positions = np . arange ( 0 , length , period ) positions += np . random . uniform ( - jitter * period , jitter * period , len ( positions )) positions = np . clip ( positions . astype ( int ), 0 , length - 1 ) signal [ positions ] = amplitude else : signal [:: period ] = amplitude metadata = { 'type' : 'impulse_train' , 'period' : period , 'amplitude' : amplitude , 'jitter' : jitter , 'num_impulses' : np . sum ( signal != 0 ), 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"impulse_train"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.intermittent_signal","text":"Generate an intermittent signal (randomly switches on/off). Source code in pyeyesweb/utils/signal_generators.py 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 def intermittent_signal ( self , length : int , active_ratio : float = 0.3 , signal_freq : float = 5.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate an intermittent signal (randomly switches on/off).\"\"\" t = self . _create_time_array ( length ) base_signal = amplitude * np . sin ( 2 * np . pi * signal_freq * t ) # Create random on/off pattern switch_period = int ( self . sampling_rate / 2 ) # Switch every 0.5 seconds n_switches = length // switch_period + 1 switches = np . random . random ( n_switches ) < active_ratio gate = np . repeat ( switches , switch_period )[: length ] signal = base_signal * gate metadata = { 'type' : 'intermittent_signal' , 'active_ratio' : active_ratio , 'signal_freq' : signal_freq , 'amplitude' : amplitude , 'actual_active_ratio' : np . mean ( gate ), 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"intermittent_signal"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.logistic_map","text":"Generate signal from logistic map (chaotic for r > 3.57). Source code in pyeyesweb/utils/signal_generators.py 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 def logistic_map ( self , length : int , r : float = 3.8 , x0 : float = 0.5 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate signal from logistic map (chaotic for r > 3.57).\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) x = x0 for i in range ( length ): x = r * x * ( 1 - x ) signal [ i ] = x metadata = { 'type' : 'logistic_map' , 'r' : r , 'x0' : x0 , 'chaotic' : r > 3.57 , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"logistic_map"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.lorenz_attractor","text":"Generate signal from Lorenz attractor (chaotic system). Source code in pyeyesweb/utils/signal_generators.py 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 def lorenz_attractor ( self , length : int , sigma : float = 10.0 , rho : float = 28.0 , beta : float = 8 / 3 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate signal from Lorenz attractor (chaotic system).\"\"\" dt = 0.01 t = np . arange ( 0 , length * dt , dt )[: length ] # Initialize x , y , z = 1.0 , 1.0 , 1.0 signal = np . zeros ( length ) # Integrate Lorenz equations for i in range ( length ): dx = sigma * ( y - x ) * dt dy = ( x * ( rho - z ) - y ) * dt dz = ( x * y - beta * z ) * dt x += dx y += dy z += dz signal [ i ] = x # Use x coordinate as signal # Normalize signal = ( signal - np . mean ( signal )) / np . std ( signal ) metadata = { 'type' : 'lorenz_attractor' , 'sigma' : sigma , 'rho' : rho , 'beta' : beta , 'sampling_rate' : self . sampling_rate } return t * self . sampling_rate , signal , metadata","title":"lorenz_attractor"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.multi_sine","text":"Generate multiple sine waves combined. Source code in pyeyesweb/utils/signal_generators.py 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 def multi_sine ( self , length : int , frequencies : List [ float ] = None , amplitudes : List [ float ] = None , phases : List [ float ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate multiple sine waves combined.\"\"\" if frequencies is None : frequencies = [ 1.0 , 3.0 , 5.0 ] if amplitudes is None : amplitudes = [ 1.0 ] * len ( frequencies ) if phases is None : phases = [ 0.0 ] * len ( frequencies ) t = self . _create_time_array ( length ) signal = np . zeros ( length ) for freq , amp , phase in zip ( frequencies , amplitudes , phases ): signal += amp * np . sin ( 2 * np . pi * freq * t + phase ) metadata = { 'type' : 'multi_sine' , 'frequencies' : frequencies , 'amplitudes' : amplitudes , 'phases' : phases , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"multi_sine"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.noisy_sine","text":"Generate a sine wave with added noise. Source code in pyeyesweb/utils/signal_generators.py 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 def noisy_sine ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , noise_level : float = 0.1 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sine wave with added noise.\"\"\" t , clean_signal , _ = self . sine_wave ( length , freq , amplitude ) noise = np . random . normal ( 0 , noise_level , length ) signal = clean_signal + noise metadata = { 'type' : 'noisy_sine' , 'frequency' : freq , 'amplitude' : amplitude , 'noise_level' : noise_level , 'snr' : 20 * np . log10 ( amplitude / noise_level ) if noise_level > 0 else np . inf , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"noisy_sine"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.phase_modulated","text":"Generate a phase modulated signal. Source code in pyeyesweb/utils/signal_generators.py 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 def phase_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , modulation_index : float = np . pi , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a phase modulated signal.\"\"\" t = self . _create_time_array ( length ) phase_modulation = modulation_index * np . sin ( 2 * np . pi * modulation_freq * t ) signal = np . sin ( 2 * np . pi * carrier_freq * t + phase_modulation ) metadata = { 'type' : 'phase_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'modulation_index' : modulation_index , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"phase_modulated"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.pink_noise","text":"Generate pink (1/f) noise. Source code in pyeyesweb/utils/signal_generators.py 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 def pink_noise ( self , length : int , amplitude : float = 1.0 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate pink (1/f) noise.\"\"\" if seed is not None : np . random . seed ( seed ) # Generate white noise white = np . random . randn ( length ) # Apply 1/f filter in frequency domain fft = np . fft . rfft ( white ) freqs = np . fft . rfftfreq ( length ) freqs [ 0 ] = 1 # Avoid division by zero fft = fft / np . sqrt ( freqs ) signal = np . fft . irfft ( fft , length ) # Normalize signal = amplitude * signal / np . std ( signal ) t = self . _create_time_array ( length ) metadata = { 'type' : 'pink_noise' , 'amplitude' : amplitude , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"pink_noise"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.pulse_width_modulated","text":"Generate a pulse width modulated signal. Source code in pyeyesweb/utils/signal_generators.py 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 def pulse_width_modulated ( self , length : int , carrier_freq : float = 10.0 , modulation_freq : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a pulse width modulated signal.\"\"\" t = self . _create_time_array ( length ) duty_cycle = 0.5 + 0.4 * np . sin ( 2 * np . pi * modulation_freq * t ) signal = np . zeros ( length ) # Generate PWM signal phase = ( carrier_freq * t ) % 1 for i in range ( length ): signal [ i ] = 1 if phase [ i ] < duty_cycle [ i ] else - 1 metadata = { 'type' : 'pulse_width_modulated' , 'carrier_freq' : carrier_freq , 'modulation_freq' : modulation_freq , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"pulse_width_modulated"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.ramp_function","text":"Generate a ramp function. Source code in pyeyesweb/utils/signal_generators.py 597 598 599 600 601 602 603 604 605 606 607 608 def ramp_function ( self , length : int , slope : float = 1.0 , start_value : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a ramp function.\"\"\" t = self . _create_time_array ( length ) signal = start_value + slope * t metadata = { 'type' : 'ramp' , 'slope' : slope , 'start_value' : start_value , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"ramp_function"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.random_signal","text":"Generate random uniform noise. Source code in pyeyesweb/utils/signal_generators.py 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def random_signal ( self , length : int , seed : Optional [ int ] = None , min_val : float = - 1.0 , max_val : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate random uniform noise.\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) signal = np . random . uniform ( min_val , max_val , length ) metadata = { 'type' : 'random' , 'distribution' : 'uniform' , 'range' : [ min_val , max_val ], 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"random_signal"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.sawtooth_wave","text":"Generate a sawtooth wave. Source code in pyeyesweb/utils/signal_generators.py 280 281 282 283 284 285 286 287 288 289 290 291 292 def sawtooth_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , width : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sawtooth wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . sawtooth ( 2 * np . pi * freq * t , width = width ) metadata = { 'type' : 'sawtooth' , 'frequency' : freq , 'amplitude' : amplitude , 'width' : width , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"sawtooth_wave"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.sine_wave","text":"Generate a sine wave. Source code in pyeyesweb/utils/signal_generators.py 237 238 239 240 241 242 243 244 245 246 247 248 249 def sine_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , phase : float = 0.0 , dc_offset : float = 0.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sine wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * np . sin ( 2 * np . pi * freq * t + phase ) + dc_offset metadata = self . _create_metadata ( 'sine' , frequency = freq , amplitude = amplitude , phase = phase , dc_offset = dc_offset ) return t , signal , metadata","title":"sine_wave"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.sine_with_drift","text":"Generate a sine wave with linear drift. Source code in pyeyesweb/utils/signal_generators.py 931 932 933 934 935 936 937 938 939 940 941 942 943 944 def sine_with_drift ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , drift_rate : float = 0.01 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a sine wave with linear drift.\"\"\" t , sine_signal , _ = self . sine_wave ( length , freq , amplitude ) drift = drift_rate * t signal = sine_signal + drift metadata = { 'type' : 'sine_with_drift' , 'frequency' : freq , 'amplitude' : amplitude , 'drift_rate' : drift_rate , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"sine_with_drift"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.spike_train","text":"Generate a neural spike train. Source code in pyeyesweb/utils/signal_generators.py 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 def spike_train ( self , length : int , spike_rate : float = 10.0 , refractory_period : float = 0.002 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a neural spike train.\"\"\" t = self . _create_time_array ( length ) dt = 1.0 / self . sampling_rate signal = np . zeros ( length ) # Generate spikes with refractory period last_spike_time = - refractory_period spike_times = [] for i , time in enumerate ( t ): if time - last_spike_time > refractory_period : if np . random . random () < spike_rate * dt : signal [ i ] = amplitude last_spike_time = time spike_times . append ( time ) metadata = { 'type' : 'spike_train' , 'spike_rate' : spike_rate , 'refractory_period' : refractory_period , 'amplitude' : amplitude , 'num_spikes' : len ( spike_times ), 'actual_rate' : len ( spike_times ) / ( t [ - 1 ] - t [ 0 ]) if t [ - 1 ] > t [ 0 ] else 0 , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"spike_train"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.square_wave","text":"Generate a square wave. Source code in pyeyesweb/utils/signal_generators.py 266 267 268 269 270 271 272 273 274 275 276 277 278 def square_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , duty_cycle : float = 0.5 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a square wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . square ( 2 * np . pi * freq * t , duty = duty_cycle ) metadata = { 'type' : 'square' , 'frequency' : freq , 'amplitude' : amplitude , 'duty_cycle' : duty_cycle , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"square_wave"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.step_function","text":"Generate a step function. Source code in pyeyesweb/utils/signal_generators.py 583 584 585 586 587 588 589 590 591 592 593 594 595 def step_function ( self , length : int , step_time : float = 0.5 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a step function.\"\"\" t = np . linspace ( 0 , 1 , length ) signal = np . ones ( length ) * amplitude signal [ t < step_time ] = 0 metadata = { 'type' : 'step' , 'step_time' : step_time , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t * length / self . sampling_rate , signal , metadata","title":"step_function"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.switched_signal","text":"Generate a signal that switches between two frequencies. Source code in pyeyesweb/utils/signal_generators.py 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 def switched_signal ( self , length : int , switch_period : int = 200 , freq1 : float = 2.0 , freq2 : float = 8.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a signal that switches between two frequencies.\"\"\" t = self . _create_time_array ( length ) signal = np . zeros ( length ) for i in range ( 0 , length , switch_period * 2 ): # First period: freq1 end1 = min ( i + switch_period , length ) signal [ i : end1 ] = amplitude * np . sin ( 2 * np . pi * freq1 * t [ i : end1 ]) # Second period: freq2 start2 = end1 end2 = min ( start2 + switch_period , length ) if start2 < length : signal [ start2 : end2 ] = amplitude * np . sin ( 2 * np . pi * freq2 * t [ start2 : end2 ]) metadata = { 'type' : 'switched_signal' , 'switch_period' : switch_period , 'freq1' : freq1 , 'freq2' : freq2 , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"switched_signal"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.tremor_signal","text":"Generate a tremor-like signal (slow movement with superimposed tremor). Source code in pyeyesweb/utils/signal_generators.py 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 def tremor_signal ( self , length : int , tremor_freq : float = 5.0 , base_freq : float = 0.5 , tremor_amplitude : float = 0.3 , base_amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a tremor-like signal (slow movement with superimposed tremor).\"\"\" t = self . _create_time_array ( length ) base_movement = base_amplitude * np . sin ( 2 * np . pi * base_freq * t ) tremor = tremor_amplitude * np . sin ( 2 * np . pi * tremor_freq * t ) signal = base_movement + tremor metadata = { 'type' : 'tremor' , 'tremor_freq' : tremor_freq , 'base_freq' : base_freq , 'tremor_amplitude' : tremor_amplitude , 'base_amplitude' : base_amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"tremor_signal"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.triangle_wave","text":"Generate a triangle wave. Source code in pyeyesweb/utils/signal_generators.py 294 295 296 297 298 299 300 301 302 303 304 305 def triangle_wave ( self , length : int , freq : float = 1.0 , amplitude : float = 1.0 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate a triangle wave.\"\"\" t = self . _create_time_array ( length ) signal = amplitude * sp_signal . sawtooth ( 2 * np . pi * freq * t , width = 0.5 ) metadata = { 'type' : 'triangle' , 'frequency' : freq , 'amplitude' : amplitude , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"triangle_wave"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.SignalGenerator.white_noise","text":"Generate white noise with specified power. Source code in pyeyesweb/utils/signal_generators.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def white_noise ( self , length : int , power : float = 1.0 , seed : Optional [ int ] = None , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\"Generate white noise with specified power.\"\"\" if seed is not None : np . random . seed ( seed ) t = self . _create_time_array ( length ) signal = np . random . normal ( 0 , np . sqrt ( power ), length ) metadata = { 'type' : 'white_noise' , 'power' : power , 'seed' : seed , 'sampling_rate' : self . sampling_rate } return t , signal , metadata","title":"white_noise"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.generate_signal","text":"Quick signal generation without instantiating the class. Args: signal_type: Type of signal to generate length: Number of samples **kwargs: Additional parameters Returns: Tuple of (time_array, signal_array, metadata_dict) Source code in pyeyesweb/utils/signal_generators.py 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 def generate_signal ( signal_type : str , length : int = 1000 , ** kwargs ) -> Tuple [ np . ndarray , np . ndarray , Dict ]: \"\"\" Quick signal generation without instantiating the class. Args: signal_type: Type of signal to generate length: Number of samples **kwargs: Additional parameters Returns: Tuple of (time_array, signal_array, metadata_dict) \"\"\" generator = SignalGenerator () return generator . generate ( signal_type , length , ** kwargs )","title":"generate_signal"},{"location":"API/utils/signal_generators/#pyeyesweb.utils.signal_generators.list_available_signals","text":"Get list of all available signal types. Source code in pyeyesweb/utils/signal_generators.py 1017 1018 1019 1020 def list_available_signals () -> List [ str ]: \"\"\"Get list of all available signal types.\"\"\" generator = SignalGenerator () return generator . available_signals","title":"list_available_signals"},{"location":"API/utils/signal_processing/","text":"Signal Processing Signal processing utilities for PyEyesWeb. This module provides signal processing functions including filtering, phase extraction, and smoothing operations used throughout the library. apply_savgol_filter ( signal , rate_hz = 50.0 ) Apply Savitzky-Golay filter if enough data is available. Savitzky-Golay filtering smooths data while preserving features better than moving average filters. Uses polynomial order 3 and adaptive window length. Parameters: signal ( array - like ) \u2013 1D signal to filter. rate_hz ( float , default: 50.0 ) \u2013 Sampling rate in Hz (currently unused but kept for API compatibility). Returns: ndarray \u2013 Filtered signal if sufficient data (\u22655 samples), otherwise original signal as array. Window length is min(n_samples, 11) and must be odd. Notes Requires at least 5 samples for filtering Window length must exceed polynomial order (3) Returns original signal if filtering fails Source code in pyeyesweb/utils/signal_processing.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def apply_savgol_filter ( signal , rate_hz = 50.0 ): \"\"\"Apply Savitzky-Golay filter if enough data is available. Savitzky-Golay filtering smooths data while preserving features better than moving average filters. Uses polynomial order 3 and adaptive window length. Parameters ---------- signal : array-like 1D signal to filter. rate_hz : float, optional Sampling rate in Hz (currently unused but kept for API compatibility). Returns ------- ndarray Filtered signal if sufficient data (\u22655 samples), otherwise original signal as array. Window length is min(n_samples, 11) and must be odd. Notes ----- - Requires at least 5 samples for filtering - Window length must exceed polynomial order (3) - Returns original signal if filtering fails \"\"\" if len ( signal ) < 5 : return np . array ( signal ) N = len ( signal ) polyorder = 3 window_length = min ( N if N % 2 == 1 else N - 1 , 11 ) if window_length <= polyorder : return np . array ( signal ) try : from scipy.signal import savgol_filter return savgol_filter ( signal , window_length = window_length , polyorder = polyorder ) except Exception : return np . array ( signal ) bandpass_filter ( data , filter_params ) Apply a band-pass filter if filter_params is set. Uses a 4th order Butterworth band-pass filter with zero-phase filtering (filtfilt) to avoid phase distortion. Parameters: data ( ndarray ) \u2013 Signal data of shape (n_samples, n_channels). filter_params ( tuple of (float, float, float) or None ) \u2013 Filter parameters as (lowcut_hz, highcut_hz, sampling_rate_hz). If None, returns data unchanged. Returns: ndarray \u2013 Filtered data with same shape as input. Returns original data if filter_params is None. Examples: >>> data = np . random . randn ( 1000 , 2 ) # 2 channels, 1000 samples >>> filtered = bandpass_filter ( data , ( 1.0 , 10.0 , 100.0 )) # 1-10 Hz Source code in pyeyesweb/utils/signal_processing.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def bandpass_filter ( data , filter_params ): \"\"\"Apply a band-pass filter if filter_params is set. Uses a 4th order Butterworth band-pass filter with zero-phase filtering (filtfilt) to avoid phase distortion. Parameters ---------- data : ndarray Signal data of shape (n_samples, n_channels). filter_params : tuple of (float, float, float) or None Filter parameters as (lowcut_hz, highcut_hz, sampling_rate_hz). If None, returns data unchanged. Returns ------- ndarray Filtered data with same shape as input. Returns original data if filter_params is None. Examples -------- >>> data = np.random.randn(1000, 2) # 2 channels, 1000 samples >>> filtered = bandpass_filter(data, (1.0, 10.0, 100.0)) # 1-10 Hz \"\"\" if filter_params is None : return data lowcut , highcut , fs = validate_filter_params ( * filter_params ) nyquist = 0.5 * fs low = lowcut / nyquist high = highcut / nyquist b , a = butter ( 4 , [ low , high ], btype = 'band' ) filtered_data = np . zeros_like ( data ) for i in range ( data . shape [ 1 ]): filtered_data [:, i ] = filtfilt ( b , a , data [:, i ]) return filtered_data compute_hilbert_phases ( sig ) Compute phase information from signals using Hilbert Transform. The Hilbert transform creates an analytic signal from which instantaneous phase can be extracted. Assumes input has exactly 2 channels. Parameters: sig ( ndarray ) \u2013 Signal array of shape (n_samples, 2) with two channels. Returns: phase1 ( ndarray ) \u2013 Phase values for first channel in radians [-\u03c0, \u03c0]. phase2 ( ndarray ) \u2013 Phase values for second channel in radians [-\u03c0, \u03c0]. Notes The Hilbert transform assumes the signal is narrowband or has been appropriately filtered for meaningful phase extraction. Source code in pyeyesweb/utils/signal_processing.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def compute_hilbert_phases ( sig ): \"\"\"Compute phase information from signals using Hilbert Transform. The Hilbert transform creates an analytic signal from which instantaneous phase can be extracted. Assumes input has exactly 2 channels. Parameters ---------- sig : ndarray Signal array of shape (n_samples, 2) with two channels. Returns ------- phase1 : ndarray Phase values for first channel in radians [-\u03c0, \u03c0]. phase2 : ndarray Phase values for second channel in radians [-\u03c0, \u03c0]. Notes ----- The Hilbert transform assumes the signal is narrowband or has been appropriately filtered for meaningful phase extraction. \"\"\" analytic_signal1 = hilbert ( sig [:, 0 ]) analytic_signal2 = hilbert ( sig [:, 1 ]) phase1 = np . angle ( analytic_signal1 ) phase2 = np . angle ( analytic_signal2 ) return phase1 , phase2 compute_phase_synchronization ( signals , filter_params = None ) Compute phase synchronization between two signals. Parameters: signals ( ndarray ) \u2013 Array of shape (n_samples, 2) containing two signals filter_params ( tuple or None , default: None ) \u2013 Optional filter parameters (lowcut, highcut, fs) Returns: float \u2013 Phase Locking Value between 0 and 1 Source code in pyeyesweb/utils/signal_processing.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def compute_phase_synchronization ( signals , filter_params = None ): \"\"\"Compute phase synchronization between two signals. Parameters ---------- signals : ndarray Array of shape (n_samples, 2) containing two signals filter_params : tuple or None Optional filter parameters (lowcut, highcut, fs) Returns ------- float Phase Locking Value between 0 and 1 \"\"\" from pyeyesweb.utils.math_utils import center_signals , compute_phase_locking_value sig = bandpass_filter ( signals , filter_params ) sig = center_signals ( sig ) phase1 , phase2 = compute_hilbert_phases ( sig ) return compute_phase_locking_value ( phase1 , phase2 ) validate_filter_params ( lowcut , highcut , fs ) Validate filter frequency parameters. Centralized validation for filter parameters used in bandpass_filter and Synchronization class. Parameters: lowcut ( float ) \u2013 Low cutoff frequency in Hz highcut ( float ) \u2013 High cutoff frequency in Hz fs ( float ) \u2013 Sampling frequency in Hz Returns: tuple \u2013 Validated (lowcut, highcut, fs) Raises: ValueError \u2013 If parameters are invalid Source code in pyeyesweb/utils/signal_processing.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def validate_filter_params ( lowcut , highcut , fs ): \"\"\"Validate filter frequency parameters. Centralized validation for filter parameters used in bandpass_filter and Synchronization class. Parameters ---------- lowcut : float Low cutoff frequency in Hz highcut : float High cutoff frequency in Hz fs : float Sampling frequency in Hz Returns ------- tuple Validated (lowcut, highcut, fs) Raises ------ ValueError If parameters are invalid \"\"\" # Validate individual parameters if fs <= 0 : raise ValueError ( f \"Sampling frequency must be positive, got { fs } \" ) if lowcut <= 0 : raise ValueError ( f \"Low cutoff frequency must be positive, got { lowcut } \" ) if highcut <= 0 : raise ValueError ( f \"High cutoff frequency must be positive, got { highcut } \" ) # Validate relationships if lowcut >= highcut : raise ValueError ( f \"Low cutoff ( { lowcut } ) must be less than high cutoff ( { highcut } )\" ) nyquist = fs / 2 if highcut >= nyquist : raise ValueError ( f \"High cutoff ( { highcut } ) must be less than Nyquist frequency ( { nyquist } )\" ) return lowcut , highcut , fs","title":"Signal Processing"},{"location":"API/utils/signal_processing/#signal-processing","text":"Signal processing utilities for PyEyesWeb. This module provides signal processing functions including filtering, phase extraction, and smoothing operations used throughout the library.","title":"Signal Processing"},{"location":"API/utils/signal_processing/#pyeyesweb.utils.signal_processing.apply_savgol_filter","text":"Apply Savitzky-Golay filter if enough data is available. Savitzky-Golay filtering smooths data while preserving features better than moving average filters. Uses polynomial order 3 and adaptive window length. Parameters: signal ( array - like ) \u2013 1D signal to filter. rate_hz ( float , default: 50.0 ) \u2013 Sampling rate in Hz (currently unused but kept for API compatibility). Returns: ndarray \u2013 Filtered signal if sufficient data (\u22655 samples), otherwise original signal as array. Window length is min(n_samples, 11) and must be odd. Notes Requires at least 5 samples for filtering Window length must exceed polynomial order (3) Returns original signal if filtering fails Source code in pyeyesweb/utils/signal_processing.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def apply_savgol_filter ( signal , rate_hz = 50.0 ): \"\"\"Apply Savitzky-Golay filter if enough data is available. Savitzky-Golay filtering smooths data while preserving features better than moving average filters. Uses polynomial order 3 and adaptive window length. Parameters ---------- signal : array-like 1D signal to filter. rate_hz : float, optional Sampling rate in Hz (currently unused but kept for API compatibility). Returns ------- ndarray Filtered signal if sufficient data (\u22655 samples), otherwise original signal as array. Window length is min(n_samples, 11) and must be odd. Notes ----- - Requires at least 5 samples for filtering - Window length must exceed polynomial order (3) - Returns original signal if filtering fails \"\"\" if len ( signal ) < 5 : return np . array ( signal ) N = len ( signal ) polyorder = 3 window_length = min ( N if N % 2 == 1 else N - 1 , 11 ) if window_length <= polyorder : return np . array ( signal ) try : from scipy.signal import savgol_filter return savgol_filter ( signal , window_length = window_length , polyorder = polyorder ) except Exception : return np . array ( signal )","title":"apply_savgol_filter"},{"location":"API/utils/signal_processing/#pyeyesweb.utils.signal_processing.bandpass_filter","text":"Apply a band-pass filter if filter_params is set. Uses a 4th order Butterworth band-pass filter with zero-phase filtering (filtfilt) to avoid phase distortion. Parameters: data ( ndarray ) \u2013 Signal data of shape (n_samples, n_channels). filter_params ( tuple of (float, float, float) or None ) \u2013 Filter parameters as (lowcut_hz, highcut_hz, sampling_rate_hz). If None, returns data unchanged. Returns: ndarray \u2013 Filtered data with same shape as input. Returns original data if filter_params is None. Examples: >>> data = np . random . randn ( 1000 , 2 ) # 2 channels, 1000 samples >>> filtered = bandpass_filter ( data , ( 1.0 , 10.0 , 100.0 )) # 1-10 Hz Source code in pyeyesweb/utils/signal_processing.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def bandpass_filter ( data , filter_params ): \"\"\"Apply a band-pass filter if filter_params is set. Uses a 4th order Butterworth band-pass filter with zero-phase filtering (filtfilt) to avoid phase distortion. Parameters ---------- data : ndarray Signal data of shape (n_samples, n_channels). filter_params : tuple of (float, float, float) or None Filter parameters as (lowcut_hz, highcut_hz, sampling_rate_hz). If None, returns data unchanged. Returns ------- ndarray Filtered data with same shape as input. Returns original data if filter_params is None. Examples -------- >>> data = np.random.randn(1000, 2) # 2 channels, 1000 samples >>> filtered = bandpass_filter(data, (1.0, 10.0, 100.0)) # 1-10 Hz \"\"\" if filter_params is None : return data lowcut , highcut , fs = validate_filter_params ( * filter_params ) nyquist = 0.5 * fs low = lowcut / nyquist high = highcut / nyquist b , a = butter ( 4 , [ low , high ], btype = 'band' ) filtered_data = np . zeros_like ( data ) for i in range ( data . shape [ 1 ]): filtered_data [:, i ] = filtfilt ( b , a , data [:, i ]) return filtered_data","title":"bandpass_filter"},{"location":"API/utils/signal_processing/#pyeyesweb.utils.signal_processing.compute_hilbert_phases","text":"Compute phase information from signals using Hilbert Transform. The Hilbert transform creates an analytic signal from which instantaneous phase can be extracted. Assumes input has exactly 2 channels. Parameters: sig ( ndarray ) \u2013 Signal array of shape (n_samples, 2) with two channels. Returns: phase1 ( ndarray ) \u2013 Phase values for first channel in radians [-\u03c0, \u03c0]. phase2 ( ndarray ) \u2013 Phase values for second channel in radians [-\u03c0, \u03c0]. Notes The Hilbert transform assumes the signal is narrowband or has been appropriately filtered for meaningful phase extraction. Source code in pyeyesweb/utils/signal_processing.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def compute_hilbert_phases ( sig ): \"\"\"Compute phase information from signals using Hilbert Transform. The Hilbert transform creates an analytic signal from which instantaneous phase can be extracted. Assumes input has exactly 2 channels. Parameters ---------- sig : ndarray Signal array of shape (n_samples, 2) with two channels. Returns ------- phase1 : ndarray Phase values for first channel in radians [-\u03c0, \u03c0]. phase2 : ndarray Phase values for second channel in radians [-\u03c0, \u03c0]. Notes ----- The Hilbert transform assumes the signal is narrowband or has been appropriately filtered for meaningful phase extraction. \"\"\" analytic_signal1 = hilbert ( sig [:, 0 ]) analytic_signal2 = hilbert ( sig [:, 1 ]) phase1 = np . angle ( analytic_signal1 ) phase2 = np . angle ( analytic_signal2 ) return phase1 , phase2","title":"compute_hilbert_phases"},{"location":"API/utils/signal_processing/#pyeyesweb.utils.signal_processing.compute_phase_synchronization","text":"Compute phase synchronization between two signals. Parameters: signals ( ndarray ) \u2013 Array of shape (n_samples, 2) containing two signals filter_params ( tuple or None , default: None ) \u2013 Optional filter parameters (lowcut, highcut, fs) Returns: float \u2013 Phase Locking Value between 0 and 1 Source code in pyeyesweb/utils/signal_processing.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def compute_phase_synchronization ( signals , filter_params = None ): \"\"\"Compute phase synchronization between two signals. Parameters ---------- signals : ndarray Array of shape (n_samples, 2) containing two signals filter_params : tuple or None Optional filter parameters (lowcut, highcut, fs) Returns ------- float Phase Locking Value between 0 and 1 \"\"\" from pyeyesweb.utils.math_utils import center_signals , compute_phase_locking_value sig = bandpass_filter ( signals , filter_params ) sig = center_signals ( sig ) phase1 , phase2 = compute_hilbert_phases ( sig ) return compute_phase_locking_value ( phase1 , phase2 )","title":"compute_phase_synchronization"},{"location":"API/utils/signal_processing/#pyeyesweb.utils.signal_processing.validate_filter_params","text":"Validate filter frequency parameters. Centralized validation for filter parameters used in bandpass_filter and Synchronization class. Parameters: lowcut ( float ) \u2013 Low cutoff frequency in Hz highcut ( float ) \u2013 High cutoff frequency in Hz fs ( float ) \u2013 Sampling frequency in Hz Returns: tuple \u2013 Validated (lowcut, highcut, fs) Raises: ValueError \u2013 If parameters are invalid Source code in pyeyesweb/utils/signal_processing.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def validate_filter_params ( lowcut , highcut , fs ): \"\"\"Validate filter frequency parameters. Centralized validation for filter parameters used in bandpass_filter and Synchronization class. Parameters ---------- lowcut : float Low cutoff frequency in Hz highcut : float High cutoff frequency in Hz fs : float Sampling frequency in Hz Returns ------- tuple Validated (lowcut, highcut, fs) Raises ------ ValueError If parameters are invalid \"\"\" # Validate individual parameters if fs <= 0 : raise ValueError ( f \"Sampling frequency must be positive, got { fs } \" ) if lowcut <= 0 : raise ValueError ( f \"Low cutoff frequency must be positive, got { lowcut } \" ) if highcut <= 0 : raise ValueError ( f \"High cutoff frequency must be positive, got { highcut } \" ) # Validate relationships if lowcut >= highcut : raise ValueError ( f \"Low cutoff ( { lowcut } ) must be less than high cutoff ( { highcut } )\" ) nyquist = fs / 2 if highcut >= nyquist : raise ValueError ( f \"High cutoff ( { highcut } ) must be less than Nyquist frequency ( { nyquist } )\" ) return lowcut , highcut , fs","title":"validate_filter_params"},{"location":"API/utils/tsv_reader/","text":"Tsv Reader TSV file reader for time-series data with real-time playback support. This module provides a specialized TSV reader for motion capture and sensor data files, supporting multiple reading modes including real-time playback simulation. TSVReader TSV file reader with multiple reading modes for time-series data. Provides three reading modes: 1. Block reading: Read fixed-size chunks of data 2. Time-based access: Get data at specific time points 3. Real-time iteration: Simulate real-time playback with timing delays Parameters: time_col ( str , default: 'Time' ) \u2013 Name of the column containing time values (default: \"Time\"). Source code in pyeyesweb/utils/tsv_reader.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 class TSVReader : \"\"\"TSV file reader with multiple reading modes for time-series data. Provides three reading modes: 1. Block reading: Read fixed-size chunks of data 2. Time-based access: Get data at specific time points 3. Real-time iteration: Simulate real-time playback with timing delays Parameters ---------- time_col : str, optional Name of the column containing time values (default: \"Time\"). \"\"\" def __init__ ( self , time_col = \"Time\" ): self . time_col = time_col self . headers = [] self . data = None self . time_data = None self . index = 0 self . prev_time = None self . filename = None self . n = None self . time_value = None self . use_time = False self . speed_factor = 1.0 # ------------------- FILE NAME ------------------- def _set_file_name ( self , filename ): self . filename = filename self . reset () self . _load_file () # ------------------- TIME COLUMN ------------------- def _set_time_column ( self , time_col ): self . time_col = time_col # ------------------- SIZE OF BLOCK ------------------- def _set_block_size ( self , block_size ): self . n = block_size self . time_value = None self . use_time = False self . speed_factor = 1.0 # ------------------- SPECIFIC TIME ------------------- def _set_time_value ( self , time_value ): self . time_value = time_value self . n = None self . use_time = False self . speed_factor = 1.0 # ------------------- REAL TIME READING ------------------- def _set_use_time_and_speed ( self , factor = 1.0 ): self . use_time = True self . speed_factor = factor self . time_value = None self . n = None def _load_file ( self ): \"\"\"Load TSV file quickly using np.loadtxt. Expects a TSV file with a header line starting with 'Frame'. \"\"\" with open ( self . filename , \"r\" ) as f : for i , line in enumerate ( f ): if line . startswith ( \"Frame\" ): header_idx = i self . headers = line . strip () . split ( \" \\t \" ) break else : raise ValueError ( \"Header 'Frame...' not found\" ) self . data = np . loadtxt ( self . filename , delimiter = \" \\t \" , skiprows = header_idx + 1 , dtype = float ) if self . data . ndim == 1 : self . data = self . data . reshape ( 1 , - 1 ) try : time_idx = self . headers . index ( self . time_col ) except ValueError : raise ValueError ( f \"Column ' { self . time_col } ' not found in headers\" ) self . time_data = self . data [:, time_idx ] def __call__ ( self , time_value = None ): if time_value is not None : self . time_value = time_value return self . _get_row_by_time () if self . n is not None : return self . _get_n_rows () return self . _iter_rows_gen () # ------------------- READ ROW BY TIME ------------------- def _get_row_by_time ( self ): idx = np . searchsorted ( self . time_data , self . time_value ) if idx == len ( self . time_data ): idx -= 1 elif idx > 0 and abs ( self . time_data [ idx - 1 ] - self . time_value ) < abs ( self . time_data [ idx ] - self . time_value ): idx -= 1 return self . data [ idx ] # ------------------- READ BLOCK ------------------- def _get_n_rows ( self ): start = self . index end = min ( self . index + self . n , len ( self . data )) self . index = end return self . data [ start : end ] # ------------------- READ ROW BY ROW ------------------- def _iter_rows_gen ( self , chunk_size = 1000 ): time_idx = self . headers . index ( self . time_col ) while self . index < len ( self . data ): end = min ( self . index + chunk_size , len ( self . data )) chunk = self . data [ self . index : end ] for row in chunk : current_time = row [ time_idx ] if self . use_time and self . prev_time is not None : delay = ( current_time - self . prev_time ) / self . speed_factor if delay > 0 : self . _sleep_accurate ( delay ) self . prev_time = current_time yield row self . index = end # ------------------- SLEEP ------------------- def _sleep_accurate ( self , delay_sec ): \"\"\"Sleep for the specified duration without burning CPU cycles. For delays >= 1ms, uses time.sleep() which yields the CPU to OS. For submillisecond delays, uses a hybrid approach for accuracy. \"\"\" if delay_sec <= 0 : return # For delays >= 1ms, use regular sleep (yields CPU) if delay_sec >= 0.001 : time . sleep ( delay_sec ) else : # For sub-millisecond delays, minimize busy wait time # Sleep for most of the duration, then busy wait for precision if delay_sec > 0.0002 : # > 0.2ms time . sleep ( delay_sec * 0.5 ) # Sleep 50% of time # Brief busy-wait only for final precision target = time . perf_counter () + ( delay_sec * 0.5 if delay_sec > 0.0002 else delay_sec ) while time . perf_counter () < target : pass # ------------------- RESET ------------------- def reset ( self ): self . index = 0 self . prev_time = None","title":"Tsv Reader"},{"location":"API/utils/tsv_reader/#tsv-reader","text":"TSV file reader for time-series data with real-time playback support. This module provides a specialized TSV reader for motion capture and sensor data files, supporting multiple reading modes including real-time playback simulation.","title":"Tsv Reader"},{"location":"API/utils/tsv_reader/#pyeyesweb.utils.tsv_reader.TSVReader","text":"TSV file reader with multiple reading modes for time-series data. Provides three reading modes: 1. Block reading: Read fixed-size chunks of data 2. Time-based access: Get data at specific time points 3. Real-time iteration: Simulate real-time playback with timing delays Parameters: time_col ( str , default: 'Time' ) \u2013 Name of the column containing time values (default: \"Time\"). Source code in pyeyesweb/utils/tsv_reader.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 class TSVReader : \"\"\"TSV file reader with multiple reading modes for time-series data. Provides three reading modes: 1. Block reading: Read fixed-size chunks of data 2. Time-based access: Get data at specific time points 3. Real-time iteration: Simulate real-time playback with timing delays Parameters ---------- time_col : str, optional Name of the column containing time values (default: \"Time\"). \"\"\" def __init__ ( self , time_col = \"Time\" ): self . time_col = time_col self . headers = [] self . data = None self . time_data = None self . index = 0 self . prev_time = None self . filename = None self . n = None self . time_value = None self . use_time = False self . speed_factor = 1.0 # ------------------- FILE NAME ------------------- def _set_file_name ( self , filename ): self . filename = filename self . reset () self . _load_file () # ------------------- TIME COLUMN ------------------- def _set_time_column ( self , time_col ): self . time_col = time_col # ------------------- SIZE OF BLOCK ------------------- def _set_block_size ( self , block_size ): self . n = block_size self . time_value = None self . use_time = False self . speed_factor = 1.0 # ------------------- SPECIFIC TIME ------------------- def _set_time_value ( self , time_value ): self . time_value = time_value self . n = None self . use_time = False self . speed_factor = 1.0 # ------------------- REAL TIME READING ------------------- def _set_use_time_and_speed ( self , factor = 1.0 ): self . use_time = True self . speed_factor = factor self . time_value = None self . n = None def _load_file ( self ): \"\"\"Load TSV file quickly using np.loadtxt. Expects a TSV file with a header line starting with 'Frame'. \"\"\" with open ( self . filename , \"r\" ) as f : for i , line in enumerate ( f ): if line . startswith ( \"Frame\" ): header_idx = i self . headers = line . strip () . split ( \" \\t \" ) break else : raise ValueError ( \"Header 'Frame...' not found\" ) self . data = np . loadtxt ( self . filename , delimiter = \" \\t \" , skiprows = header_idx + 1 , dtype = float ) if self . data . ndim == 1 : self . data = self . data . reshape ( 1 , - 1 ) try : time_idx = self . headers . index ( self . time_col ) except ValueError : raise ValueError ( f \"Column ' { self . time_col } ' not found in headers\" ) self . time_data = self . data [:, time_idx ] def __call__ ( self , time_value = None ): if time_value is not None : self . time_value = time_value return self . _get_row_by_time () if self . n is not None : return self . _get_n_rows () return self . _iter_rows_gen () # ------------------- READ ROW BY TIME ------------------- def _get_row_by_time ( self ): idx = np . searchsorted ( self . time_data , self . time_value ) if idx == len ( self . time_data ): idx -= 1 elif idx > 0 and abs ( self . time_data [ idx - 1 ] - self . time_value ) < abs ( self . time_data [ idx ] - self . time_value ): idx -= 1 return self . data [ idx ] # ------------------- READ BLOCK ------------------- def _get_n_rows ( self ): start = self . index end = min ( self . index + self . n , len ( self . data )) self . index = end return self . data [ start : end ] # ------------------- READ ROW BY ROW ------------------- def _iter_rows_gen ( self , chunk_size = 1000 ): time_idx = self . headers . index ( self . time_col ) while self . index < len ( self . data ): end = min ( self . index + chunk_size , len ( self . data )) chunk = self . data [ self . index : end ] for row in chunk : current_time = row [ time_idx ] if self . use_time and self . prev_time is not None : delay = ( current_time - self . prev_time ) / self . speed_factor if delay > 0 : self . _sleep_accurate ( delay ) self . prev_time = current_time yield row self . index = end # ------------------- SLEEP ------------------- def _sleep_accurate ( self , delay_sec ): \"\"\"Sleep for the specified duration without burning CPU cycles. For delays >= 1ms, uses time.sleep() which yields the CPU to OS. For submillisecond delays, uses a hybrid approach for accuracy. \"\"\" if delay_sec <= 0 : return # For delays >= 1ms, use regular sleep (yields CPU) if delay_sec >= 0.001 : time . sleep ( delay_sec ) else : # For sub-millisecond delays, minimize busy wait time # Sleep for most of the duration, then busy wait for precision if delay_sec > 0.0002 : # > 0.2ms time . sleep ( delay_sec * 0.5 ) # Sleep 50% of time # Brief busy-wait only for final precision target = time . perf_counter () + ( delay_sec * 0.5 if delay_sec > 0.0002 else delay_sec ) while time . perf_counter () < target : pass # ------------------- RESET ------------------- def reset ( self ): self . index = 0 self . prev_time = None","title":"TSVReader"},{"location":"API/utils/validators/","text":"Validators Validation utilities for PyEyesWeb. This module provides common validation functions used across multiple PyEyesWeb modules to ensure consistent error handling. validate_and_normalize_filter_params ( filter_params ) Validate and normalize filter parameters. Parameters: filter_params ( tuple / list or None ) \u2013 Filter parameters as (lowcut, highcut, fs) or None Returns: tuple or None \u2013 Validated (lowcut, highcut, fs) tuple or None if input was None Source code in pyeyesweb/utils/validators.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def validate_and_normalize_filter_params ( filter_params ): \"\"\"Validate and normalize filter parameters. Parameters ---------- filter_params : tuple/list or None Filter parameters as (lowcut, highcut, fs) or None Returns ------- tuple or None Validated (lowcut, highcut, fs) tuple or None if input was None \"\"\" if filter_params is None : return None # Import here to avoid circular dependency from pyeyesweb.utils.signal_processing import validate_filter_params filter_params = validate_filter_params_tuple ( filter_params ) lowcut , highcut , fs = validate_filter_params ( * filter_params ) return ( lowcut , highcut , fs ) validate_boolean ( value , name ) Validate boolean parameter. Parameters: value ( any ) \u2013 Value to validate name ( str ) \u2013 Parameter name for error messages Returns: bool \u2013 Validated boolean value Raises: TypeError \u2013 If value is not a boolean Examples: >>> validate_boolean ( True , 'use_filter' ) True >>> validate_boolean ( 1 , 'output_phase' ) TypeError: output_phase must be boolean, got int Source code in pyeyesweb/utils/validators.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def validate_boolean ( value , name ): \"\"\"Validate boolean parameter. Parameters ---------- value : any Value to validate name : str Parameter name for error messages Returns ------- bool Validated boolean value Raises ------ TypeError If value is not a boolean Examples -------- >>> validate_boolean(True, 'use_filter') True >>> validate_boolean(1, 'output_phase') TypeError: output_phase must be boolean, got int \"\"\" if not isinstance ( value , bool ): raise TypeError ( f \" { name } must be boolean, got { type ( value ) . __name__ } \" ) return value validate_filter_params_tuple ( value , name = 'filter_params' ) Validate filter parameters tuple structure. Ensures the value is a tuple/list with exactly 3 numeric elements before passing to validate_filter_params for frequency validation. Parameters: value ( any ) \u2013 Value to validate as filter parameters tuple name ( str , default: 'filter_params' ) \u2013 Parameter name for error messages (default: 'filter_params') Returns: tuple \u2013 Validated tuple of (lowcut, highcut, fs) Raises: TypeError \u2013 If value is not a tuple/list or contains non-numeric elements ValueError \u2013 If value doesn't have exactly 3 elements Examples: >>> validate_filter_params_tuple (( 1.0 , 10.0 , 100.0 )) (1.0, 10.0, 100.0) >>> validate_filter_params_tuple ([ 1 , 10 , 100 ]) (1, 10, 100) >>> validate_filter_params_tuple ( \"invalid\" ) TypeError: filter_params must be a tuple or list, got str Source code in pyeyesweb/utils/validators.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def validate_filter_params_tuple ( value , name = 'filter_params' ): \"\"\"Validate filter parameters tuple structure. Ensures the value is a tuple/list with exactly 3 numeric elements before passing to validate_filter_params for frequency validation. Parameters ---------- value : any Value to validate as filter parameters tuple name : str, optional Parameter name for error messages (default: 'filter_params') Returns ------- tuple Validated tuple of (lowcut, highcut, fs) Raises ------ TypeError If value is not a tuple/list or contains non-numeric elements ValueError If value doesn't have exactly 3 elements Examples -------- >>> validate_filter_params_tuple((1.0, 10.0, 100.0)) (1.0, 10.0, 100.0) >>> validate_filter_params_tuple([1, 10, 100]) (1, 10, 100) >>> validate_filter_params_tuple(\"invalid\") TypeError: filter_params must be a tuple or list, got str \"\"\" if not isinstance ( value , ( tuple , list )): raise TypeError ( f \" { name } must be a tuple or list, got { type ( value ) . __name__ } \" ) if len ( value ) != 3 : raise ValueError ( f \" { name } must have 3 elements (lowcut, highcut, fs), got { len ( value ) } \" ) if not all ( isinstance ( x , ( int , float )) for x in value ): raise TypeError ( f \" { name } must contain only numbers\" ) return tuple ( value ) validate_integer ( value , name , min_val = None , max_val = None ) Validate integer parameter with optional bounds checking. Parameters: value ( any ) \u2013 Value to validate name ( str ) \u2013 Parameter name for error messages min_val ( int , default: None ) \u2013 Minimum allowed value (inclusive) max_val ( int , default: None ) \u2013 Maximum allowed value (inclusive) Returns: int \u2013 Validated integer value Raises: TypeError \u2013 If value is not an integer ValueError \u2013 If value is outside specified bounds Examples: >>> validate_integer ( 100 , 'sensitivity' , min_val = 1 , max_val = 10000 ) 100 >>> validate_integer ( 0 , 'max_length' , min_val = 1 ) ValueError: max_length must be >= 1, got 0 Source code in pyeyesweb/utils/validators.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def validate_integer ( value , name , min_val = None , max_val = None ): \"\"\"Validate integer parameter with optional bounds checking. Parameters ---------- value : any Value to validate name : str Parameter name for error messages min_val : int, optional Minimum allowed value (inclusive) max_val : int, optional Maximum allowed value (inclusive) Returns ------- int Validated integer value Raises ------ TypeError If value is not an integer ValueError If value is outside specified bounds Examples -------- >>> validate_integer(100, 'sensitivity', min_val=1, max_val=10000) 100 >>> validate_integer(0, 'max_length', min_val=1) ValueError: max_length must be >= 1, got 0 \"\"\" if not isinstance ( value , int ): raise TypeError ( f \" { name } must be an integer, got { type ( value ) . __name__ } \" ) if min_val is not None and value < min_val : raise ValueError ( f \" { name } must be >= { min_val } , got { value } \" ) if max_val is not None and value > max_val : raise ValueError ( f \" { name } must be <= { max_val } , got { value } \" ) return value validate_numeric ( value , name , min_val = None , max_val = None ) Validate numeric parameter with optional bounds checking. Parameters: value ( any ) \u2013 Value to validate name ( str ) \u2013 Parameter name for error messages min_val ( float , default: None ) \u2013 Minimum allowed value (inclusive) max_val ( float , default: None ) \u2013 Maximum allowed value (inclusive) Returns: float \u2013 Validated numeric value as float Raises: TypeError \u2013 If value is not numeric (int or float) ValueError \u2013 If value is outside specified bounds Examples: >>> validate_numeric ( 50.0 , 'rate_hz' , min_val = 0.1 , max_val = 100000 ) 50.0 >>> validate_numeric ( - 1 , 'phase' , min_val = 0 , max_val = 1 ) ValueError: phase must be >= 0, got -1 Source code in pyeyesweb/utils/validators.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def validate_numeric ( value , name , min_val = None , max_val = None ): \"\"\"Validate numeric parameter with optional bounds checking. Parameters ---------- value : any Value to validate name : str Parameter name for error messages min_val : float, optional Minimum allowed value (inclusive) max_val : float, optional Maximum allowed value (inclusive) Returns ------- float Validated numeric value as float Raises ------ TypeError If value is not numeric (int or float) ValueError If value is outside specified bounds Examples -------- >>> validate_numeric(50.0, 'rate_hz', min_val=0.1, max_val=100000) 50.0 >>> validate_numeric(-1, 'phase', min_val=0, max_val=1) ValueError: phase must be >= 0, got -1 \"\"\" if not isinstance ( value , ( int , float )): raise TypeError ( f \" { name } must be a number, got { type ( value ) . __name__ } \" ) value = float ( value ) if min_val is not None and value < min_val : raise ValueError ( f \" { name } must be >= { min_val } , got { value } \" ) if max_val is not None and value > max_val : raise ValueError ( f \" { name } must be <= { max_val } , got { value } \" ) return value validate_range ( value , name , min_val , max_val ) Validate that a value is within a specific range. Useful for parameters that must be within a specific range like phase_threshold (0-1), percentages (0-100), etc. Parameters: value ( float or int ) \u2013 Value to validate name ( str ) \u2013 Parameter name for error messages min_val ( float ) \u2013 Minimum allowed value (inclusive) max_val ( float ) \u2013 Maximum allowed value (inclusive) Returns: float \u2013 Validated value Raises: ValueError \u2013 If value is outside the specified range Examples: >>> validate_range ( 0.7 , 'phase_threshold' , 0 , 1 ) 0.7 >>> validate_range ( 1.5 , 'probability' , 0 , 1 ) ValueError: probability must be between 0 and 1, got 1.5 Source code in pyeyesweb/utils/validators.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def validate_range ( value , name , min_val , max_val ): \"\"\"Validate that a value is within a specific range. Useful for parameters that must be within a specific range like phase_threshold (0-1), percentages (0-100), etc. Parameters ---------- value : float or int Value to validate name : str Parameter name for error messages min_val : float Minimum allowed value (inclusive) max_val : float Maximum allowed value (inclusive) Returns ------- float Validated value Raises ------ ValueError If value is outside the specified range Examples -------- >>> validate_range(0.7, 'phase_threshold', 0, 1) 0.7 >>> validate_range(1.5, 'probability', 0, 1) ValueError: probability must be between 0 and 1, got 1.5 \"\"\" if not min_val <= value <= max_val : raise ValueError ( f \" { name } must be between { min_val } and { max_val } , got { value } \" ) return value validate_window_size ( value , name = 'window_size' ) Validate window size parameter. Standard validation for window sizes used across multiple modules. Parameters: value ( int ) \u2013 Window size value name ( str , default: 'window_size' ) \u2013 Parameter name for error messages Returns: int \u2013 Validated window size Source code in pyeyesweb/utils/validators.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def validate_window_size ( value , name = 'window_size' ): \"\"\"Validate window size parameter. Standard validation for window sizes used across multiple modules. Parameters ---------- value : int Window size value name : str Parameter name for error messages Returns ------- int Validated window size \"\"\" return validate_integer ( value , name , min_val = 1 , max_val = 10000 )","title":"Validators"},{"location":"API/utils/validators/#validators","text":"Validation utilities for PyEyesWeb. This module provides common validation functions used across multiple PyEyesWeb modules to ensure consistent error handling.","title":"Validators"},{"location":"API/utils/validators/#pyeyesweb.utils.validators.validate_and_normalize_filter_params","text":"Validate and normalize filter parameters. Parameters: filter_params ( tuple / list or None ) \u2013 Filter parameters as (lowcut, highcut, fs) or None Returns: tuple or None \u2013 Validated (lowcut, highcut, fs) tuple or None if input was None Source code in pyeyesweb/utils/validators.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def validate_and_normalize_filter_params ( filter_params ): \"\"\"Validate and normalize filter parameters. Parameters ---------- filter_params : tuple/list or None Filter parameters as (lowcut, highcut, fs) or None Returns ------- tuple or None Validated (lowcut, highcut, fs) tuple or None if input was None \"\"\" if filter_params is None : return None # Import here to avoid circular dependency from pyeyesweb.utils.signal_processing import validate_filter_params filter_params = validate_filter_params_tuple ( filter_params ) lowcut , highcut , fs = validate_filter_params ( * filter_params ) return ( lowcut , highcut , fs )","title":"validate_and_normalize_filter_params"},{"location":"API/utils/validators/#pyeyesweb.utils.validators.validate_boolean","text":"Validate boolean parameter. Parameters: value ( any ) \u2013 Value to validate name ( str ) \u2013 Parameter name for error messages Returns: bool \u2013 Validated boolean value Raises: TypeError \u2013 If value is not a boolean Examples: >>> validate_boolean ( True , 'use_filter' ) True >>> validate_boolean ( 1 , 'output_phase' ) TypeError: output_phase must be boolean, got int Source code in pyeyesweb/utils/validators.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def validate_boolean ( value , name ): \"\"\"Validate boolean parameter. Parameters ---------- value : any Value to validate name : str Parameter name for error messages Returns ------- bool Validated boolean value Raises ------ TypeError If value is not a boolean Examples -------- >>> validate_boolean(True, 'use_filter') True >>> validate_boolean(1, 'output_phase') TypeError: output_phase must be boolean, got int \"\"\" if not isinstance ( value , bool ): raise TypeError ( f \" { name } must be boolean, got { type ( value ) . __name__ } \" ) return value","title":"validate_boolean"},{"location":"API/utils/validators/#pyeyesweb.utils.validators.validate_filter_params_tuple","text":"Validate filter parameters tuple structure. Ensures the value is a tuple/list with exactly 3 numeric elements before passing to validate_filter_params for frequency validation. Parameters: value ( any ) \u2013 Value to validate as filter parameters tuple name ( str , default: 'filter_params' ) \u2013 Parameter name for error messages (default: 'filter_params') Returns: tuple \u2013 Validated tuple of (lowcut, highcut, fs) Raises: TypeError \u2013 If value is not a tuple/list or contains non-numeric elements ValueError \u2013 If value doesn't have exactly 3 elements Examples: >>> validate_filter_params_tuple (( 1.0 , 10.0 , 100.0 )) (1.0, 10.0, 100.0) >>> validate_filter_params_tuple ([ 1 , 10 , 100 ]) (1, 10, 100) >>> validate_filter_params_tuple ( \"invalid\" ) TypeError: filter_params must be a tuple or list, got str Source code in pyeyesweb/utils/validators.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def validate_filter_params_tuple ( value , name = 'filter_params' ): \"\"\"Validate filter parameters tuple structure. Ensures the value is a tuple/list with exactly 3 numeric elements before passing to validate_filter_params for frequency validation. Parameters ---------- value : any Value to validate as filter parameters tuple name : str, optional Parameter name for error messages (default: 'filter_params') Returns ------- tuple Validated tuple of (lowcut, highcut, fs) Raises ------ TypeError If value is not a tuple/list or contains non-numeric elements ValueError If value doesn't have exactly 3 elements Examples -------- >>> validate_filter_params_tuple((1.0, 10.0, 100.0)) (1.0, 10.0, 100.0) >>> validate_filter_params_tuple([1, 10, 100]) (1, 10, 100) >>> validate_filter_params_tuple(\"invalid\") TypeError: filter_params must be a tuple or list, got str \"\"\" if not isinstance ( value , ( tuple , list )): raise TypeError ( f \" { name } must be a tuple or list, got { type ( value ) . __name__ } \" ) if len ( value ) != 3 : raise ValueError ( f \" { name } must have 3 elements (lowcut, highcut, fs), got { len ( value ) } \" ) if not all ( isinstance ( x , ( int , float )) for x in value ): raise TypeError ( f \" { name } must contain only numbers\" ) return tuple ( value )","title":"validate_filter_params_tuple"},{"location":"API/utils/validators/#pyeyesweb.utils.validators.validate_integer","text":"Validate integer parameter with optional bounds checking. Parameters: value ( any ) \u2013 Value to validate name ( str ) \u2013 Parameter name for error messages min_val ( int , default: None ) \u2013 Minimum allowed value (inclusive) max_val ( int , default: None ) \u2013 Maximum allowed value (inclusive) Returns: int \u2013 Validated integer value Raises: TypeError \u2013 If value is not an integer ValueError \u2013 If value is outside specified bounds Examples: >>> validate_integer ( 100 , 'sensitivity' , min_val = 1 , max_val = 10000 ) 100 >>> validate_integer ( 0 , 'max_length' , min_val = 1 ) ValueError: max_length must be >= 1, got 0 Source code in pyeyesweb/utils/validators.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def validate_integer ( value , name , min_val = None , max_val = None ): \"\"\"Validate integer parameter with optional bounds checking. Parameters ---------- value : any Value to validate name : str Parameter name for error messages min_val : int, optional Minimum allowed value (inclusive) max_val : int, optional Maximum allowed value (inclusive) Returns ------- int Validated integer value Raises ------ TypeError If value is not an integer ValueError If value is outside specified bounds Examples -------- >>> validate_integer(100, 'sensitivity', min_val=1, max_val=10000) 100 >>> validate_integer(0, 'max_length', min_val=1) ValueError: max_length must be >= 1, got 0 \"\"\" if not isinstance ( value , int ): raise TypeError ( f \" { name } must be an integer, got { type ( value ) . __name__ } \" ) if min_val is not None and value < min_val : raise ValueError ( f \" { name } must be >= { min_val } , got { value } \" ) if max_val is not None and value > max_val : raise ValueError ( f \" { name } must be <= { max_val } , got { value } \" ) return value","title":"validate_integer"},{"location":"API/utils/validators/#pyeyesweb.utils.validators.validate_numeric","text":"Validate numeric parameter with optional bounds checking. Parameters: value ( any ) \u2013 Value to validate name ( str ) \u2013 Parameter name for error messages min_val ( float , default: None ) \u2013 Minimum allowed value (inclusive) max_val ( float , default: None ) \u2013 Maximum allowed value (inclusive) Returns: float \u2013 Validated numeric value as float Raises: TypeError \u2013 If value is not numeric (int or float) ValueError \u2013 If value is outside specified bounds Examples: >>> validate_numeric ( 50.0 , 'rate_hz' , min_val = 0.1 , max_val = 100000 ) 50.0 >>> validate_numeric ( - 1 , 'phase' , min_val = 0 , max_val = 1 ) ValueError: phase must be >= 0, got -1 Source code in pyeyesweb/utils/validators.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def validate_numeric ( value , name , min_val = None , max_val = None ): \"\"\"Validate numeric parameter with optional bounds checking. Parameters ---------- value : any Value to validate name : str Parameter name for error messages min_val : float, optional Minimum allowed value (inclusive) max_val : float, optional Maximum allowed value (inclusive) Returns ------- float Validated numeric value as float Raises ------ TypeError If value is not numeric (int or float) ValueError If value is outside specified bounds Examples -------- >>> validate_numeric(50.0, 'rate_hz', min_val=0.1, max_val=100000) 50.0 >>> validate_numeric(-1, 'phase', min_val=0, max_val=1) ValueError: phase must be >= 0, got -1 \"\"\" if not isinstance ( value , ( int , float )): raise TypeError ( f \" { name } must be a number, got { type ( value ) . __name__ } \" ) value = float ( value ) if min_val is not None and value < min_val : raise ValueError ( f \" { name } must be >= { min_val } , got { value } \" ) if max_val is not None and value > max_val : raise ValueError ( f \" { name } must be <= { max_val } , got { value } \" ) return value","title":"validate_numeric"},{"location":"API/utils/validators/#pyeyesweb.utils.validators.validate_range","text":"Validate that a value is within a specific range. Useful for parameters that must be within a specific range like phase_threshold (0-1), percentages (0-100), etc. Parameters: value ( float or int ) \u2013 Value to validate name ( str ) \u2013 Parameter name for error messages min_val ( float ) \u2013 Minimum allowed value (inclusive) max_val ( float ) \u2013 Maximum allowed value (inclusive) Returns: float \u2013 Validated value Raises: ValueError \u2013 If value is outside the specified range Examples: >>> validate_range ( 0.7 , 'phase_threshold' , 0 , 1 ) 0.7 >>> validate_range ( 1.5 , 'probability' , 0 , 1 ) ValueError: probability must be between 0 and 1, got 1.5 Source code in pyeyesweb/utils/validators.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def validate_range ( value , name , min_val , max_val ): \"\"\"Validate that a value is within a specific range. Useful for parameters that must be within a specific range like phase_threshold (0-1), percentages (0-100), etc. Parameters ---------- value : float or int Value to validate name : str Parameter name for error messages min_val : float Minimum allowed value (inclusive) max_val : float Maximum allowed value (inclusive) Returns ------- float Validated value Raises ------ ValueError If value is outside the specified range Examples -------- >>> validate_range(0.7, 'phase_threshold', 0, 1) 0.7 >>> validate_range(1.5, 'probability', 0, 1) ValueError: probability must be between 0 and 1, got 1.5 \"\"\" if not min_val <= value <= max_val : raise ValueError ( f \" { name } must be between { min_val } and { max_val } , got { value } \" ) return value","title":"validate_range"},{"location":"API/utils/validators/#pyeyesweb.utils.validators.validate_window_size","text":"Validate window size parameter. Standard validation for window sizes used across multiple modules. Parameters: value ( int ) \u2013 Window size value name ( str , default: 'window_size' ) \u2013 Parameter name for error messages Returns: int \u2013 Validated window size Source code in pyeyesweb/utils/validators.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def validate_window_size ( value , name = 'window_size' ): \"\"\"Validate window size parameter. Standard validation for window sizes used across multiple modules. Parameters ---------- value : int Window size value name : str Parameter name for error messages Returns ------- int Validated window size \"\"\" return validate_integer ( value , name , min_val = 1 , max_val = 10000 )","title":"validate_window_size"},{"location":"user_guide/getting_started/","text":"/* 1. Force the Description column (2nd column) to wrap text */ .rst-content table.docutils td:nth-child(2) { white-space: normal !important; /* Forces text to wrap */ word-wrap: break-word !important; max-width: 300px !important; /* Sets the constraint */ } /* 2. Fix the Huge Icons */ .rst-content .twemoji { height: 1.2em !important; width: 1.2em !important; vertical-align: text-bottom; } Getting Started PyEyesWeb is an open-source Python library for the analysis of expressive qualities in human movement. It provides algorithms to extract movement features from motion data, enabling researchers, artists, and developers to quantify and analyze movement expressivity. Installation You can install PyEyesWeb using pip. Open your terminal and run: pip install pyeyesweb Quick Start Here's a simple example to get you started with PyEyesWeb. This example demonstrates how to compute the smoothness index from motion data. Note Motion data can vary significantly depending on the use case. In this example, we assume a simplified scenario where you have a CSV file containing pre-calculated velocity modules. The file is structured with frames in rows and the velocity value in a single column. We will loop through this file row-by-row, update a sliding window, and compute smoothness in real-time. import csv from pyeyesweb.data_models import SlidingWindow from pyeyesweb.low_level import Smoothness # 1. Initialize the Smoothness analyzer smoothness = Smoothness ( rate_hz = 50.0 ) # 2. Create a sliding window to store the last 100 frames of data window = SlidingWindow ( max_length = 100 , n_columns = 1 ) # 3. Simulate a real-time loop reading from a CSV with open ( 'velocity_data.csv' , 'r' ) as f : reader = csv . reader ( f ) for row in reader : # Extract the velocity module (assuming it is in the first column) velocity_val = float ( row [ 0 ]) # Append the new frame to the sliding window window . append ([ velocity_val ]) #(1)! # Compute smoothness features on the current window sparc , jerk = smoothness ( window ) #(2)! # Check if a valid result was returned # (feature may return None if window has not enough samples) if sparc is not None and jerk is not None : print ( f \"SPARC: { sparc : .3f } | Jerk: { jerk : .3f } \" ) The SlidingWindow expects a list of values for every frame (even if there is only 1 sample). As the loop runs, new data is added to the end, and old data is automatically discarded once the max_length is reached. The smoothness callable processes the current state of the window. It returns the SPARC (Spectral Arc Length) and Jerk RMS. If the window does not yet contain enough data to compute the metric, it may return None. Subpackages PyEyesWeb is organized into subpackages analyzing movement features at different levels of abstraction and time scales 1 . Subpackage Description Implemented physical_signals Data acquisition from physical and virtual sensors (e.g., motion capture, IMU, video, physiological signals). low_level Extraction of instantaneous descriptors from raw data (e.g., velocity, acceleration, kinetic energy, posture). mid_level Structural and amodal descriptors over movement units or windows (e.g., fluidity, coordination, lightness). high_level Expressive and communicative qualities perceived by an observer (e.g., emotion, saliency, social signals). analysis_primitives General-purpose operators applied at all levels (e.g., statistical moments, entropy, recurrence, predictive models). References Camurri, A., Volpe, G., Piana, S., Mancini, M., Niewiadomski, R., Ferrari, N., & Canepa, C. (2016, July). The dancer in the eye: towards a multi-layered computational framework of qualities in movement. In Proceedings of the 3rd International Symposium on Movement and Computing (pp. 1-7). \u21a9","title":"Getting Started"},{"location":"user_guide/getting_started/#getting-started","text":"PyEyesWeb is an open-source Python library for the analysis of expressive qualities in human movement. It provides algorithms to extract movement features from motion data, enabling researchers, artists, and developers to quantify and analyze movement expressivity.","title":"Getting Started"},{"location":"user_guide/getting_started/#installation","text":"You can install PyEyesWeb using pip. Open your terminal and run: pip install pyeyesweb","title":"Installation"},{"location":"user_guide/getting_started/#quick-start","text":"Here's a simple example to get you started with PyEyesWeb. This example demonstrates how to compute the smoothness index from motion data. Note Motion data can vary significantly depending on the use case. In this example, we assume a simplified scenario where you have a CSV file containing pre-calculated velocity modules. The file is structured with frames in rows and the velocity value in a single column. We will loop through this file row-by-row, update a sliding window, and compute smoothness in real-time. import csv from pyeyesweb.data_models import SlidingWindow from pyeyesweb.low_level import Smoothness # 1. Initialize the Smoothness analyzer smoothness = Smoothness ( rate_hz = 50.0 ) # 2. Create a sliding window to store the last 100 frames of data window = SlidingWindow ( max_length = 100 , n_columns = 1 ) # 3. Simulate a real-time loop reading from a CSV with open ( 'velocity_data.csv' , 'r' ) as f : reader = csv . reader ( f ) for row in reader : # Extract the velocity module (assuming it is in the first column) velocity_val = float ( row [ 0 ]) # Append the new frame to the sliding window window . append ([ velocity_val ]) #(1)! # Compute smoothness features on the current window sparc , jerk = smoothness ( window ) #(2)! # Check if a valid result was returned # (feature may return None if window has not enough samples) if sparc is not None and jerk is not None : print ( f \"SPARC: { sparc : .3f } | Jerk: { jerk : .3f } \" ) The SlidingWindow expects a list of values for every frame (even if there is only 1 sample). As the loop runs, new data is added to the end, and old data is automatically discarded once the max_length is reached. The smoothness callable processes the current state of the window. It returns the SPARC (Spectral Arc Length) and Jerk RMS. If the window does not yet contain enough data to compute the metric, it may return None.","title":"Quick Start"},{"location":"user_guide/getting_started/#subpackages","text":"PyEyesWeb is organized into subpackages analyzing movement features at different levels of abstraction and time scales 1 . Subpackage Description Implemented physical_signals Data acquisition from physical and virtual sensors (e.g., motion capture, IMU, video, physiological signals). low_level Extraction of instantaneous descriptors from raw data (e.g., velocity, acceleration, kinetic energy, posture). mid_level Structural and amodal descriptors over movement units or windows (e.g., fluidity, coordination, lightness). high_level Expressive and communicative qualities perceived by an observer (e.g., emotion, saliency, social signals). analysis_primitives General-purpose operators applied at all levels (e.g., statistical moments, entropy, recurrence, predictive models).","title":"Subpackages"},{"location":"user_guide/getting_started/#references","text":"Camurri, A., Volpe, G., Piana, S., Mancini, M., Niewiadomski, R., Ferrari, N., & Canepa, C. (2016, July). The dancer in the eye: towards a multi-layered computational framework of qualities in movement. In Proceedings of the 3rd International Symposium on Movement and Computing (pp. 1-7). \u21a9","title":"References"},{"location":"user_guide/integrations/","text":"Integrations PyEyesWeb aims to provide seamless integration with popular tools in the creative and artistic ecosystem. TouchDesigner We provide a plugin consisting of a set of TouchDesigner custom components (i.e., .tox files). Each .tox component implements a movement feature as outlined in the documentation .","title":"Integrations"},{"location":"user_guide/integrations/#integrations","text":"PyEyesWeb aims to provide seamless integration with popular tools in the creative and artistic ecosystem.","title":"Integrations"},{"location":"user_guide/integrations/#touchdesigner","text":"We provide a plugin consisting of a set of TouchDesigner custom components (i.e., .tox files). Each .tox component implements a movement feature as outlined in the documentation .","title":"TouchDesigner"},{"location":"user_guide/theoretical_framework/theoretical_framework/","text":"Theoretical Framework PyEyesWeb inherits from the rich tradition of computational movement analysis initiated by the EyesWeb project 1 2 3 and grounds on the multi-layered computational framework of qualities in movement developed in the DANCE project 4 . Conceptual Model Warning This is the result of an open field of research. As such, certain aspects of the model may be provisional or subject to refinement. Some concepts are open to interpretation, and current limitations are actively being addressed in the ongoing work. The framework describes how raw sensor data can be progressively transformed into meaningful descriptions of expressive movement qualities and is organized into four layers. The layers represent a conceptual model and not a strict processing pipeline . Layers Overview Attention! The concept of timescale is crucial in this framework, and each layer operates at different temporal scales. As an example, a key distinction from Layers 2 and 3 is moving from instantaneous or short-window features (~0.5s) to longer windows (0.5-3s) or movement units (e.g., a specific sport gesture, a choreographic phase). One same feature can occur at different layers and yield different interpretations depending on the timescale of analysis. Layer 1 \u2013 Physical Signals : raw data captured by virtual sensors , i.e., physical devices (motion capture, accelerometers, video, RGB-D cameras, physiological sensors, etc.) enriched with preprocessing (denoising, filtering, extraction of trajectories, silhouettes, respiration, etc.). Foundation for all higher layers. \u2192 Learn more Layer 2 \u2013 Low-Level Features : instantaneous or short-window (0.5s) descriptors computed from physical signals. Examples are velocity, acceleration, kinetic energy, balance, smoothness, etc. Represented as time-series. \u2192 Learn more Layer 3 \u2013 Mid-Level Features : operates on movement units or longer windows . Examples are directness, lightness, suddenness, fluidity, repetitiveness. Introduce amodal descriptors across modalities. \u2192 Learn more Layer 4 \u2013 Expressive Qualities : focuses on what an observer perceives from movement: emotional expression, saliency, attraction/repulsion, hesitation, predictability. Involves memory and context , influencing how movement is interpreted. Requires context and ML mappings. \u2192 Learn more Analysis Primitives : Core computational tools applied across all layers. Includes: statistical moments, entropy, shape descriptors (peaks, slopes), synchronization, time-frequency transforms, predictive and physical models (e.g., mass\u2013spring). Provide the building blocks for extracting meaningful features. \u2192 Learn more References Camurri, A., Mazzarino, B., & Volpe, G. (2003, April). Analysis of expressive gesture: The eyesweb expressive gesture processing library. In International gesture workshop (pp. 460-467). Berlin, Heidelberg: Springer Berlin Heidelberg. \u21a9 Camurri, A., Coletta, P., Massari, A., Mazzarino, B., Peri, M., Ricchetti, M., ... & Volpe, G. (2004, March). Toward real-time multimodal processing: EyesWeb 4.0. In Proc. AISB (pp. 22-26). \u21a9 Volpe, G., Alborno, P., Camurri, A., Coletta, P., Ghisio, S., Mancini, M., ... & Sagoleo, R. (2016). Designing multimodal interactive systems using EyesWeb XMI. In CEUR Workshop Proceedings (pp. 49-56). CEUR-WS. \u21a9 Camurri, A., Volpe, G., Piana, S., Mancini, M., Niewiadomski, R., Ferrari, N., & Canepa, C. (2016, July). The dancer in the eye: towards a multi-layered computational framework of qualities in movement. In Proceedings of the 3rd International Symposium on Movement and Computing (pp. 1-7). \u21a9","title":"Theoretical Framework"},{"location":"user_guide/theoretical_framework/theoretical_framework/#theoretical-framework","text":"PyEyesWeb inherits from the rich tradition of computational movement analysis initiated by the EyesWeb project 1 2 3 and grounds on the multi-layered computational framework of qualities in movement developed in the DANCE project 4 .","title":"Theoretical Framework"},{"location":"user_guide/theoretical_framework/theoretical_framework/#conceptual-model","text":"Warning This is the result of an open field of research. As such, certain aspects of the model may be provisional or subject to refinement. Some concepts are open to interpretation, and current limitations are actively being addressed in the ongoing work. The framework describes how raw sensor data can be progressively transformed into meaningful descriptions of expressive movement qualities and is organized into four layers. The layers represent a conceptual model and not a strict processing pipeline .","title":"Conceptual Model"},{"location":"user_guide/theoretical_framework/theoretical_framework/#layers-overview","text":"Attention! The concept of timescale is crucial in this framework, and each layer operates at different temporal scales. As an example, a key distinction from Layers 2 and 3 is moving from instantaneous or short-window features (~0.5s) to longer windows (0.5-3s) or movement units (e.g., a specific sport gesture, a choreographic phase). One same feature can occur at different layers and yield different interpretations depending on the timescale of analysis. Layer 1 \u2013 Physical Signals : raw data captured by virtual sensors , i.e., physical devices (motion capture, accelerometers, video, RGB-D cameras, physiological sensors, etc.) enriched with preprocessing (denoising, filtering, extraction of trajectories, silhouettes, respiration, etc.). Foundation for all higher layers. \u2192 Learn more Layer 2 \u2013 Low-Level Features : instantaneous or short-window (0.5s) descriptors computed from physical signals. Examples are velocity, acceleration, kinetic energy, balance, smoothness, etc. Represented as time-series. \u2192 Learn more Layer 3 \u2013 Mid-Level Features : operates on movement units or longer windows . Examples are directness, lightness, suddenness, fluidity, repetitiveness. Introduce amodal descriptors across modalities. \u2192 Learn more Layer 4 \u2013 Expressive Qualities : focuses on what an observer perceives from movement: emotional expression, saliency, attraction/repulsion, hesitation, predictability. Involves memory and context , influencing how movement is interpreted. Requires context and ML mappings. \u2192 Learn more Analysis Primitives : Core computational tools applied across all layers. Includes: statistical moments, entropy, shape descriptors (peaks, slopes), synchronization, time-frequency transforms, predictive and physical models (e.g., mass\u2013spring). Provide the building blocks for extracting meaningful features. \u2192 Learn more","title":"Layers Overview"},{"location":"user_guide/theoretical_framework/theoretical_framework/#references","text":"Camurri, A., Mazzarino, B., & Volpe, G. (2003, April). Analysis of expressive gesture: The eyesweb expressive gesture processing library. In International gesture workshop (pp. 460-467). Berlin, Heidelberg: Springer Berlin Heidelberg. \u21a9 Camurri, A., Coletta, P., Massari, A., Mazzarino, B., Peri, M., Ricchetti, M., ... & Volpe, G. (2004, March). Toward real-time multimodal processing: EyesWeb 4.0. In Proc. AISB (pp. 22-26). \u21a9 Volpe, G., Alborno, P., Camurri, A., Coletta, P., Ghisio, S., Mancini, M., ... & Sagoleo, R. (2016). Designing multimodal interactive systems using EyesWeb XMI. In CEUR Workshop Proceedings (pp. 49-56). CEUR-WS. \u21a9 Camurri, A., Volpe, G., Piana, S., Mancini, M., Niewiadomski, R., Ferrari, N., & Canepa, C. (2016, July). The dancer in the eye: towards a multi-layered computational framework of qualities in movement. In Proceedings of the 3rd International Symposium on Movement and Computing (pp. 1-7). \u21a9","title":"References"},{"location":"user_guide/theoretical_framework/analysis_primitives/analysis_primitives/","text":"/* 1. First Column: Allow wrapping and set modest width */ .rst-content table.docutils td:nth-child(1) { width: 25%; /* Fixed width for the feature name */ white-space: normal !important; /* ALLOWS wrapping */ word-wrap: break-word; /* Breaks long words if necessary */ } /* 2. Second Column: Maximize width */ .rst-content table.docutils td:nth-child(2) { white-space: normal !important; word-wrap: break-word !important; width: 70%; /* Takes up the majority of the table */ } /* 3. Third Column: Keep it tight (Optional but recommended) */ .rst-content table.docutils td:nth-child(3) { width: 10%; white-space: nowrap; text-align: center; } /* 4. Fix the Huge Icons */ .rst-content .twemoji { height: 1.2em !important; width: 1.2em !important; vertical-align: text-bottom; } Analysis Primitives Analysis primitives are operators applied across layers to extract meaningful patterns from features. They summarize, transform, or model data at various temporal and spatial scales. Example of Analysis Primitives Primitive Type Description Implemented Statistical Moments Unary operators summarizing distributions (mean, variance, skewness, kurtosis). Shape Descriptors Peaks, slopes, valleys in time-series; geometric descriptors of movement curves. Entropy 1 Approximate/sample entropy, recurrence analysis; quantify predictability or irregularity. Time-Frequency Transforms Fourier or wavelet transforms to detect rhythm, periodicity, or temporal structures. Symmetry 2 Unary/binary operators measuring geometric or dynamic balance (e.g., left vs. right entropy or energy). Synchronization 3 4 Binary/n-ary operators measuring alignment of signals (cross-correlation, phase-locking, group entrainment). Causality 4 Directional relationships (e.g., Granger causality, transfer entropy) to detect leader\u2013follower dynamics. Clusterability 5 Measures the tendency of data points to form clusters by means of the Hopkins statistics. Predictive Models Hidden Markov Models, classifiers, neural networks; used for gesture segmentation or quality inference. Saliency / Rarity 6 Detecting unusual occurrences in movement with respect to most frequent patterns. References Glowinski, D., Mancini, M., & Camurri, A. (2013, March). Studying the effect of creative joint action on musicians\u2019 behavior. In International Conference on Arts and Technology (pp. 113-119). Berlin, Heidelberg: Springer Berlin Heidelberg. \u21a9 Glowinski, D., Dael, N., Camurri, A., Volpe, G., Mortillaro, M., & Scherer, K. (2011). Toward a minimal representation of affective gestures. IEEE Transactions on Affective Computing, 2(2), 106-118. \u21a9 Varni, G., Volpe, G., & Camurri, A. (2010). A system for real-time multimodal analysis of nonverbal affective social interaction in user-centric media. IEEE Transactions on Multimedia, 12(6), 576-590. \u21a9 Sabharwal, S. R., Varlet, M., Breaden, M., Volpe, G., Camurri, A., & Keller, P. E. (2022). huSync-A model and system for the measure of synchronization in small groups: A case study on musical joint action. IEEE Access, 10, 92357-92372. \u21a9 \u21a9 Corbellini, N., Ceccaldi, E., Varni, G., & Volpe, G. (2022, August). An exploratory study on group potency classification from non-verbal social behaviours. In International Conference on Pattern Recognition (pp. 240-255). Cham: Springer Nature Switzerland. \u21a9 Niewiadomski, R., Mancini, M., Cera, A., Piana, S., Canepa, C., & Camurri, A. (2019). Does embodied training improve the recognition of mid-level expressive movement qualities sonification?. Journal on Multimodal User Interfaces, 13, 191-203. \u21a9","title":"Analysis Primitives"},{"location":"user_guide/theoretical_framework/analysis_primitives/analysis_primitives/#analysis-primitives","text":"Analysis primitives are operators applied across layers to extract meaningful patterns from features. They summarize, transform, or model data at various temporal and spatial scales.","title":"Analysis Primitives"},{"location":"user_guide/theoretical_framework/analysis_primitives/analysis_primitives/#example-of-analysis-primitives","text":"Primitive Type Description Implemented Statistical Moments Unary operators summarizing distributions (mean, variance, skewness, kurtosis). Shape Descriptors Peaks, slopes, valleys in time-series; geometric descriptors of movement curves. Entropy 1 Approximate/sample entropy, recurrence analysis; quantify predictability or irregularity. Time-Frequency Transforms Fourier or wavelet transforms to detect rhythm, periodicity, or temporal structures. Symmetry 2 Unary/binary operators measuring geometric or dynamic balance (e.g., left vs. right entropy or energy). Synchronization 3 4 Binary/n-ary operators measuring alignment of signals (cross-correlation, phase-locking, group entrainment). Causality 4 Directional relationships (e.g., Granger causality, transfer entropy) to detect leader\u2013follower dynamics. Clusterability 5 Measures the tendency of data points to form clusters by means of the Hopkins statistics. Predictive Models Hidden Markov Models, classifiers, neural networks; used for gesture segmentation or quality inference. Saliency / Rarity 6 Detecting unusual occurrences in movement with respect to most frequent patterns.","title":"Example of Analysis Primitives"},{"location":"user_guide/theoretical_framework/analysis_primitives/analysis_primitives/#references","text":"Glowinski, D., Mancini, M., & Camurri, A. (2013, March). Studying the effect of creative joint action on musicians\u2019 behavior. In International Conference on Arts and Technology (pp. 113-119). Berlin, Heidelberg: Springer Berlin Heidelberg. \u21a9 Glowinski, D., Dael, N., Camurri, A., Volpe, G., Mortillaro, M., & Scherer, K. (2011). Toward a minimal representation of affective gestures. IEEE Transactions on Affective Computing, 2(2), 106-118. \u21a9 Varni, G., Volpe, G., & Camurri, A. (2010). A system for real-time multimodal analysis of nonverbal affective social interaction in user-centric media. IEEE Transactions on Multimedia, 12(6), 576-590. \u21a9 Sabharwal, S. R., Varlet, M., Breaden, M., Volpe, G., Camurri, A., & Keller, P. E. (2022). huSync-A model and system for the measure of synchronization in small groups: A case study on musical joint action. IEEE Access, 10, 92357-92372. \u21a9 \u21a9 Corbellini, N., Ceccaldi, E., Varni, G., & Volpe, G. (2022, August). An exploratory study on group potency classification from non-verbal social behaviours. In International Conference on Pattern Recognition (pp. 240-255). Cham: Springer Nature Switzerland. \u21a9 Niewiadomski, R., Mancini, M., Cera, A., Piana, S., Canepa, C., & Camurri, A. (2019). Does embodied training improve the recognition of mid-level expressive movement qualities sonification?. Journal on Multimodal User Interfaces, 13, 191-203. \u21a9","title":"References"},{"location":"user_guide/theoretical_framework/analysis_primitives/bilateral_symmetry/","text":"Bilateral Symmetry Analysis Module Bilateral symmetry assessment is crucial for understanding movement disorders, rehabilitation progress, and motor control strategies. This module analyzes coordination patterns using three complementary methods : Bilateral Symmetry Index (BSI) \u2014 spatial mirror-symmetry measure. Phase Synchronization (PLV) \u2014 temporal coordination of left\u2013right movement. Canonical Correlation Analysis (CCA) \u2014 multivariate correlation of trajectories. Note These methods assume approximately Gaussian noise and sufficient frame history. Bilateral Symmetry Index (BSI) The BSI quantifies spatial symmetry between left and right trajectories by comparing mirrored movements across the sagittal plane. Input: - Left and right 3D joint trajectories $$ L, R \\in \\mathbb{R}^{n \\times 3} $$ Procedure: Mirror right trajectory across sagittal plane: \\[ R' = [-R_x, R_y, R_z] \\] Compute absolute difference: \\[ D = |L - R'| \\] Normalize by total magnitude: \\[ S = |L| + |R'| \\] Relative asymmetry percentage: \\[ A = \\frac{1}{n} \\sum_i \\frac{D_i}{S_i} \\times 100 \\] Symmetry index: \\[ \\text{BSI} = 1 - \\frac{A}{100} \\] Output: \\[ \\text{BSI} \\in [0, 1] \\] Tip Higher BSI indicates more symmetric posture and movement. Phase Synchronization (Analytic Signal Approach) Phase synchronization quantifies the temporal relationship between left and right signals by analysing the constancy of their phase difference. It is computed via Hilbert transform and expressed as Phase Locking Value (PLV) . Input: Two 1D limb displacement signals $$ x(t), y(t) $$. Procedure: Compute analytic signals: \\[ z_x(t) = x(t) + iH[x(t)], \\] \\[ z_y(t) = y(t) + iH[y(t)] \\] Extract instantaneous phases: \\[ \\phi_x(t), \\phi_y(t) \\] Compute phase difference: \\[ \\Delta \\phi(t) = \\phi_x(t) - \\phi_y(t) \\] Compute Phase Locking Value (PLV): \\[ \\text{PLV} = \\left| \\frac{1}{N} \\sum_t e^{i \\Delta \\phi(t)} \\right| \\] Output: \\[ \\text{PLV} \\in [0, 1] \\] Tip PLV > 0.7 \u2192 strong coupling PLV 0.4\u20130.7 \u2192 moderate PLV < 0.4 \u2192 weak coupling Canonical Correlation Analysis (CCA) CCA measures multivariate correspondence between left and right 3D trajectories. It finds linear combinations that maximize correlation, revealing shared coordination structure. Input: \\[ L, R \\in \\mathbb{R}^{n \\times 3} \\] Procedure: Flatten coordinates into features. Apply one-component CCA: \\[ (U, V) = \\text{CCA}(L, R) \\] Compute canonical correlation: \\[ \\rho = \\text{corr}(U, V) \\] Output: \\[ \\rho \\in [0, 1] \\] Tip 1.0\u20130.8: excellent coordination 0.8\u20130.6: good 0.6\u20130.4: moderate <0.4: poor","title":"Bilateral Symmetry"},{"location":"user_guide/theoretical_framework/analysis_primitives/bilateral_symmetry/#bilateral-symmetry-analysis-module","text":"Bilateral symmetry assessment is crucial for understanding movement disorders, rehabilitation progress, and motor control strategies. This module analyzes coordination patterns using three complementary methods : Bilateral Symmetry Index (BSI) \u2014 spatial mirror-symmetry measure. Phase Synchronization (PLV) \u2014 temporal coordination of left\u2013right movement. Canonical Correlation Analysis (CCA) \u2014 multivariate correlation of trajectories. Note These methods assume approximately Gaussian noise and sufficient frame history.","title":"Bilateral Symmetry Analysis Module"},{"location":"user_guide/theoretical_framework/analysis_primitives/bilateral_symmetry/#bilateral-symmetry-index-bsi","text":"The BSI quantifies spatial symmetry between left and right trajectories by comparing mirrored movements across the sagittal plane. Input: - Left and right 3D joint trajectories $$ L, R \\in \\mathbb{R}^{n \\times 3} $$ Procedure: Mirror right trajectory across sagittal plane: \\[ R' = [-R_x, R_y, R_z] \\] Compute absolute difference: \\[ D = |L - R'| \\] Normalize by total magnitude: \\[ S = |L| + |R'| \\] Relative asymmetry percentage: \\[ A = \\frac{1}{n} \\sum_i \\frac{D_i}{S_i} \\times 100 \\] Symmetry index: \\[ \\text{BSI} = 1 - \\frac{A}{100} \\] Output: \\[ \\text{BSI} \\in [0, 1] \\] Tip Higher BSI indicates more symmetric posture and movement.","title":"Bilateral Symmetry Index (BSI)"},{"location":"user_guide/theoretical_framework/analysis_primitives/bilateral_symmetry/#phase-synchronization-analytic-signal-approach","text":"Phase synchronization quantifies the temporal relationship between left and right signals by analysing the constancy of their phase difference. It is computed via Hilbert transform and expressed as Phase Locking Value (PLV) . Input: Two 1D limb displacement signals $$ x(t), y(t) $$. Procedure: Compute analytic signals: \\[ z_x(t) = x(t) + iH[x(t)], \\] \\[ z_y(t) = y(t) + iH[y(t)] \\] Extract instantaneous phases: \\[ \\phi_x(t), \\phi_y(t) \\] Compute phase difference: \\[ \\Delta \\phi(t) = \\phi_x(t) - \\phi_y(t) \\] Compute Phase Locking Value (PLV): \\[ \\text{PLV} = \\left| \\frac{1}{N} \\sum_t e^{i \\Delta \\phi(t)} \\right| \\] Output: \\[ \\text{PLV} \\in [0, 1] \\] Tip PLV > 0.7 \u2192 strong coupling PLV 0.4\u20130.7 \u2192 moderate PLV < 0.4 \u2192 weak coupling","title":"Phase Synchronization (Analytic Signal Approach)"},{"location":"user_guide/theoretical_framework/analysis_primitives/bilateral_symmetry/#canonical-correlation-analysis-cca","text":"CCA measures multivariate correspondence between left and right 3D trajectories. It finds linear combinations that maximize correlation, revealing shared coordination structure. Input: \\[ L, R \\in \\mathbb{R}^{n \\times 3} \\] Procedure: Flatten coordinates into features. Apply one-component CCA: \\[ (U, V) = \\text{CCA}(L, R) \\] Compute canonical correlation: \\[ \\rho = \\text{corr}(U, V) \\] Output: \\[ \\rho \\in [0, 1] \\] Tip 1.0\u20130.8: excellent coordination 0.8\u20130.6: good 0.6\u20130.4: moderate <0.4: poor","title":"Canonical Correlation Analysis (CCA)"},{"location":"user_guide/theoretical_framework/analysis_primitives/synchronization/","text":"Synchronization Analysis Module The Synchronization Module quantifies temporal coordination between multiple participants, body segments, or movement components. It provides methods to assess how well different movement signals align in time and phase . Note The method assumes stationarity over the analysis window and Gaussian noise. Algorithms Details Phase Coupling Analysis Phase synchronization measures temporal alignment independent of amplitude: Hilbert Transform : Compute analytic signals \\[ z_x(t) = x(t) + i H[x(t)], \\quad z_y(t) = y(t) + i H[y(t)] \\] Instantaneous Phases : \\[ \\phi_x(t) = \\arg(z_x(t)), \\quad \\phi_y(t) = \\arg(z_y(t)) \\] Phase Difference : \\[ \\Delta \\phi(t) = \\phi_x(t) - \\phi_y(t) \\] Phase Locking Value (PLV) : \\[ \\text{PLV} = \\left| \\frac{1}{N} \\sum_{t=1}^{N} e^{i \\Delta \\phi(t)} \\right| \\] Output: \\( \\text{PLV} \\in [0,1] \\) Interpretation \\( PLV > 0.7 \\) : strong phase coupling \\( 0.4 < PLV \\leq 0.7 \\) : moderate phase coupling \\( PLV \\leq 0.4 \\) : weak phase coupling References To be added","title":"Synchronization"},{"location":"user_guide/theoretical_framework/analysis_primitives/synchronization/#synchronization-analysis-module","text":"The Synchronization Module quantifies temporal coordination between multiple participants, body segments, or movement components. It provides methods to assess how well different movement signals align in time and phase . Note The method assumes stationarity over the analysis window and Gaussian noise.","title":"Synchronization Analysis Module"},{"location":"user_guide/theoretical_framework/analysis_primitives/synchronization/#algorithms-details","text":"","title":"Algorithms Details"},{"location":"user_guide/theoretical_framework/analysis_primitives/synchronization/#phase-coupling-analysis","text":"Phase synchronization measures temporal alignment independent of amplitude: Hilbert Transform : Compute analytic signals \\[ z_x(t) = x(t) + i H[x(t)], \\quad z_y(t) = y(t) + i H[y(t)] \\] Instantaneous Phases : \\[ \\phi_x(t) = \\arg(z_x(t)), \\quad \\phi_y(t) = \\arg(z_y(t)) \\] Phase Difference : \\[ \\Delta \\phi(t) = \\phi_x(t) - \\phi_y(t) \\] Phase Locking Value (PLV) : \\[ \\text{PLV} = \\left| \\frac{1}{N} \\sum_{t=1}^{N} e^{i \\Delta \\phi(t)} \\right| \\] Output: \\( \\text{PLV} \\in [0,1] \\) Interpretation \\( PLV > 0.7 \\) : strong phase coupling \\( 0.4 < PLV \\leq 0.7 \\) : moderate phase coupling \\( PLV \\leq 0.4 \\) : weak phase coupling","title":"Phase Coupling Analysis"},{"location":"user_guide/theoretical_framework/analysis_primitives/synchronization/#references","text":"To be added","title":"References"},{"location":"user_guide/theoretical_framework/high_level/high_level/","text":"/* 1. First Column: Allow wrapping and set modest width */ .rst-content table.docutils td:nth-child(1) { width: 20%; /* Fixed width for the feature name */ white-space: normal !important; /* ALLOWS wrapping */ word-wrap: break-word; /* Breaks long words if necessary */ } /* 2. Second Column: Maximize width */ .rst-content table.docutils td:nth-child(2) { white-space: normal !important; word-wrap: break-word !important; width: 65%; /* Takes up the majority of the table */ } /* 3. Third Column: Keep it tight (Optional but recommended) */ .rst-content table.docutils td:nth-child(3) { width: 10%; white-space: nowrap; text-align: center; } /* 4. Fix the Huge Icons */ .rst-content .twemoji { height: 1.2em !important; width: 1.2em !important; vertical-align: text-bottom; } Layer 4 \u2013 Expressive Qualities The highest layer focuses on how an observer perceives movement , connecting computational features with human-centered interpretation . It addresses nonverbal communication, emotions, and intentions conveyed through movement, supporting cross-modal experiences (e.g., \u201clistening to a choreography\u201d) and enabling applications in art, therapy, rehabilitation, and HCI Key Concepts Observer perspective : perception, not physical effort, defines qualities. Memory and context : recent history influences interpretation (e.g., expectancy, contrast, saliency). Machine learning : used to map mid-level trajectories to expressive qualities. Examples of Expressive Qualities Quality Description Implemented Predictability / Expectancy Extent to which movement can be anticipated by an observer. Hesitation When intention behind movement is unclear to an observer. Attraction / Repulsion Degree to which an observer feels drawn to or repelled by the movement. Groove Extent to which movement elicits movement in the observer. Saliency How a movement stands out compared to others in context. Emotion Expressive emotional content conveyed via body movement (categorical or dimensional). References TODO","title":"Layer 4 - High-Level Features"},{"location":"user_guide/theoretical_framework/high_level/high_level/#layer-4-expressive-qualities","text":"The highest layer focuses on how an observer perceives movement , connecting computational features with human-centered interpretation . It addresses nonverbal communication, emotions, and intentions conveyed through movement, supporting cross-modal experiences (e.g., \u201clistening to a choreography\u201d) and enabling applications in art, therapy, rehabilitation, and HCI Key Concepts Observer perspective : perception, not physical effort, defines qualities. Memory and context : recent history influences interpretation (e.g., expectancy, contrast, saliency). Machine learning : used to map mid-level trajectories to expressive qualities.","title":"Layer 4 \u2013 Expressive Qualities"},{"location":"user_guide/theoretical_framework/high_level/high_level/#examples-of-expressive-qualities","text":"Quality Description Implemented Predictability / Expectancy Extent to which movement can be anticipated by an observer. Hesitation When intention behind movement is unclear to an observer. Attraction / Repulsion Degree to which an observer feels drawn to or repelled by the movement. Groove Extent to which movement elicits movement in the observer. Saliency How a movement stands out compared to others in context. Emotion Expressive emotional content conveyed via body movement (categorical or dimensional).","title":"Examples of Expressive Qualities"},{"location":"user_guide/theoretical_framework/high_level/high_level/#references","text":"TODO","title":"References"},{"location":"user_guide/theoretical_framework/low_level/contraction_expansion/","text":"Contraction-Expansion Analysis Module The Contraction-Expansion module quantifies spatial dynamics of movement patterns by analyzing how trajectories expand and contract in space over time. Spatial movement analysis examines how body segments or markers change their relative positions during movement. The module provides: 2D/3D area calculations : geometric area enclosed by trajectory points Volume analysis : 3D spatial volume changes Expansion-contraction rates : temporal derivatives of spatial measures Algorithms Details 2D Area computation The 2D implementation uses the Shoelace formula 1 for polygon area computation: \\[ A = \\tfrac{1}{2} \\Bigg| \\sum_i \\big( x_i y_{i+1} - x_{i+1} y_i \\big) \\Bigg| \\] 3D Volume computation Calculates 3D volume using tetrahedron volume decomposition: Divide the 3D shape into tetrahedra For a polyhedron defined by vertices \\( \\{p_i\\}_{i=0}^n \\) , partition it into a set of tetrahedra \\(\\mathcal{T} = \\{ (p_0, p_{i}, p_{j}, p_{k}) \\}\\) . Calculate each tetrahedron volume using the scalar triple product For tetrahedron with vertices \\(p_0, p_1, p_2, p_3 \\in \\mathbb{R}^3\\) \\[ V_{\\text{tetra}} = \\frac{1}{6} \\; \\det \\begin{bmatrix} x_1 - x_0 & x_2 - x_0 & x_3 - x_0 \\\\ y_1 - y_0 & y_2 - y_0 & y_3 - y_0 \\\\ z_1 - z_0 & z_2 - z_0 & z_3 - z_0 \\end{bmatrix} \\] or equivalently: \\[ V_{\\text{tetra}} = \\tfrac{1}{6} \\, \\big( (p_1 - p_0) \\cdot \\big( (p_2 - p_0) \\times (p_3 - p_0) \\big) \\big) \\] Sum volumes with appropriate signs The total polyhedron volume is obtained as: \\[ V = \\sum_{k} V_{\\text{tetra},k} \\] where the sign of each \\(V_{\\text{tetra},k}\\) depends on the orientation of the vertices. Performance Optimization Numba JIT Compilation The module uses Numba's Just-In-Time compilation for performance. Note Numba introduces compilation time on first use, but caches compiled functions for subsequent calls. This approach grants 10-100x speedups over pure Python, avoiding overhead in critical loops. @jit ( nopython = True , cache = True ) def _area_2d_fast ( points ): # Ultra-fast computation with compiled code pass Memory Efficiency The implementation is optimized for memory efficiency by using in-place calculations, minimizing temporary arrays, and leveraging efficient array operations. Limitations & Considerations Assumes meaningful geometric shapes from selected points. Requires consistent point topology across frames. May not capture the full spatial complexity of movement. Performance depends on point selection strategy . Always consider the movement context when interpreting results. Normalize for subject/task differences when appropriate. References TODO \u21a9","title":"Contraction-Expansion Analysis Module"},{"location":"user_guide/theoretical_framework/low_level/contraction_expansion/#contraction-expansion-analysis-module","text":"The Contraction-Expansion module quantifies spatial dynamics of movement patterns by analyzing how trajectories expand and contract in space over time. Spatial movement analysis examines how body segments or markers change their relative positions during movement. The module provides: 2D/3D area calculations : geometric area enclosed by trajectory points Volume analysis : 3D spatial volume changes Expansion-contraction rates : temporal derivatives of spatial measures","title":"Contraction-Expansion Analysis Module"},{"location":"user_guide/theoretical_framework/low_level/contraction_expansion/#algorithms-details","text":"","title":"Algorithms Details"},{"location":"user_guide/theoretical_framework/low_level/contraction_expansion/#2d-area-computation","text":"The 2D implementation uses the Shoelace formula 1 for polygon area computation: \\[ A = \\tfrac{1}{2} \\Bigg| \\sum_i \\big( x_i y_{i+1} - x_{i+1} y_i \\big) \\Bigg| \\]","title":"2D Area computation"},{"location":"user_guide/theoretical_framework/low_level/contraction_expansion/#3d-volume-computation","text":"Calculates 3D volume using tetrahedron volume decomposition: Divide the 3D shape into tetrahedra For a polyhedron defined by vertices \\( \\{p_i\\}_{i=0}^n \\) , partition it into a set of tetrahedra \\(\\mathcal{T} = \\{ (p_0, p_{i}, p_{j}, p_{k}) \\}\\) . Calculate each tetrahedron volume using the scalar triple product For tetrahedron with vertices \\(p_0, p_1, p_2, p_3 \\in \\mathbb{R}^3\\) \\[ V_{\\text{tetra}} = \\frac{1}{6} \\; \\det \\begin{bmatrix} x_1 - x_0 & x_2 - x_0 & x_3 - x_0 \\\\ y_1 - y_0 & y_2 - y_0 & y_3 - y_0 \\\\ z_1 - z_0 & z_2 - z_0 & z_3 - z_0 \\end{bmatrix} \\] or equivalently: \\[ V_{\\text{tetra}} = \\tfrac{1}{6} \\, \\big( (p_1 - p_0) \\cdot \\big( (p_2 - p_0) \\times (p_3 - p_0) \\big) \\big) \\] Sum volumes with appropriate signs The total polyhedron volume is obtained as: \\[ V = \\sum_{k} V_{\\text{tetra},k} \\] where the sign of each \\(V_{\\text{tetra},k}\\) depends on the orientation of the vertices.","title":"3D Volume computation"},{"location":"user_guide/theoretical_framework/low_level/contraction_expansion/#performance-optimization","text":"","title":"Performance Optimization"},{"location":"user_guide/theoretical_framework/low_level/contraction_expansion/#numba-jit-compilation","text":"The module uses Numba's Just-In-Time compilation for performance. Note Numba introduces compilation time on first use, but caches compiled functions for subsequent calls. This approach grants 10-100x speedups over pure Python, avoiding overhead in critical loops. @jit ( nopython = True , cache = True ) def _area_2d_fast ( points ): # Ultra-fast computation with compiled code pass","title":"Numba JIT Compilation"},{"location":"user_guide/theoretical_framework/low_level/contraction_expansion/#memory-efficiency","text":"The implementation is optimized for memory efficiency by using in-place calculations, minimizing temporary arrays, and leveraging efficient array operations. Limitations & Considerations Assumes meaningful geometric shapes from selected points. Requires consistent point topology across frames. May not capture the full spatial complexity of movement. Performance depends on point selection strategy . Always consider the movement context when interpreting results. Normalize for subject/task differences when appropriate.","title":"Memory Efficiency"},{"location":"user_guide/theoretical_framework/low_level/contraction_expansion/#references","text":"TODO \u21a9","title":"References"},{"location":"user_guide/theoretical_framework/low_level/low_level/","text":"/* 1. First Column: Allow wrapping and set modest width */ .rst-content table.docutils td:nth-child(1) { width: 35%; /* Fixed width for the feature name */ white-space: normal !important; /* ALLOWS wrapping */ word-wrap: break-word; /* Breaks long words if necessary */ } /* 2. Second Column: Maximize width */ .rst-content table.docutils td:nth-child(2) { white-space: normal !important; word-wrap: break-word !important; width: 65%; /* Takes up the majority of the table */ } /* 3. Third Column: Keep it tight (Optional but recommended) */ .rst-content table.docutils td:nth-child(3) { width: 10%; white-space: nowrap; text-align: center; } /* 4. Fix the Huge Icons */ .rst-content .twemoji { height: 1.2em !important; width: 1.2em !important; vertical-align: text-bottom; } Layer 2 \u2013 Low-Level Features Low-level features are instantaneous descriptors of movement, usually computed directly from raw data (Layer 1) or from short sliding windows of samples (~0.5s). They are typically represented as time-series with the same sampling rate as the input signals. Examples of Low-Level Features Feature Description Implemented Kinectic Energy / Quantity of Motion (QoM) 1 2 Energy of a cloud of moving joints, weighted by their masses or area of the difference between consecutive silhouettes in consecutive frames Postural Contraction 1 2 Extent to which body posture is close to its barycenter. Smoothness 3 Motion of a joint according to biomechanics laws of smoothness. Equilibrium 4 Projection of the body\u2019s barycenter onto the floor within the support area of the feet Postural Tension 5 Vector describing angular relations between feet, hips, trunk, shoulders, and head; inspired by angles in classical painting/sculpture used to express tension. References Glowinski, D., Dael, N., Camurri, A., Volpe, G., Mortillaro, M., & Scherer, K. (2011). Toward a minimal representation of affective gestures. IEEE Transactions on Affective Computing, 2(2), 106-118. \u21a9 \u21a9 Camurri, A., Lagerl\u00f6f, I., & Volpe, G. (2003). Recognizing emotion from dance movement: comparison of spectator recognition and automated techniques. International journal of human-computer studies, 59(1-2), 213-225. \u21a9 \u21a9 Mazzarino, B., & Mancini, M. (2009). The need for impulsivity & smoothness: improving hci by qualitatively measuring new high-level human motion features. In Proceedings of the International Conference on Signal Processing and Multimedia Applications (IEEE sponsored). \u21a9 Ghisio, S., Coletta, P., Piana, S., Alborno, P., Volpe, G., Camurri, A., ... & Ravaschio, A. (2015, June). An open platform for full body interactive sonification exergames. In 2015 7th International Conference on Intelligent Technologies for Interactive Entertainment (INTETAIN) (pp. 168-175). IEEE. \u21a9 Camurri, Volpe, Piana, Mancini, Alborno, Ghisio (2018) The Energy Lift: automated measurement of postural tension and energy transmission. Proc. MOCO 2018 \u21a9","title":"Layer 2 - Low-Level Features"},{"location":"user_guide/theoretical_framework/low_level/low_level/#layer-2-low-level-features","text":"Low-level features are instantaneous descriptors of movement, usually computed directly from raw data (Layer 1) or from short sliding windows of samples (~0.5s). They are typically represented as time-series with the same sampling rate as the input signals.","title":"Layer 2 \u2013 Low-Level Features"},{"location":"user_guide/theoretical_framework/low_level/low_level/#examples-of-low-level-features","text":"Feature Description Implemented Kinectic Energy / Quantity of Motion (QoM) 1 2 Energy of a cloud of moving joints, weighted by their masses or area of the difference between consecutive silhouettes in consecutive frames Postural Contraction 1 2 Extent to which body posture is close to its barycenter. Smoothness 3 Motion of a joint according to biomechanics laws of smoothness. Equilibrium 4 Projection of the body\u2019s barycenter onto the floor within the support area of the feet Postural Tension 5 Vector describing angular relations between feet, hips, trunk, shoulders, and head; inspired by angles in classical painting/sculpture used to express tension.","title":"Examples of Low-Level Features"},{"location":"user_guide/theoretical_framework/low_level/low_level/#references","text":"Glowinski, D., Dael, N., Camurri, A., Volpe, G., Mortillaro, M., & Scherer, K. (2011). Toward a minimal representation of affective gestures. IEEE Transactions on Affective Computing, 2(2), 106-118. \u21a9 \u21a9 Camurri, A., Lagerl\u00f6f, I., & Volpe, G. (2003). Recognizing emotion from dance movement: comparison of spectator recognition and automated techniques. International journal of human-computer studies, 59(1-2), 213-225. \u21a9 \u21a9 Mazzarino, B., & Mancini, M. (2009). The need for impulsivity & smoothness: improving hci by qualitatively measuring new high-level human motion features. In Proceedings of the International Conference on Signal Processing and Multimedia Applications (IEEE sponsored). \u21a9 Ghisio, S., Coletta, P., Piana, S., Alborno, P., Volpe, G., Camurri, A., ... & Ravaschio, A. (2015, June). An open platform for full body interactive sonification exergames. In 2015 7th International Conference on Intelligent Technologies for Interactive Entertainment (INTETAIN) (pp. 168-175). IEEE. \u21a9 Camurri, Volpe, Piana, Mancini, Alborno, Ghisio (2018) The Energy Lift: automated measurement of postural tension and energy transmission. Proc. MOCO 2018 \u21a9","title":"References"},{"location":"user_guide/theoretical_framework/low_level/postural_balance/","text":"Equilibrium Analysis Module The Equilibrium module quantifies balance control by evaluating the barycenter position relative to an elliptical region defined by the two feet. The metric produces a normalized equilibrium value in \\([0, 1]\\) indicating whether the barycenter lies within the ellipse, and the ellipse orientation angle in degrees. Algorithms Details Ellipse Construction Inputs : Left foot position \\( p_s = (x_s, y_s) \\) Right foot position \\( p_d = (x_d, y_d) \\) Barycenter \\( b = (x_b, y_b) \\) Margin \\( m \\) (mm) Y-axis weight \\( w_y \\) Bounding box with margin : \\[ \\text{min} = \\min(p_s, p_d) - m, \\quad \\text{max} = \\max(p_s, p_d) + m \\] Ellipse center : \\[ c = \\frac{\\text{min} + \\text{max}}{2} \\] Ellipse Semi-axes : \\[ a = \\frac{\\text{max}_x - \\text{min}_x}{2}, \\quad b = \\frac{\\text{max}_y - \\text{min}_y}{2} \\cdot w_y \\] Ellipse orientation \\[ \\theta = \\arctan2(y_d - y_s, \\; x_d - x_s) \\] Interpretation The angle \\( \\theta \\) aligns the ellipse relative to the X-axis (the line connecting the feet). Normalized Value Computation 1. Relative position : Rotate barycenter into ellipse-aligned coordinates \\[ r = R(-\\theta) \\cdot (b - c) \\] with rotation matrix \\[ R(-\\theta) = \\begin{bmatrix} \\cos(-\\theta) & -\\sin(-\\theta) \\\\ \\sin(-\\theta) & \\cos(-\\theta) \\end{bmatrix} \\] Normalization \\[ \\text{norm} = \\left(\\frac{r_x}{a}\\right)^2 + \\left(\\frac{r_y}{b}\\right)^2 \\] Equilibrium value \\[ \\text{value} = \\begin{cases} 1 - \\sqrt{\\text{norm}}, & \\text{if } \\text{norm} \\leq 1 \\\\ 0, & \\text{otherwise} \\end{cases} \\] Interpretation Value = 1 : barycenter at ellipse center (optimal balance). Value = 0 : barycenter outside ellipse (loss of balance). Values in between indicate proximity to the center.","title":"Postural Balance"},{"location":"user_guide/theoretical_framework/low_level/postural_balance/#equilibrium-analysis-module","text":"The Equilibrium module quantifies balance control by evaluating the barycenter position relative to an elliptical region defined by the two feet. The metric produces a normalized equilibrium value in \\([0, 1]\\) indicating whether the barycenter lies within the ellipse, and the ellipse orientation angle in degrees.","title":"Equilibrium Analysis Module"},{"location":"user_guide/theoretical_framework/low_level/postural_balance/#algorithms-details","text":"","title":"Algorithms Details"},{"location":"user_guide/theoretical_framework/low_level/postural_balance/#ellipse-construction","text":"Inputs : Left foot position \\( p_s = (x_s, y_s) \\) Right foot position \\( p_d = (x_d, y_d) \\) Barycenter \\( b = (x_b, y_b) \\) Margin \\( m \\) (mm) Y-axis weight \\( w_y \\) Bounding box with margin : \\[ \\text{min} = \\min(p_s, p_d) - m, \\quad \\text{max} = \\max(p_s, p_d) + m \\] Ellipse center : \\[ c = \\frac{\\text{min} + \\text{max}}{2} \\] Ellipse Semi-axes : \\[ a = \\frac{\\text{max}_x - \\text{min}_x}{2}, \\quad b = \\frac{\\text{max}_y - \\text{min}_y}{2} \\cdot w_y \\] Ellipse orientation \\[ \\theta = \\arctan2(y_d - y_s, \\; x_d - x_s) \\] Interpretation The angle \\( \\theta \\) aligns the ellipse relative to the X-axis (the line connecting the feet).","title":"Ellipse Construction"},{"location":"user_guide/theoretical_framework/low_level/postural_balance/#normalized-value-computation","text":"1. Relative position : Rotate barycenter into ellipse-aligned coordinates \\[ r = R(-\\theta) \\cdot (b - c) \\] with rotation matrix \\[ R(-\\theta) = \\begin{bmatrix} \\cos(-\\theta) & -\\sin(-\\theta) \\\\ \\sin(-\\theta) & \\cos(-\\theta) \\end{bmatrix} \\] Normalization \\[ \\text{norm} = \\left(\\frac{r_x}{a}\\right)^2 + \\left(\\frac{r_y}{b}\\right)^2 \\] Equilibrium value \\[ \\text{value} = \\begin{cases} 1 - \\sqrt{\\text{norm}}, & \\text{if } \\text{norm} \\leq 1 \\\\ 0, & \\text{otherwise} \\end{cases} \\] Interpretation Value = 1 : barycenter at ellipse center (optimal balance). Value = 0 : barycenter outside ellipse (loss of balance). Values in between indicate proximity to the center.","title":"Normalized Value Computation"},{"location":"user_guide/theoretical_framework/low_level/smoothness/","text":"Smoothness Analysis Module The Smoothness module quantifies control using established motor control metrics. Smooth movements are characterized by continuous, coordinated trajectories with minimal abrupt changes. The module implements two primary metrics validated in motor control research 1 2 : Spectral Arc Length (SPARC) : frequency domain smoothness measure. Jerk Root Mean Square (RMS) : time domain measure based on movement derivatives. Algorithms Details SPARC Calculation The Spectral Arc Length (SPARC) quantifies movement smoothness by measuring the arc length of the normalized Fourier magnitude spectrum of the signal. Compute the Fourier magnitude spectrum Take the Fast Fourier Transform (FFT) of the input signal \\( s(t) \\) and keep only the positive frequencies: \\[ Y(f) = |\\text{FFT}[s(t)]|_{f > 0} \\] Normalize the spectrum Normalize the magnitude by its maximum value: \\[ \\hat{Y}(f) = \\frac{Y(f)}{\\max(Y(f))} \\] Calculate arc length Compute the total Euclidean arc length of the normalized spectrum: \\[ L = \\sum_{i=1}^{N-1} \\sqrt{(f_{i+1} - f_i)^2 + (\\hat{Y}_{i+1} - \\hat{Y}_i)^2} \\] Return negative arc length Define SPARC as: \\[ \\text{SPARC} = -L \\] Tip SPARC values are negative , where more negative values indicate smoother movement . Constant signals or signals with no variation return NaN (undefined smoothness). Jerk RMS Calculation The Jerk Root Mean Square (RMS) measures smoothness as the average magnitude of the finite-difference derivative of the signal. Compute discrete derivative Approximate the first derivative using finite differences with sampling rate \\( f_s \\) : \\[ j_i = \\frac{s_{i+1} - s_i}{1 / f_s} \\] Compute RMS of derivative (jerk) values : For N samples, \\[ \\text{RMS}_j = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} j_i^2} \\] Tip Lower RMS jerk values indicate smoother motion. In this implementation, the function directly differentiates the provided signal once. Hence, the algorithm expects to receive accelerations as input; if you input position , you get velocity RMS . References Balasubramanian, S., Melendez-Calderon, A., Roby-Brami, A., & Burdet, E. (2015). On the analysis of movement smoothness. Journal of neuroengineering and rehabilitation, 12(1), 112. \u21a9 Melendez-Calderon, A., Shirota, C., & Balasubramanian, S. (2021). Estimating movement smoothness from inertial measurement units. Frontiers in bioengineering and biotechnology, 8, 558771. \u21a9","title":"Smoothness"},{"location":"user_guide/theoretical_framework/low_level/smoothness/#smoothness-analysis-module","text":"The Smoothness module quantifies control using established motor control metrics. Smooth movements are characterized by continuous, coordinated trajectories with minimal abrupt changes. The module implements two primary metrics validated in motor control research 1 2 : Spectral Arc Length (SPARC) : frequency domain smoothness measure. Jerk Root Mean Square (RMS) : time domain measure based on movement derivatives.","title":"Smoothness Analysis Module"},{"location":"user_guide/theoretical_framework/low_level/smoothness/#algorithms-details","text":"","title":"Algorithms Details"},{"location":"user_guide/theoretical_framework/low_level/smoothness/#sparc-calculation","text":"The Spectral Arc Length (SPARC) quantifies movement smoothness by measuring the arc length of the normalized Fourier magnitude spectrum of the signal. Compute the Fourier magnitude spectrum Take the Fast Fourier Transform (FFT) of the input signal \\( s(t) \\) and keep only the positive frequencies: \\[ Y(f) = |\\text{FFT}[s(t)]|_{f > 0} \\] Normalize the spectrum Normalize the magnitude by its maximum value: \\[ \\hat{Y}(f) = \\frac{Y(f)}{\\max(Y(f))} \\] Calculate arc length Compute the total Euclidean arc length of the normalized spectrum: \\[ L = \\sum_{i=1}^{N-1} \\sqrt{(f_{i+1} - f_i)^2 + (\\hat{Y}_{i+1} - \\hat{Y}_i)^2} \\] Return negative arc length Define SPARC as: \\[ \\text{SPARC} = -L \\] Tip SPARC values are negative , where more negative values indicate smoother movement . Constant signals or signals with no variation return NaN (undefined smoothness).","title":"SPARC Calculation"},{"location":"user_guide/theoretical_framework/low_level/smoothness/#jerk-rms-calculation","text":"The Jerk Root Mean Square (RMS) measures smoothness as the average magnitude of the finite-difference derivative of the signal. Compute discrete derivative Approximate the first derivative using finite differences with sampling rate \\( f_s \\) : \\[ j_i = \\frac{s_{i+1} - s_i}{1 / f_s} \\] Compute RMS of derivative (jerk) values : For N samples, \\[ \\text{RMS}_j = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} j_i^2} \\] Tip Lower RMS jerk values indicate smoother motion. In this implementation, the function directly differentiates the provided signal once. Hence, the algorithm expects to receive accelerations as input; if you input position , you get velocity RMS .","title":"Jerk RMS Calculation"},{"location":"user_guide/theoretical_framework/low_level/smoothness/#references","text":"Balasubramanian, S., Melendez-Calderon, A., Roby-Brami, A., & Burdet, E. (2015). On the analysis of movement smoothness. Journal of neuroengineering and rehabilitation, 12(1), 112. \u21a9 Melendez-Calderon, A., Shirota, C., & Balasubramanian, S. (2021). Estimating movement smoothness from inertial measurement units. Frontiers in bioengineering and biotechnology, 8, 558771. \u21a9","title":"References"},{"location":"user_guide/theoretical_framework/mid_level/mid_level/","text":"/* 1. First Column: Allow wrapping and set modest width */ .rst-content table.docutils td:nth-child(1) { width: 20%; /* Fixed width for the feature name */ white-space: normal !important; /* ALLOWS wrapping */ word-wrap: break-word; /* Breaks long words if necessary */ } /* 2. Second Column: Maximize width */ .rst-content table.docutils td:nth-child(2) { white-space: normal !important; word-wrap: break-word !important; width: 65%; /* Takes up the majority of the table */ } /* 3. Third Column: Keep it tight (Optional but recommended) */ .rst-content table.docutils td:nth-child(3) { width: 10%; white-space: nowrap; text-align: center; } /* 4. Fix the Huge Icons */ .rst-content .twemoji { height: 1.2em !important; width: 1.2em !important; vertical-align: text-bottom; } Layer 3 \u2013 Mid-Level Features Mid-level features capture structural properties of movement across units or time windows. They operate at a higher abstraction than low-level descriptors, often integrating multiple signals into amodal features . Key Concepts Segmentation : movements are divided into units that depends on the context (e.g., technical gestures in sport, choreographic phrases) or analyzed over defined windows (e.g., 0.5s - 3s). Amodal descriptors : features meaningful across modalities (e.g., movement and audio). Trajectories in feature space : sequences of values describing movement dynamics in multidimensional spaces. Examples of Mid-Level Features Feature Description Implemented Directness 1 Straight vs. flexible trajectory toward a target (Laban\u2019s Space). Lightness 2 3 Influence of gravity on movement (vertical vs. horizontal acceleration). Impulsivity 4 5 Abrupt movement without preparation by antagonist muscles. Fluidity 6 7 Smooth, wave-like propagation of movement across joints. Fragility 2 3 Vulnerability and delicacy in movement. References Piana, S., Staglian\u00f2, A., Camurri, A., & Odone, F. (2013). A set of full-body movement features for emotion recognition to help children affected by autism spectrum condition. In IDGEI International Workshop (Vol. 23). \u21a9 Niewiadomski, R., Mancini, M., Piana, S., Alborno, P., Volpe, G., & Camurri, A. (2017). Low-intrusive recognition of expressive movement qualities. In Proceedings of the 19th ACM international conference on multimodal interaction (pp. 230-237).) \u21a9 \u21a9 Niewiadomski, R., Mancini, M., Cera, A., Piana, S., Canepa, C., & Camurri, A. (2019). Does embodied training improve the recognition of mid-level expressive movement qualities sonification?. Journal on Multimodal User Interfaces, 13, 191-203. \u21a9 \u21a9 Mazzarino, B., & Mancini, M. (2009). The need for impulsivity & smoothness: improving hci by qualitatively measuring new high-level human motion features. In Proceedings of the International Conference on Signal Processing and Multimedia Applications. \u21a9 Niewiadomski, R., Mancini, M., Volpe, G., & Camurri, A. (2015). Automated detection of impulsive movements in HCI. In Proceedings of the 11th Biannual Conference of the Italian SIGCHI Chapter (pp. 166-169). \u21a9 Piana, S., Alborno, P., Niewiadomski, R., Mancini, M., Volpe, G., & Camurri, A. (2016). Movement fluidity analysis based on performance and perception. In Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems (pp. 1629-1636). \u21a9 Alborno, P., Cera, A., Piana, S., Mancini, M., Niewiadomski, R., Canepa, C., Volpe G. & Camurri, A. (2016). Interactive sonification of movement qualities\u2013a case study on fluidity. Proceedings of ISon, 35. \u21a9","title":"Layer 3 - Mid-Level Features"},{"location":"user_guide/theoretical_framework/mid_level/mid_level/#layer-3-mid-level-features","text":"Mid-level features capture structural properties of movement across units or time windows. They operate at a higher abstraction than low-level descriptors, often integrating multiple signals into amodal features . Key Concepts Segmentation : movements are divided into units that depends on the context (e.g., technical gestures in sport, choreographic phrases) or analyzed over defined windows (e.g., 0.5s - 3s). Amodal descriptors : features meaningful across modalities (e.g., movement and audio). Trajectories in feature space : sequences of values describing movement dynamics in multidimensional spaces.","title":"Layer 3 \u2013 Mid-Level Features"},{"location":"user_guide/theoretical_framework/mid_level/mid_level/#examples-of-mid-level-features","text":"Feature Description Implemented Directness 1 Straight vs. flexible trajectory toward a target (Laban\u2019s Space). Lightness 2 3 Influence of gravity on movement (vertical vs. horizontal acceleration). Impulsivity 4 5 Abrupt movement without preparation by antagonist muscles. Fluidity 6 7 Smooth, wave-like propagation of movement across joints. Fragility 2 3 Vulnerability and delicacy in movement.","title":"Examples of Mid-Level Features"},{"location":"user_guide/theoretical_framework/mid_level/mid_level/#references","text":"Piana, S., Staglian\u00f2, A., Camurri, A., & Odone, F. (2013). A set of full-body movement features for emotion recognition to help children affected by autism spectrum condition. In IDGEI International Workshop (Vol. 23). \u21a9 Niewiadomski, R., Mancini, M., Piana, S., Alborno, P., Volpe, G., & Camurri, A. (2017). Low-intrusive recognition of expressive movement qualities. In Proceedings of the 19th ACM international conference on multimodal interaction (pp. 230-237).) \u21a9 \u21a9 Niewiadomski, R., Mancini, M., Cera, A., Piana, S., Canepa, C., & Camurri, A. (2019). Does embodied training improve the recognition of mid-level expressive movement qualities sonification?. Journal on Multimodal User Interfaces, 13, 191-203. \u21a9 \u21a9 Mazzarino, B., & Mancini, M. (2009). The need for impulsivity & smoothness: improving hci by qualitatively measuring new high-level human motion features. In Proceedings of the International Conference on Signal Processing and Multimedia Applications. \u21a9 Niewiadomski, R., Mancini, M., Volpe, G., & Camurri, A. (2015). Automated detection of impulsive movements in HCI. In Proceedings of the 11th Biannual Conference of the Italian SIGCHI Chapter (pp. 166-169). \u21a9 Piana, S., Alborno, P., Niewiadomski, R., Mancini, M., Volpe, G., & Camurri, A. (2016). Movement fluidity analysis based on performance and perception. In Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems (pp. 1629-1636). \u21a9 Alborno, P., Cera, A., Piana, S., Mancini, M., Niewiadomski, R., Canepa, C., Volpe G. & Camurri, A. (2016). Interactive sonification of movement qualities\u2013a case study on fluidity. Proceedings of ISon, 35. \u21a9","title":"References"},{"location":"user_guide/theoretical_framework/physical_signals/physical_signals/","text":"Layer 1 \u2013 Physical Signals Layer 1 is the foundation of the conceptual model . It deals with the raw data captured by physical sensors and their preprocessing into virtual sensors . Key Concepts A virtual sensor is a combination of: One or more physical sensors (e.g., MoCap, accelerometer, Kinect, respiration band). Signal conditioning (denoising, filtering, synchronization). Feature-specific extraction (e.g., 3D joint trajectories, silhouettes, barycenter). Examples of Physical & Virtual Sensors Virtual Sensor / Signal Description Implemented Kinematic 3D joint positions, trajectories, barycenter Optical Silhouette, RGB-D images, depth maps Inertial Accelerometers, gyroscopes Physiological EMG, EEG, ECG, respiration Pressure or Contact Ground reaction force, foot weight distribution Acoustic Breath, utterance, exhalation","title":"Layer 1 - Physical Signals"},{"location":"user_guide/theoretical_framework/physical_signals/physical_signals/#layer-1-physical-signals","text":"Layer 1 is the foundation of the conceptual model . It deals with the raw data captured by physical sensors and their preprocessing into virtual sensors . Key Concepts A virtual sensor is a combination of: One or more physical sensors (e.g., MoCap, accelerometer, Kinect, respiration band). Signal conditioning (denoising, filtering, synchronization). Feature-specific extraction (e.g., 3D joint trajectories, silhouettes, barycenter).","title":"Layer 1 \u2013 Physical Signals"},{"location":"user_guide/theoretical_framework/physical_signals/physical_signals/#examples-of-physical-virtual-sensors","text":"Virtual Sensor / Signal Description Implemented Kinematic 3D joint positions, trajectories, barycenter Optical Silhouette, RGB-D images, depth maps Inertial Accelerometers, gyroscopes Physiological EMG, EEG, ECG, respiration Pressure or Contact Ground reaction force, foot weight distribution Acoustic Breath, utterance, exhalation","title":"Examples of Physical &amp; Virtual Sensors"},{"location":"user_guide/theoretical_framework/utils/","text":"","title":"Index"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/","text":"TSV Reader Module The TSV Reader module provides efficient parsing and validation of motion capture data stored in Tab-Separated Values (TSV) format. It handles common motion capture file structures and provides data integrity checking. Overview Motion capture systems often export data in TSV format with specific structures: - Header information (marker names, coordinate labels) - Frame-by-frame coordinate data - Timestamp information - Quality indicators and metadata The TSV Reader module standardizes this data input for analysis pipeline integration. Class: TSVReader Constructor TSVReader ( file_path , validate_data = True , coordinate_system = 'xyz' ) Parameters: - file_path (str): Path to TSV file - validate_data (bool): Enable data validation and integrity checking - coordinate_system (str): Expected coordinate ordering ('xyz', 'xzy', etc.) Methods load_data() Loads complete TSV file into memory with validation. Returns: Dictionary containing: - 'data' : Numpy array of motion data (frames \u00d7 markers \u00d7 coordinates) - 'marker_names' : List of marker identifiers - 'frame_rate' : Sampling frequency (if available) - 'metadata' : Additional file information stream_frames(buffer_size=100) Generator for memory-efficient streaming of large files. Parameters: - buffer_size (int): Number of frames to buffer Yields: Frame dictionaries with marker coordinates and metadata get_marker_trajectory(marker_name) Extracts complete trajectory for a specific marker. Parameters: - marker_name (str): Name of target marker Returns: Numpy array of shape (n_frames, n_coordinates) File Format Specifications Standard TSV Structure # Header with metadata Frame Timestamp MARKER1_X MARKER1_Y MARKER1_Z MARKER2_X MARKER2_Y MARKER2_Z 1 0.000 10.5 20.3 15.2 8.7 18.9 14.1 2 0.020 10.6 20.4 15.3 8.8 19.0 14.2 ... Supported Variations Multiple coordinate systems (XYZ, XZY, YXZ, etc.) Optional quality indicators per marker Variable marker counts per frame Custom delimiter support Usage Examples Basic File Loading from pyeyesweb.utils.tsv_reader import TSVReader # Load complete motion capture file reader = TSVReader ( 'motion_capture_data.tsv' ) data = reader . load_data () print ( f \"Loaded { data [ 'data' ] . shape [ 0 ] } frames\" ) print ( f \"Markers: { data [ 'marker_names' ] } \" ) print ( f \"Frame rate: { data [ 'frame_rate' ] } Hz\" ) # Access specific marker data marker_trajectory = data [ 'data' ][:, 0 , :] # First marker, all coordinates Streaming Large Files def process_large_mocap_file ( file_path ): \"\"\"Process large motion capture files without loading into memory.\"\"\" reader = TSVReader ( file_path ) frame_count = 0 marker_velocities = [] previous_frame = None for frame in reader . stream_frames ( buffer_size = 50 ): frame_count += 1 if previous_frame is not None : # Calculate frame-to-frame velocity velocity = calculate_velocity ( previous_frame , frame ) marker_velocities . append ( velocity ) previous_frame = frame # Process in chunks to avoid memory overflow if frame_count % 1000 == 0 : print ( f \"Processed { frame_count } frames\" ) return marker_velocities # Usage with large files velocities = process_large_mocap_file ( 'large_motion_file.tsv' ) Marker-Specific Analysis def analyze_specific_markers ( file_path , target_markers ): \"\"\"Extract and analyze specific markers from motion data.\"\"\" reader = TSVReader ( file_path ) results = {} for marker_name in target_markers : try : trajectory = reader . get_marker_trajectory ( marker_name ) # Basic trajectory analysis results [ marker_name ] = { 'mean_position' : np . mean ( trajectory , axis = 0 ), 'position_std' : np . std ( trajectory , axis = 0 ), 'trajectory_length' : calculate_path_length ( trajectory ), 'velocity_profile' : calculate_velocity ( trajectory ) } except KeyError : print ( f \"Marker ' { marker_name } ' not found in file\" ) results [ marker_name ] = None return results # Analyze specific markers target_markers = [ 'HEAD' , 'LEFT_WRIST' , 'RIGHT_WRIST' ] marker_analysis = analyze_specific_markers ( 'motion_data.tsv' , target_markers ) Data Validation and Quality Assessment def validate_motion_data ( file_path ): \"\"\"Comprehensive validation of motion capture data quality.\"\"\" reader = TSVReader ( file_path , validate_data = True ) validation_report = { 'file_valid' : True , 'missing_frames' : [], 'outlier_markers' : [], 'discontinuities' : [], 'quality_summary' : {} } try : data = reader . load_data () # Check for missing data missing_mask = np . isnan ( data [ 'data' ]) if np . any ( missing_mask ): missing_frames = np . where ( np . any ( missing_mask , axis = ( 1 , 2 )))[ 0 ] validation_report [ 'missing_frames' ] = missing_frames . tolist () # Detect outliers using z-score for marker_idx , marker_name in enumerate ( data [ 'marker_names' ]): marker_data = data [ 'data' ][:, marker_idx , :] z_scores = np . abs ( stats . zscore ( marker_data , axis = 0 , nan_policy = 'omit' )) if np . any ( z_scores > 3 ): validation_report [ 'outlier_markers' ] . append ( marker_name ) # Check for large discontinuities velocities = np . diff ( data [ 'data' ], axis = 0 ) velocity_magnitudes = np . linalg . norm ( velocities , axis = 2 ) # Detect frames with unusually high velocities velocity_threshold = np . percentile ( velocity_magnitudes . flatten (), 99 ) discontinuous_frames = np . where ( np . any ( velocity_magnitudes > velocity_threshold , axis = 1 ) )[ 0 ] validation_report [ 'discontinuities' ] = discontinuous_frames . tolist () validation_report [ 'quality_summary' ] = { 'total_frames' : data [ 'data' ] . shape [ 0 ], 'missing_data_percentage' : np . sum ( missing_mask ) / missing_mask . size * 100 , 'outlier_markers_count' : len ( validation_report [ 'outlier_markers' ]), 'discontinuity_frames' : len ( validation_report [ 'discontinuities' ]) } except Exception as e : validation_report [ 'file_valid' ] = False validation_report [ 'error_message' ] = str ( e ) return validation_report # Validate file quality quality_report = validate_motion_data ( 'motion_capture_data.tsv' ) print ( f \"File valid: { quality_report [ 'file_valid' ] } \" ) print ( f \"Missing data: { quality_report [ 'quality_summary' ][ 'missing_data_percentage' ] : .2f } %\" ) Configuration Options Coordinate System Handling # Different coordinate system conventions readers = { 'standard' : TSVReader ( 'data.tsv' , coordinate_system = 'xyz' ), 'maya' : TSVReader ( 'maya_export.tsv' , coordinate_system = 'xzy' ), 'blender' : TSVReader ( 'blender_data.tsv' , coordinate_system = 'xyz' ) } # Automatic coordinate system detection reader = TSVReader ( 'unknown_system.tsv' , coordinate_system = 'auto' ) Custom Validation Rules def custom_validator ( data_frame ): \"\"\"Custom validation function for specific data requirements.\"\"\" # Check frame completeness if len ( data_frame ) < expected_marker_count * 3 : return False , \"Incomplete frame data\" # Check coordinate ranges coordinates = np . array ( data_frame ) . reshape ( - 1 , 3 ) if np . any ( np . abs ( coordinates ) > 1000 ): # 1 meter limit return False , \"Coordinates outside expected range\" return True , \"Valid frame\" # Use custom validation reader = TSVReader ( 'data.tsv' , custom_validator = custom_validator ) Performance Optimization Memory Management Streaming interface for large files Configurable buffer sizes Lazy loading of marker data Processing Speed NumPy-based operations Vectorized coordinate transformations Efficient string parsing Error Handling Graceful handling of malformed data Detailed error reporting Recovery from partial file corruption Integration with Analysis Modules Pipeline Integration def complete_analysis_pipeline ( tsv_file ): \"\"\"Complete motion analysis using TSV reader and analysis user_guide.\"\"\" # Load data reader = TSVReader ( tsv_file ) data = reader . load_data () # Initialize analysis user_guide smoothness = Smoothness ( rate_hz = data [ 'frame_rate' ]) bilateral_analyzer = BilateralSymmetryAnalyzer () results = {} # Analyze each marker for i , marker_name in enumerate ( data [ 'marker_names' ]): marker_trajectory = data [ 'data' ][:, i , :] # Smoothness analysis window = SlidingWindow ( window_size = 100 ) window . add_frames ( marker_trajectory ) results [ marker_name ] = { 'smoothness' : smoothness ( window ), 'trajectory_stats' : { 'mean_position' : np . mean ( marker_trajectory , axis = 0 ), 'movement_range' : np . ptp ( marker_trajectory , axis = 0 ) } } # Bilateral analysis (if applicable) left_markers = [ name for name in data [ 'marker_names' ] if 'LEFT' in name ] right_markers = [ name for name in data [ 'marker_names' ] if 'RIGHT' in name ] if left_markers and right_markers : bilateral_results = analyze_bilateral_coordination ( data , left_markers , right_markers , bilateral_analyzer ) results [ 'bilateral_analysis' ] = bilateral_results return results Common File Format Issues Missing Data Handling # Strategies for handling missing marker data def interpolate_missing_data ( trajectory ): \"\"\"Linear interpolation for short gaps in trajectory data.\"\"\" mask = ~ np . isnan ( trajectory ) . any ( axis = 1 ) if np . sum ( mask ) < 2 : return trajectory # Cannot interpolate valid_indices = np . where ( mask )[ 0 ] for coord_idx in range ( trajectory . shape [ 1 ]): trajectory [:, coord_idx ] = np . interp ( np . arange ( len ( trajectory )), valid_indices , trajectory [ valid_indices , coord_idx ] ) return trajectory Timestamp Synchronization # Handle irregular timestamps def regularize_timestamps ( data , target_fps = 50 ): \"\"\"Resample data to regular timestamps.\"\"\" original_times = data [ 'timestamps' ] target_times = np . arange ( 0 , original_times [ - 1 ], 1 / target_fps ) resampled_data = [] for marker_idx in range ( data [ 'data' ] . shape [ 1 ]): marker_trajectory = data [ 'data' ][:, marker_idx , :] resampled_trajectory = np . zeros (( len ( target_times ), 3 )) for coord_idx in range ( 3 ): resampled_trajectory [:, coord_idx ] = np . interp ( target_times , original_times , marker_trajectory [:, coord_idx ] ) resampled_data . append ( resampled_trajectory ) return { 'data' : np . stack ( resampled_data , axis = 1 ), 'timestamps' : target_times , 'frame_rate' : target_fps } Best Practices File Organization Consistent marker naming conventions Include metadata headers Regular timestamp intervals Quality indicators when available Data Preprocessing Validate data integrity before analysis Handle missing data appropriately Consider coordinate system transformations Apply appropriate filtering for noise reduction Performance Optimization Use streaming for large files Buffer size tuning for memory constraints Parallel processing for multiple files Efficient marker selection for targeted analysis","title":"TSV Reader Module"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#tsv-reader-module","text":"The TSV Reader module provides efficient parsing and validation of motion capture data stored in Tab-Separated Values (TSV) format. It handles common motion capture file structures and provides data integrity checking.","title":"TSV Reader Module"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#overview","text":"Motion capture systems often export data in TSV format with specific structures: - Header information (marker names, coordinate labels) - Frame-by-frame coordinate data - Timestamp information - Quality indicators and metadata The TSV Reader module standardizes this data input for analysis pipeline integration.","title":"Overview"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#class-tsvreader","text":"","title":"Class: TSVReader"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#constructor","text":"TSVReader ( file_path , validate_data = True , coordinate_system = 'xyz' ) Parameters: - file_path (str): Path to TSV file - validate_data (bool): Enable data validation and integrity checking - coordinate_system (str): Expected coordinate ordering ('xyz', 'xzy', etc.)","title":"Constructor"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#methods","text":"","title":"Methods"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#load_data","text":"Loads complete TSV file into memory with validation. Returns: Dictionary containing: - 'data' : Numpy array of motion data (frames \u00d7 markers \u00d7 coordinates) - 'marker_names' : List of marker identifiers - 'frame_rate' : Sampling frequency (if available) - 'metadata' : Additional file information","title":"load_data()"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#stream_framesbuffer_size100","text":"Generator for memory-efficient streaming of large files. Parameters: - buffer_size (int): Number of frames to buffer Yields: Frame dictionaries with marker coordinates and metadata","title":"stream_frames(buffer_size=100)"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#get_marker_trajectorymarker_name","text":"Extracts complete trajectory for a specific marker. Parameters: - marker_name (str): Name of target marker Returns: Numpy array of shape (n_frames, n_coordinates)","title":"get_marker_trajectory(marker_name)"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#file-format-specifications","text":"","title":"File Format Specifications"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#standard-tsv-structure","text":"# Header with metadata Frame Timestamp MARKER1_X MARKER1_Y MARKER1_Z MARKER2_X MARKER2_Y MARKER2_Z 1 0.000 10.5 20.3 15.2 8.7 18.9 14.1 2 0.020 10.6 20.4 15.3 8.8 19.0 14.2 ...","title":"Standard TSV Structure"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#supported-variations","text":"Multiple coordinate systems (XYZ, XZY, YXZ, etc.) Optional quality indicators per marker Variable marker counts per frame Custom delimiter support","title":"Supported Variations"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#usage-examples","text":"","title":"Usage Examples"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#basic-file-loading","text":"from pyeyesweb.utils.tsv_reader import TSVReader # Load complete motion capture file reader = TSVReader ( 'motion_capture_data.tsv' ) data = reader . load_data () print ( f \"Loaded { data [ 'data' ] . shape [ 0 ] } frames\" ) print ( f \"Markers: { data [ 'marker_names' ] } \" ) print ( f \"Frame rate: { data [ 'frame_rate' ] } Hz\" ) # Access specific marker data marker_trajectory = data [ 'data' ][:, 0 , :] # First marker, all coordinates","title":"Basic File Loading"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#streaming-large-files","text":"def process_large_mocap_file ( file_path ): \"\"\"Process large motion capture files without loading into memory.\"\"\" reader = TSVReader ( file_path ) frame_count = 0 marker_velocities = [] previous_frame = None for frame in reader . stream_frames ( buffer_size = 50 ): frame_count += 1 if previous_frame is not None : # Calculate frame-to-frame velocity velocity = calculate_velocity ( previous_frame , frame ) marker_velocities . append ( velocity ) previous_frame = frame # Process in chunks to avoid memory overflow if frame_count % 1000 == 0 : print ( f \"Processed { frame_count } frames\" ) return marker_velocities # Usage with large files velocities = process_large_mocap_file ( 'large_motion_file.tsv' )","title":"Streaming Large Files"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#marker-specific-analysis","text":"def analyze_specific_markers ( file_path , target_markers ): \"\"\"Extract and analyze specific markers from motion data.\"\"\" reader = TSVReader ( file_path ) results = {} for marker_name in target_markers : try : trajectory = reader . get_marker_trajectory ( marker_name ) # Basic trajectory analysis results [ marker_name ] = { 'mean_position' : np . mean ( trajectory , axis = 0 ), 'position_std' : np . std ( trajectory , axis = 0 ), 'trajectory_length' : calculate_path_length ( trajectory ), 'velocity_profile' : calculate_velocity ( trajectory ) } except KeyError : print ( f \"Marker ' { marker_name } ' not found in file\" ) results [ marker_name ] = None return results # Analyze specific markers target_markers = [ 'HEAD' , 'LEFT_WRIST' , 'RIGHT_WRIST' ] marker_analysis = analyze_specific_markers ( 'motion_data.tsv' , target_markers )","title":"Marker-Specific Analysis"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#data-validation-and-quality-assessment","text":"def validate_motion_data ( file_path ): \"\"\"Comprehensive validation of motion capture data quality.\"\"\" reader = TSVReader ( file_path , validate_data = True ) validation_report = { 'file_valid' : True , 'missing_frames' : [], 'outlier_markers' : [], 'discontinuities' : [], 'quality_summary' : {} } try : data = reader . load_data () # Check for missing data missing_mask = np . isnan ( data [ 'data' ]) if np . any ( missing_mask ): missing_frames = np . where ( np . any ( missing_mask , axis = ( 1 , 2 )))[ 0 ] validation_report [ 'missing_frames' ] = missing_frames . tolist () # Detect outliers using z-score for marker_idx , marker_name in enumerate ( data [ 'marker_names' ]): marker_data = data [ 'data' ][:, marker_idx , :] z_scores = np . abs ( stats . zscore ( marker_data , axis = 0 , nan_policy = 'omit' )) if np . any ( z_scores > 3 ): validation_report [ 'outlier_markers' ] . append ( marker_name ) # Check for large discontinuities velocities = np . diff ( data [ 'data' ], axis = 0 ) velocity_magnitudes = np . linalg . norm ( velocities , axis = 2 ) # Detect frames with unusually high velocities velocity_threshold = np . percentile ( velocity_magnitudes . flatten (), 99 ) discontinuous_frames = np . where ( np . any ( velocity_magnitudes > velocity_threshold , axis = 1 ) )[ 0 ] validation_report [ 'discontinuities' ] = discontinuous_frames . tolist () validation_report [ 'quality_summary' ] = { 'total_frames' : data [ 'data' ] . shape [ 0 ], 'missing_data_percentage' : np . sum ( missing_mask ) / missing_mask . size * 100 , 'outlier_markers_count' : len ( validation_report [ 'outlier_markers' ]), 'discontinuity_frames' : len ( validation_report [ 'discontinuities' ]) } except Exception as e : validation_report [ 'file_valid' ] = False validation_report [ 'error_message' ] = str ( e ) return validation_report # Validate file quality quality_report = validate_motion_data ( 'motion_capture_data.tsv' ) print ( f \"File valid: { quality_report [ 'file_valid' ] } \" ) print ( f \"Missing data: { quality_report [ 'quality_summary' ][ 'missing_data_percentage' ] : .2f } %\" )","title":"Data Validation and Quality Assessment"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#configuration-options","text":"","title":"Configuration Options"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#coordinate-system-handling","text":"# Different coordinate system conventions readers = { 'standard' : TSVReader ( 'data.tsv' , coordinate_system = 'xyz' ), 'maya' : TSVReader ( 'maya_export.tsv' , coordinate_system = 'xzy' ), 'blender' : TSVReader ( 'blender_data.tsv' , coordinate_system = 'xyz' ) } # Automatic coordinate system detection reader = TSVReader ( 'unknown_system.tsv' , coordinate_system = 'auto' )","title":"Coordinate System Handling"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#custom-validation-rules","text":"def custom_validator ( data_frame ): \"\"\"Custom validation function for specific data requirements.\"\"\" # Check frame completeness if len ( data_frame ) < expected_marker_count * 3 : return False , \"Incomplete frame data\" # Check coordinate ranges coordinates = np . array ( data_frame ) . reshape ( - 1 , 3 ) if np . any ( np . abs ( coordinates ) > 1000 ): # 1 meter limit return False , \"Coordinates outside expected range\" return True , \"Valid frame\" # Use custom validation reader = TSVReader ( 'data.tsv' , custom_validator = custom_validator )","title":"Custom Validation Rules"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#performance-optimization","text":"","title":"Performance Optimization"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#memory-management","text":"Streaming interface for large files Configurable buffer sizes Lazy loading of marker data","title":"Memory Management"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#processing-speed","text":"NumPy-based operations Vectorized coordinate transformations Efficient string parsing","title":"Processing Speed"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#error-handling","text":"Graceful handling of malformed data Detailed error reporting Recovery from partial file corruption","title":"Error Handling"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#integration-with-analysis-modules","text":"","title":"Integration with Analysis Modules"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#pipeline-integration","text":"def complete_analysis_pipeline ( tsv_file ): \"\"\"Complete motion analysis using TSV reader and analysis user_guide.\"\"\" # Load data reader = TSVReader ( tsv_file ) data = reader . load_data () # Initialize analysis user_guide smoothness = Smoothness ( rate_hz = data [ 'frame_rate' ]) bilateral_analyzer = BilateralSymmetryAnalyzer () results = {} # Analyze each marker for i , marker_name in enumerate ( data [ 'marker_names' ]): marker_trajectory = data [ 'data' ][:, i , :] # Smoothness analysis window = SlidingWindow ( window_size = 100 ) window . add_frames ( marker_trajectory ) results [ marker_name ] = { 'smoothness' : smoothness ( window ), 'trajectory_stats' : { 'mean_position' : np . mean ( marker_trajectory , axis = 0 ), 'movement_range' : np . ptp ( marker_trajectory , axis = 0 ) } } # Bilateral analysis (if applicable) left_markers = [ name for name in data [ 'marker_names' ] if 'LEFT' in name ] right_markers = [ name for name in data [ 'marker_names' ] if 'RIGHT' in name ] if left_markers and right_markers : bilateral_results = analyze_bilateral_coordination ( data , left_markers , right_markers , bilateral_analyzer ) results [ 'bilateral_analysis' ] = bilateral_results return results","title":"Pipeline Integration"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#common-file-format-issues","text":"","title":"Common File Format Issues"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#missing-data-handling","text":"# Strategies for handling missing marker data def interpolate_missing_data ( trajectory ): \"\"\"Linear interpolation for short gaps in trajectory data.\"\"\" mask = ~ np . isnan ( trajectory ) . any ( axis = 1 ) if np . sum ( mask ) < 2 : return trajectory # Cannot interpolate valid_indices = np . where ( mask )[ 0 ] for coord_idx in range ( trajectory . shape [ 1 ]): trajectory [:, coord_idx ] = np . interp ( np . arange ( len ( trajectory )), valid_indices , trajectory [ valid_indices , coord_idx ] ) return trajectory","title":"Missing Data Handling"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#timestamp-synchronization","text":"# Handle irregular timestamps def regularize_timestamps ( data , target_fps = 50 ): \"\"\"Resample data to regular timestamps.\"\"\" original_times = data [ 'timestamps' ] target_times = np . arange ( 0 , original_times [ - 1 ], 1 / target_fps ) resampled_data = [] for marker_idx in range ( data [ 'data' ] . shape [ 1 ]): marker_trajectory = data [ 'data' ][:, marker_idx , :] resampled_trajectory = np . zeros (( len ( target_times ), 3 )) for coord_idx in range ( 3 ): resampled_trajectory [:, coord_idx ] = np . interp ( target_times , original_times , marker_trajectory [:, coord_idx ] ) resampled_data . append ( resampled_trajectory ) return { 'data' : np . stack ( resampled_data , axis = 1 ), 'timestamps' : target_times , 'frame_rate' : target_fps }","title":"Timestamp Synchronization"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#best-practices","text":"","title":"Best Practices"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#file-organization","text":"Consistent marker naming conventions Include metadata headers Regular timestamp intervals Quality indicators when available","title":"File Organization"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#data-preprocessing","text":"Validate data integrity before analysis Handle missing data appropriately Consider coordinate system transformations Apply appropriate filtering for noise reduction","title":"Data Preprocessing"},{"location":"user_guide/theoretical_framework/utils/tsv_reader/#performance-optimization_1","text":"Use streaming for large files Buffer size tuning for memory constraints Parallel processing for multiple files Efficient marker selection for targeted analysis","title":"Performance Optimization"}]}