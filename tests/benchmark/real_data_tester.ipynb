{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df444472",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    base_dir = Path(os.getcwd())\n",
    "    test_files = sorted([f for f in base_dir.rglob(\"*smooth_*.tsv\")])\n",
    "    body_parts = {\"Arm\": [name+pos for name in [\"Wrist\",\"Forearm\",\"Triceps\"] for pos in [\"\",\"In\",\"Out\"]]}\n",
    "    body_parts[\"RArm\"] = [\"R\"+name for name in body_parts[\"Arm\"]]\n",
    "    body_parts[\"LArm\"] = [\"L\"+name for name in body_parts[\"Arm\"]]\n",
    "    body_parts[\"BothArms\"] = body_parts[\"RArm\"] + body_parts[\"LArm\"]\n",
    "    body_parts.pop(\"Arm\")\n",
    "\n",
    "    from utils import read_custom_tsv\n",
    "\n",
    "    # Prepare annotation mapping\n",
    "    annotations = {}\n",
    "    for i, file_test in enumerate(test_files):\n",
    "\n",
    "        # Determine smoothness\n",
    "        smoothness = \"NOsmooth\" not in file_test.name\n",
    "        # Determine markers\n",
    "        markers = None\n",
    "        metadata, df = read_custom_tsv(file_test)\n",
    "        marker_names = metadata[\"marker_names\"]    \n",
    "        allowed_keys = [bp for bp in body_parts.keys() if bp + \".tsv\" in file_test.name]\n",
    "        target_markers = [marker for marker in marker_names if any(any(mk == marker for mk in body_parts[bp]) for bp in allowed_keys)]\n",
    "        # New filename\n",
    "        new_name = f\"sample_{i}.tsv\"\n",
    "        \n",
    "        print(f\"Renaming {file_test.name} to {new_name}\")\n",
    "        # Rename file\n",
    "        movie_file = file_test.parent.joinpath(file_test.name.split('.')[0] + \".mp4\")\n",
    "        new_movie_name = f\"sample_{i}.mp4\"\n",
    "        movie_file.rename(movie_file.with_name(new_movie_name))\n",
    "        file_test.rename(file_test.with_name(new_name))\n",
    "        \n",
    "        # Add to annotations\n",
    "        annotations[new_name.split('.')[0]] = {\"markers\": target_markers, \"smoothness\": smoothness, \"original_mocap\": str(file_test.name.split('_')[0])}\n",
    "\n",
    "    # Save annotations\n",
    "    with open(test_files[-1].parent / \"annotations.json\", \"w\") as f:\n",
    "        json.dump(annotations, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c283093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming Ele0003_sample1_sync_BothArms.tsv to sample_0.tsv\n",
      "Renaming Ele0003_sample2_sync_BothArms.tsv to sample_1.tsv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "base_dir = Path(os.getcwd())\n",
    "test_files = sorted([f for f in base_dir.rglob(\"*sync_*.tsv\")])\n",
    "body_parts = {\"Arm\": [name+pos for name in [\"Wrist\",\"Forearm\",\"Triceps\"] for pos in [\"\",\"In\",\"Out\"]]}\n",
    "body_parts[\"RArm\"] = [\"R\"+name for name in body_parts[\"Arm\"]]\n",
    "body_parts[\"LArm\"] = [\"L\"+name for name in body_parts[\"Arm\"]]\n",
    "body_parts[\"BothArms\"] = body_parts[\"RArm\"] + body_parts[\"LArm\"]\n",
    "body_parts.pop(\"Arm\")\n",
    "\n",
    "from utils import read_custom_tsv\n",
    "\n",
    "# Prepare annotation mapping\n",
    "annotations = {}\n",
    "for i, file_test in enumerate(test_files):\n",
    "\n",
    "    # Determine synchronization\n",
    "    synch = \"sync\" in file_test.name\n",
    "    # Determine markers\n",
    "    markers = None\n",
    "    metadata, df = read_custom_tsv(file_test)\n",
    "    marker_names = metadata[\"marker_names\"]    \n",
    "    allowed_keys = [bp for bp in body_parts.keys() if bp + \".tsv\" in file_test.name]\n",
    "    target_markers = [marker for marker in marker_names if any(any(mk == marker for mk in body_parts[bp]) for bp in allowed_keys)]\n",
    "    # New filename\n",
    "    new_name = f\"sample_{i}.tsv\"\n",
    "    \n",
    "    print(f\"Renaming {file_test.name} to {new_name}\")\n",
    "    # Rename file\n",
    "    movie_file = file_test.parent.joinpath(file_test.name.split('.')[0] + \".mp4\")\n",
    "    new_movie_name = f\"sample_{i}.mp4\"\n",
    "    movie_file.rename(movie_file.with_name(new_movie_name))\n",
    "    file_test.rename(file_test.with_name(new_name))\n",
    "    \n",
    "    # Add to annotations\n",
    "    annotations[new_name.split('.')[0]] = {\"markers\": target_markers, \"synchronization\": synch, \"original_mocap\": str(file_test.name.split('_')[0])}\n",
    "\n",
    "# Save annotations\n",
    "with open(test_files[-1].parent / \"annotations.json\", \"w\") as f:\n",
    "    json.dump(annotations, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba2d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking sample_0 with sliding window length 50 ...\n",
      "Benchmarking sample_0 with sliding window length 100 ...\n",
      "Benchmarking sample_0 with sliding window length 200 ...\n",
      "Benchmarking sample_1 with sliding window length 50 ...\n",
      "Benchmarking sample_1 with sliding window length 100 ...\n",
      "Benchmarking sample_1 with sliding window length 200 ...\n",
      "Benchmarking sample_2 with sliding window length 50 ...\n",
      "Benchmarking sample_2 with sliding window length 100 ...\n",
      "Benchmarking sample_2 with sliding window length 200 ...\n",
      "Benchmarking sample_3 with sliding window length 50 ...\n",
      "Benchmarking sample_3 with sliding window length 100 ...\n",
      "Benchmarking sample_3 with sliding window length 200 ...\n",
      "Benchmarking sample_4 with sliding window length 50 ...\n",
      "Benchmarking sample_4 with sliding window length 100 ...\n",
      "Benchmarking sample_4 with sliding window length 200 ...\n",
      "Benchmarking sample_5 with sliding window length 50 ...\n",
      "Benchmarking sample_5 with sliding window length 100 ...\n",
      "Benchmarking sample_5 with sliding window length 200 ...\n",
      "Benchmarking sample_6 with sliding window length 50 ...\n",
      "Benchmarking sample_6 with sliding window length 100 ...\n",
      "Benchmarking sample_6 with sliding window length 200 ...\n",
      "Benchmarking sample_7 with sliding window length 50 ...\n",
      "Benchmarking sample_7 with sliding window length 100 ...\n",
      "Benchmarking sample_7 with sliding window length 200 ...\n",
      "Benchmarking sample_8 with sliding window length 50 ...\n",
      "Benchmarking sample_8 with sliding window length 100 ...\n",
      "Benchmarking sample_8 with sliding window length 200 ...\n",
      "Benchmarking sample_9 with sliding window length 50 ...\n",
      "Benchmarking sample_9 with sliding window length 100 ...\n",
      "Benchmarking sample_9 with sliding window length 200 ...\n",
      "Benchmarking sample_10 with sliding window length 50 ...\n",
      "Benchmarking sample_10 with sliding window length 100 ...\n",
      "Benchmarking sample_10 with sliding window length 200 ...\n",
      "Benchmarking sample_11 with sliding window length 50 ...\n",
      "Benchmarking sample_11 with sliding window length 100 ...\n",
      "Benchmarking sample_11 with sliding window length 200 ...\n",
      "Benchmarking sample_12 with sliding window length 50 ...\n",
      "Benchmarking sample_12 with sliding window length 100 ...\n",
      "Benchmarking sample_12 with sliding window length 200 ...\n",
      "Benchmarking sample_13 with sliding window length 50 ...\n",
      "Benchmarking sample_13 with sliding window length 100 ...\n",
      "Benchmarking sample_13 with sliding window length 200 ...\n",
      "Benchmarking sample_14 with sliding window length 50 ...\n",
      "Benchmarking sample_14 with sliding window length 100 ...\n",
      "Benchmarking sample_14 with sliding window length 200 ...\n",
      "Benchmarking sample_15 with sliding window length 50 ...\n",
      "Benchmarking sample_15 with sliding window length 100 ...\n",
      "Benchmarking sample_15 with sliding window length 200 ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path(\"../../pyeyesweb/\").resolve()))\n",
    "from pyeyesweb.data_models.sliding_window import SlidingWindow\n",
    "from pyeyesweb.low_level.smoothness import Smoothness\n",
    "from pyeyesweb.analysis_primitives.synchronization import Synchronization\n",
    "import numpy as np\n",
    "from pyeyesweb.utils.signal_processing import apply_savgol_filter\n",
    "from utils import read_custom_tsv\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "class FeatureBenchmarker:\n",
    "    def __init__(self, sliding_window_lengths=[50, 100, 200]):\n",
    "        self.sliding_window_lengths = sliding_window_lengths\n",
    "        self.features_implementations = {\"smoothness\": Smoothness, \n",
    "                                         \"synchronization\": Synchronization}\n",
    "\n",
    "\n",
    "class SmoothnessBenchmarker(FeatureBenchmarker):\n",
    "    def __init__(self, sliding_window_lengths=[50, 100, 200]):\n",
    "        super().__init__(sliding_window_lengths=sliding_window_lengths)\n",
    "\n",
    "    def benchmark(self):\n",
    "        FeatureClass = self.features_implementations[\"smoothness\"]\n",
    "\n",
    "        base_dir = Path(os.getcwd()) / \"data\" / \"low_level\" / \"smoothness\"\n",
    "        with open(base_dir / \"annotations.json\", \"r\") as f:\n",
    "            annotations = json.load(f)\n",
    "\n",
    "        for sample_file_name in annotations.keys():\n",
    "            test_file = base_dir / (sample_file_name + \".tsv\")\n",
    "            if not test_file.exists():\n",
    "                continue\n",
    "            out_file = base_dir / (sample_file_name + \"_results.json\")\n",
    "            metadata, df = read_custom_tsv(test_file)\n",
    "            rate_hz = metadata[\"frequency\"]\n",
    "            featureInstance = FeatureClass(rate_hz=rate_hz)\n",
    "            target_markers = annotations[sample_file_name][\"markers\"]\n",
    "            if any(marker not in metadata[\"marker_names\"] for marker in target_markers):\n",
    "                raise ValueError(f\"Markers {target_markers} not found in file {test_file}. Available markers: {metadata['marker_names']}\")\n",
    "            out = {\"features\": [{\"name\": \"smoothness\",\n",
    "                                \"data\": [],\n",
    "                                \"ground truth\": annotations[sample_file_name][\"smoothness\"]\n",
    "                                }]}\n",
    "            for length in self.sliding_window_lengths:\n",
    "                print(f\"Benchmarking {sample_file_name} with sliding window length {length} ...\")\n",
    "                sliding_len_results = {\"sliding_window_max_length\": length,\n",
    "                                        \"sparc\": {},\n",
    "                                        \"jerk_rms\": {}}\n",
    "\n",
    "                for marker in target_markers:\n",
    "                    sliding_window_single = SlidingWindow(max_length=length, n_columns=1)\n",
    "                    cols = [f\"{marker} X\", f\"{marker} Y\", f\"{marker} Z\"]\n",
    "                    coords = apply_savgol_filter(df[cols].values, rate_hz=rate_hz)\n",
    "                    vel = ((coords[1:] - coords[:-1]) ** 2).sum(axis=1) ** 0.5\n",
    "                    vel = apply_savgol_filter(np.insert(vel, 0, 0), rate_hz=rate_hz)\n",
    "                    if \"sparc\" not in sliding_len_results:\n",
    "                        sliding_len_results[\"sparc\"] = {}\n",
    "                    if \"jerk_rms\" not in sliding_len_results:\n",
    "                        sliding_len_results[\"jerk_rms\"] = {}\n",
    "                    sliding_len_results[\"sparc\"][marker] = []\n",
    "                    sliding_len_results[\"jerk_rms\"][marker] = []\n",
    "                    for v in vel:\n",
    "                        sliding_window_single.append([v])\n",
    "                        sm = featureInstance(sliding_window_single)\n",
    "                        sparc, jerk = sm[\"sparc\"], sm[\"jerk_rms\"]\n",
    "                        sliding_len_results[\"sparc\"][marker].append(sparc)\n",
    "                        sliding_len_results[\"jerk_rms\"][marker].append(jerk)\n",
    "                    sliding_len_results[\"sparc\"][marker] = np.array(sliding_len_results[\"sparc\"][marker]).tolist()\n",
    "                    sliding_len_results[\"jerk_rms\"][marker] = np.array(sliding_len_results[\"jerk_rms\"][marker]).tolist()\n",
    "                out[\"features\"][0][\"data\"].append(sliding_len_results)\n",
    "            with open(out_file, \"w\") as f:\n",
    "                json.dump(out, f, indent=4)\n",
    "        \n",
    "class SynchronizationBenchmarker(FeatureBenchmarker):\n",
    "    def __init__(self, sliding_window_lengths=[50, 100, 200]):\n",
    "        super().__init__(sliding_window_lengths=sliding_window_lengths)\n",
    "\n",
    "    def benchmark(self):\n",
    "        FeatureClass = self.features_implementations[\"synchronization\"]\n",
    "        # Implementation would be similar to SmoothnessBenchmarker\n",
    "        pass\n",
    "        \n",
    "SmoothnessBenchmarker(sliding_window_lengths=[50, 100, 200]).benchmark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyeyesweb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
